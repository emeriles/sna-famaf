SQLITE_CONNECTION = sqlite:///../data/processed/twitter_sample_full.db
CSV_RAW = ../data/raw/csvs/full.csv
CSV_CUTTED = ../data/raw/csvs/cut1_full.csv
JSON_TEXTS = ../data/raw/jsons/texts_full.json
RUNNING try_some_users
Try some users for user 163154530
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 344) (2000, 344) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6986
        True       1.00      1.00      1.00        14

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1996
        True       1.00      1.00      1.00         4

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 14707350
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 281) (2000, 281) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6821
        True       1.00      0.99      1.00       179

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1949
        True       1.00      1.00      1.00        51

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 317463387
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 837) (2000, 837) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6979
        True       1.00      1.00      1.00        21

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1994
        True       1.00      1.00      1.00         6

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 145240604
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 307) (2000, 307) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6989
        True       1.00      1.00      1.00        11

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1997
        True       1.00      1.00      1.00         3

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 143852870
Loading df
Done loading df. DF shape is :(12397359, 8) (Original: (12397359, 8)) 		Time delta is: None mins
Loading graph from ../data/graphs/latest_subgraph.gpickle
Fetched 600 level 2 neighbourhs for user 143852870.
Len of own retweets (positive examples) is (5, 2)
done getting tweets universe. Shape is  (11502, 2)
	Dataset was truncated to 10000 tweets
Extracting features Optimized. X shape is : (10000, 600)
Avance: %0.010000000000005116Avance: %0.01999999999999602Avance: %0.030000000000001137Avance: %0.03999999999999204Avance: %0.04999999999999716Avance: %0.060000000000002274Avance: %0.07000000000000739Avance: %0.0799999999999983Avance: %0.09000000000000341Avance: %0.09999999999999432Avance: %0.10999999999999943Avance: %0.12000000000000455Avance: %0.12999999999999545Avance: %0.14000000000000057Avance: %0.14999999999999147Avance: %0.1600000000000108Avance: %0.1700000000000017Avance: %0.18000000000000682Avance: %0.18999999999999773Avance: %0.20000000000000284Avance: %0.20999999999999375Avance: %0.21999999999999886Avance: %0.22999999999998977Avance: %0.23999999999999488Avance: %0.25Avance: %0.2600000000000051Avance: %0.27000000000001023Avance: %0.28000000000000114Avance: %0.29000000000000625Avance: %0.29999999999999716Avance: %0.3100000000000023Avance: %0.3199999999999932Avance: %0.3299999999999983Avance: %0.3399999999999892Avance: %0.3499999999999943Avance: %0.35999999999999943Avance: %0.37000000000000455Avance: %0.37999999999999545Avance: %0.39000000000000057Avance: %0.4000000000000057Avance: %0.4099999999999966Avance: %0.4200000000000017Avance: %0.4299999999999926Avance: %0.4399999999999977Avance: %0.44999999999998863Avance: %0.46000000000000796Avance: %0.46999999999999886Avance: %0.480000000000004Avance: %0.4899999999999949Avance: %0.5Avance: %0.5100000000000051Avance: %0.519999999999996Avance: %0.5300000000000011Avance: %0.539999999999992Avance: %0.5499999999999972Avance: %0.5600000000000023Avance: %0.5700000000000074Avance: %0.5799999999999983Avance: %0.5900000000000034Avance: %0.5999999999999943Avance: %0.6099999999999994Avance: %0.6200000000000045Avance: %0.6299999999999955Avance: %0.6400000000000006Avance: %0.6499999999999915Avance: %0.6600000000000108Avance: %0.6700000000000017Avance: %0.6800000000000068Avance: %0.6899999999999977Avance: %0.7000000000000028Avance: %0.7099999999999937Avance: %0.7199999999999989Avance: %0.730000000000004Avance: %0.7399999999999949Avance: %0.75Avance: %0.7600000000000051Avance: %0.7700000000000102Avance: %0.7800000000000011Avance: %0.7900000000000063Avance: %0.7999999999999972Avance: %0.8100000000000023Avance: %0.8199999999999932Avance: %0.8299999999999983Avance: %0.8400000000000034Avance: %0.8499999999999943Avance: %0.8599999999999994Avance: %0.8700000000000045Avance: %0.8800000000000097Avance: %0.8900000000000006Avance: %0.9000000000000057Avance: %0.9099999999999966Avance: %0.9200000000000017Avance: %0.9299999999999926Avance: %0.9399999999999977Avance: %0.9499999999999886Avance: %0.960000000000008Avance: %0.9699999999999989Avance: %0.980000000000004Avance: %0.9900000000000091Avance: %1.0Avance: %1.0100000000000051Avance: %1.019999999999996Avance: %1.0300000000000011Avance: %1.039999999999992Avance: %1.0499999999999972Avance: %1.0600000000000023Avance: %1.0700000000000074Avance: %1.0799999999999983Avance: %1.0900000000000034Avance: %1.0999999999999943Avance: %1.1099999999999994Avance: %1.1200000000000045Avance: %1.1299999999999955Avance: %1.1400000000000006Avance: %1.1499999999999915Avance: %1.1600000000000108Avance: %1.1700000000000017Avance: %1.1800000000000068Avance: %1.1899999999999977Avance: %1.2000000000000028Avance: %1.2099999999999937Avance: %1.2199999999999989Avance: %1.230000000000004Avance: %1.2399999999999949Avance: %1.25Avance: %1.259999999999991Avance: %1.2700000000000102Avance: %1.2800000000000011Avance: %1.2900000000000063Avance: %1.2999999999999972Avance: %1.3100000000000023Avance: %1.3199999999999932Avance: %1.3299999999999983Avance: %1.3400000000000034Avance: %1.3499999999999943Avance: %1.3599999999999994Avance: %1.3700000000000045Avance: %1.3800000000000097Avance: %1.3900000000000006Avance: %1.4000000000000057Avance: %1.4099999999999966Avance: %1.4200000000000017Avance: %1.4299999999999926Avance: %1.4399999999999977Avance: %1.4499999999999886Avance: %1.4599999999999937Avance: %1.4699999999999989Avance: %1.480000000000004Avance: %1.490000000000009Avance: %1.5Avance: %1.5100000000000051Avance: %1.519999999999996Avance: %1.5300000000000011Avance: %1.539999999999992Avance: %1.5499999999999972Avance: %1.559999999999988Avance: %1.5700000000000074Avance: %1.5799999999999983Avance: %1.5900000000000034Avance: %1.5999999999999943Avance: %1.6099999999999994Avance: %1.6200000000000045Avance: %1.6299999999999955Avance: %1.6400000000000006Avance: %1.6499999999999915Avance: %1.6599999999999966Avance: %1.6700000000000017Avance: %1.6800000000000068Avance: %1.6899999999999977Avance: %1.7000000000000028Avance: %1.7099999999999937Avance: %1.7199999999999989Avance: %1.730000000000004Avance: %1.7399999999999949Avance: %1.75Avance: %1.759999999999991Avance: %1.7700000000000102Avance: %1.7800000000000011Avance: %1.7900000000000063Avance: %1.7999999999999972Avance: %1.8100000000000023Avance: %1.8199999999999932Avance: %1.8299999999999983Avance: %1.8400000000000034Avance: %1.8499999999999943Avance: %1.8599999999999994Avance: %1.8700000000000045Avance: %1.8800000000000097Avance: %1.8900000000000006Avance: %1.9000000000000057Avance: %1.9099999999999966Avance: %1.9200000000000017Avance: %1.9299999999999926Avance: %1.9399999999999977Avance: %1.9500000000000028Avance: %1.9599999999999937Avance: %1.9699999999999989Avance: %1.980000000000004Avance: %1.990000000000009Avance: %2.0Avance: %2.010000000000005Avance: %2.019999999999996Avance: %2.030000000000001Avance: %2.039999999999992Avance: %2.049999999999997Avance: %2.0600000000000023Avance: %2.0700000000000074Avance: %2.0799999999999983Avance: %2.0900000000000034Avance: %2.1000000000000085Avance: %2.1099999999999994Avance: %2.1200000000000045Avance: %2.1299999999999955Avance: %2.1400000000000006Avance: %2.1499999999999915Avance: %2.1599999999999966Avance: %2.1700000000000017Avance: %2.180000000000007Avance: %2.1899999999999977Avance: %2.200000000000003Avance: %2.210000000000008Avance: %2.219999999999999Avance: %2.230000000000004Avance: %2.239999999999995Avance: %2.25Avance: %2.259999999999991Avance: %2.2700000000000102Avance: %2.280000000000001Avance: %2.2900000000000063Avance: %2.299999999999997Avance: %2.3100000000000023Avance: %2.319999999999993Avance: %2.3299999999999983Avance: %2.3400000000000034Avance: %2.3499999999999943Avance: %2.3599999999999994Avance: %2.3700000000000045Avance: %2.3800000000000097Avance: %2.3900000000000006Avance: %2.4000000000000057Avance: %2.4099999999999966Avance: %2.4200000000000017Avance: %2.4299999999999926Avance: %2.4399999999999977Avance: %2.450000000000003Avance: %2.4599999999999937Avance: %2.469999999999999Avance: %2.480000000000004Avance: %2.490000000000009Avance: %2.5Avance: %2.510000000000005Avance: %2.519999999999996Avance: %2.530000000000001Avance: %2.539999999999992Avance: %2.549999999999997Avance: %2.5600000000000023Avance: %2.569999999999993Avance: %2.5799999999999983Avance: %2.5900000000000034Avance: %2.6000000000000085Avance: %2.6099999999999994Avance: %2.6200000000000045Avance: %2.6299999999999955Avance: %2.6400000000000006Avance: %2.6499999999999915Avance: %2.6599999999999966Avance: %2.6699999999999875Avance: %2.680000000000007Avance: %2.6899999999999977Avance: %2.700000000000003Avance: %2.710000000000008Avance: %2.719999999999999Avance: %2.730000000000004Avance: %2.739999999999995Avance: %2.75Avance: %2.759999999999991Avance: %2.769999999999996Avance: %2.780000000000001Avance: %2.7900000000000063Avance: %2.799999999999997Avance: %2.8100000000000023Avance: %2.819999999999993Avance: %2.8299999999999983Avance: %2.8400000000000034Avance: %2.8499999999999943Avance: %2.8599999999999994Avance: %2.8699999999999903Avance: %2.8800000000000097Avance: %2.8900000000000006Avance: %2.9000000000000057Avance: %2.9099999999999966Avance: %2.9200000000000017Avance: %2.9299999999999926Avance: %2.9399999999999977Avance: %2.950000000000003Avance: %2.9599999999999937Avance: %2.969999999999999Avance: %2.980000000000004Avance: %2.990000000000009Avance: %3.0Avance: %3.010000000000005Avance: %3.019999999999996Avance: %3.030000000000001Avance: %3.039999999999992Avance: %3.049999999999997Avance: %3.0600000000000023Avance: %3.069999999999993Avance: %3.0799999999999983Avance: %3.0900000000000034Avance: %3.1000000000000085Avance: %3.1099999999999994Avance: %3.1200000000000045Avance: %3.1299999999999955Avance: %3.1400000000000006Avance: %3.1499999999999915Avance: %3.1599999999999966Avance: %3.1700000000000017Avance: %3.180000000000007Avance: %3.1899999999999977Avance: %3.200000000000003Avance: %3.210000000000008Avance: %3.219999999999999Avance: %3.230000000000004Avance: %3.239999999999995Avance: %3.25Avance: %3.259999999999991Avance: %3.269999999999996Avance: %3.280000000000001Avance: %3.2900000000000063Avance: %3.299999999999997Avance: %3.3100000000000023Avance: %3.3200000000000074Avance: %3.3299999999999983Avance: %3.3400000000000034Avance: %3.3499999999999943Avance: %3.3599999999999994Avance: %3.3699999999999903Avance: %3.3800000000000097Avance: %3.3900000000000006Avance: %3.4000000000000057Avance: %3.4099999999999966Avance: %3.4200000000000017Avance: %3.430000000000007Avance: %3.4399999999999977Avance: %3.450000000000003Avance: %3.4599999999999937Avance: %3.469999999999999Avance: %3.480000000000004Avance: %3.490000000000009Avance: %3.5Avance: %3.510000000000005Avance: %3.519999999999996Avance: %3.530000000000001Avance: %3.539999999999992Avance: %3.549999999999997Avance: %3.5600000000000023Avance: %3.569999999999993Avance: %3.5800000000000125Avance: %3.5900000000000034Avance: %3.6000000000000085Avance: %3.6099999999999994Avance: %3.6200000000000045Avance: %3.6299999999999955Avance: %3.6400000000000006Avance: %3.6499999999999915Avance: %3.6599999999999966Avance: %3.6700000000000017Avance: %3.680000000000007Avance: %3.6899999999999977Avance: %3.700000000000003Avance: %3.710000000000008Avance: %3.719999999999999Avance: %3.730000000000004Avance: %3.739999999999995Avance: %3.75Avance: %3.759999999999991Avance: %3.769999999999996Avance: %3.780000000000001Avance: %3.7900000000000063Avance: %3.799999999999997Avance: %3.8100000000000023Avance: %3.8200000000000074Avance: %3.8299999999999983Avance: %3.8400000000000034Avance: %3.8499999999999943Avance: %3.8599999999999994Avance: %3.8699999999999903Avance: %3.8799999999999955Avance: %3.8900000000000006Avance: %3.9000000000000057Avance: %3.9099999999999966Avance: %3.9200000000000017Avance: %3.930000000000007Avance: %3.9399999999999977Avance: %3.950000000000003Avance: %3.9599999999999937Avance: %3.969999999999999Avance: %3.9799999999999898Avance: %3.990000000000009Avance: %4.0Avance: %4.010000000000005Avance: %4.019999999999996Avance: %4.030000000000001Avance: %4.039999999999992Avance: %4.049999999999997Avance: %4.060000000000002Avance: %4.069999999999993Avance: %4.079999999999998Avance: %4.090000000000003Avance: %4.1000000000000085Avance: %4.109999999999999Avance: %4.1200000000000045Avance: %4.1299999999999955Avance: %4.140000000000001Avance: %4.1499999999999915Avance: %4.159999999999997Avance: %4.170000000000002Avance: %4.179999999999993Avance: %4.189999999999998Avance: %4.200000000000003Avance: %4.210000000000008Avance: %4.219999999999999Avance: %4.230000000000004Avance: %4.239999999999995Avance: %4.25Avance: %4.259999999999991Avance: %4.269999999999996Avance: %4.280000000000001Avance: %4.290000000000006Avance: %4.299999999999997Avance: %4.310000000000002Avance: %4.320000000000007Avance: %4.329999999999998Avance: %4.340000000000003Avance: %4.349999999999994Avance: %4.359999999999999Avance: %4.36999999999999Avance: %4.3799999999999955Avance: %4.390000000000001Avance: %4.400000000000006Avance: %4.409999999999997Avance: %4.420000000000002Avance: %4.430000000000007Avance: %4.439999999999998Avance: %4.450000000000003Avance: %4.459999999999994Avance: %4.469999999999999Avance: %4.47999999999999Avance: %4.490000000000009Avance: %4.5Avance: %4.510000000000005Avance: %4.519999999999996Avance: %4.530000000000001Avance: %4.540000000000006Avance: %4.549999999999997Avance: %4.560000000000002Avance: %4.569999999999993Avance: %4.579999999999998Avance: %4.590000000000003Avance: %4.6000000000000085Avance: %4.609999999999999Avance: %4.6200000000000045Avance: %4.6299999999999955Avance: %4.640000000000001Avance: %4.650000000000006Avance: %4.659999999999997Avance: %4.670000000000002Avance: %4.679999999999993Avance: %4.690000000000012Avance: %4.700000000000003Avance: %4.710000000000008Avance: %4.719999999999999Avance: %4.730000000000004Avance: %4.739999999999995Avance: %4.75Avance: %4.759999999999991Avance: %4.769999999999996Avance: %4.780000000000001Avance: %4.790000000000006Avance: %4.800000000000011Avance: %4.810000000000002Avance: %4.820000000000007Avance: %4.829999999999998Avance: %4.840000000000003Avance: %4.849999999999994Avance: %4.859999999999999Avance: %4.86999999999999Avance: %4.8799999999999955Avance: %4.890000000000001Avance: %4.900000000000006Avance: %4.909999999999997Avance: %4.920000000000002Avance: %4.930000000000007Avance: %4.939999999999998Avance: %4.950000000000003Avance: %4.959999999999994Avance: %4.969999999999999Avance: %4.97999999999999Avance: %4.990000000000009Avance: %5.0Avance: %5.010000000000005Avance: %5.019999999999996Avance: %5.030000000000001Avance: %5.040000000000006Avance: %5.049999999999997Avance: %5.060000000000002Avance: %5.069999999999993Avance: %5.079999999999998Avance: %5.089999999999989Avance: %5.1000000000000085Avance: %5.109999999999999Avance: %5.1200000000000045Avance: %5.1299999999999955Avance: %5.140000000000001Avance: %5.150000000000006Avance: %5.159999999999997Avance: %5.170000000000002Avance: %5.179999999999993Avance: %5.189999999999998Avance: %5.200000000000003Avance: %5.210000000000008Avance: %5.219999999999999Avance: %5.230000000000004Avance: %5.239999999999995Avance: %5.25Avance: %5.259999999999991Avance: %5.269999999999996Avance: %5.280000000000001Avance: %5.289999999999992Avance: %5.300000000000011Avance: %5.310000000000002Avance: %5.320000000000007Avance: %5.329999999999998Avance: %5.340000000000003Avance: %5.349999999999994Avance: %5.359999999999999Avance: %5.36999999999999Avance: %5.3799999999999955Avance: %5.390000000000001Avance: %5.400000000000006Avance: %5.409999999999997Avance: %5.420000000000002Avance: %5.430000000000007Avance: %5.439999999999998Avance: %5.450000000000003Avance: %5.459999999999994Avance: %5.469999999999999Avance: %5.47999999999999Avance: %5.489999999999995Avance: %5.5Avance: %5.510000000000005Avance: %5.519999999999996Avance: %5.530000000000001Avance: %5.540000000000006Avance: %5.549999999999997Avance: %5.560000000000002Avance: %5.569999999999993Avance: %5.579999999999998Avance: %5.589999999999989Avance: %5.6000000000000085Avance: %5.609999999999999Avance: %5.6200000000000045Avance: %5.6299999999999955Avance: %5.640000000000001Avance: %5.650000000000006Avance: %5.659999999999997Avance: %5.670000000000002Avance: %5.679999999999993Avance: %5.689999999999998Avance: %5.700000000000003Avance: %5.710000000000008Avance: %5.719999999999999Avance: %5.730000000000004Avance: %5.739999999999995Avance: %5.75Avance: %5.760000000000005Avance: %5.769999999999996Avance: %5.780000000000001Avance: %5.789999999999992Avance: %5.800000000000011Avance: %5.810000000000002Avance: %5.820000000000007Avance: %5.829999999999998Avance: %5.840000000000003Avance: %5.849999999999994Avance: %5.859999999999999Avance: %5.8700000000000045Avance: %5.8799999999999955Avance: %5.890000000000001Avance: %5.900000000000006Avance: %5.910000000000011Avance: %5.920000000000002Avance: %5.930000000000007Avance: %5.939999999999998Avance: %5.950000000000003Avance: %5.959999999999994Avance: %5.969999999999999Avance: %5.97999999999999Avance: %5.989999999999995Avance: %6.0Done Extracting Features 0:05:33.480342
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 600) (2000, 600) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6996
        True       1.00      1.00      1.00         4

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1999
        True       1.00      1.00      1.00         1

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 150069885
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 580) (2000, 580) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6943
        True       1.00      0.98      0.99        57

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1983
        True       1.00      1.00      1.00        17

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 62363015
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 792) (2000, 792) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': None, 'gamma': 10, 'kernel': 'poly'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6982
        True       1.00      0.72      0.84        18

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.86      0.92      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1995
        True       1.00      0.80      0.89         5

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.90      0.94      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 42775597
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 464) (2000, 464) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6784
        True       1.00      1.00      1.00       216

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1938
        True       1.00      1.00      1.00        62

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 22519602
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 778) (2000, 778) False False
The number of classes has to be greater than one; got 1 class
Try some users for user 184892265
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 426) (2000, 426) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6861
        True       1.00      0.99      1.00       139

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1961
        True       1.00      0.97      0.99        39

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.99      0.99      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 31423048
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 725) (2000, 725) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6968
        True       1.00      0.97      0.98        32

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.98      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1991
        True       1.00      1.00      1.00         9

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 55100261
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 516) (2000, 516) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6670
        True       1.00      1.00      1.00       330

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1905
        True       1.00      1.00      1.00        95

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 69962890
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 672) (2000, 672) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6987
        True       1.00      1.00      1.00        13

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1996
        True       1.00      1.00      1.00         4

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 166122395
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 497) (2000, 497) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': None, 'gamma': 10, 'kernel': 'poly'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6995
        True       1.00      0.80      0.89         5

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.90      0.94      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1999
        True       1.00      1.00      1.00         1

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 138814032
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 521) (2000, 521) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': None, 'gamma': 1, 'kernel': 'poly'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6532
        True       1.00      1.00      1.00       468

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1867
        True       1.00      0.98      0.99       133

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.99      0.99      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 169982171
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 595) (2000, 595) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6952
        True       1.00      1.00      1.00        48

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1987
        True       1.00      1.00      1.00        13

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 1198823724
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 409) (2000, 409) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6995
        True       1.00      1.00      1.00         5

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1999
        True       1.00      1.00      1.00         1

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 114510872
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 705) (2000, 705) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6874
        True       1.00      0.98      0.99       126

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1964
        True       1.00      1.00      1.00        36

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 26935676
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 473) (2000, 473) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6995
        True       1.00      1.00      1.00         5

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1999
        True       1.00      1.00      1.00         1

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 16012783
Loading df
Done loading df. DF shape is :(12397359, 8) (Original: (12397359, 8)) 		Time delta is: None mins
Loading graph from ../data/graphs/latest_subgraph.gpickle
Fetched 305 level 2 neighbourhs for user 16012783.
Len of own retweets (positive examples) is (5, 2)
done getting tweets universe. Shape is  (10625, 2)
	Dataset was truncated to 10000 tweets
Extracting features Optimized. X shape is : (10000, 305)
Avance: %0.010000000000005116Avance: %0.01999999999999602Avance: %0.030000000000001137Avance: %0.03999999999999204Avance: %0.04999999999999716Avance: %0.060000000000002274Avance: %0.07000000000000739Avance: %0.0799999999999983Avance: %0.09000000000000341Avance: %0.09999999999999432Avance: %0.10999999999999943Avance: %0.12000000000000455Avance: %0.12999999999999545Avance: %0.14000000000000057Avance: %0.14999999999999147Avance: %0.1600000000000108Avance: %0.1700000000000017Avance: %0.18000000000000682Avance: %0.18999999999999773Avance: %0.20000000000000284Avance: %0.20999999999999375Avance: %0.21999999999999886Avance: %0.22999999999998977Avance: %0.23999999999999488Avance: %0.25Avance: %0.2600000000000051Avance: %0.27000000000001023Avance: %0.28000000000000114Avance: %0.29000000000000625Avance: %0.29999999999999716Avance: %0.3100000000000023Avance: %0.3199999999999932Avance: %0.3299999999999983Avance: %0.3399999999999892Avance: %0.3499999999999943Avance: %0.35999999999999943Avance: %0.37000000000000455Avance: %0.37999999999999545Avance: %0.39000000000000057Avance: %0.4000000000000057Avance: %0.4099999999999966Avance: %0.4200000000000017Avance: %0.4299999999999926Avance: %0.4399999999999977Avance: %0.44999999999998863Avance: %0.46000000000000796Avance: %0.46999999999999886Avance: %0.480000000000004Avance: %0.4899999999999949Avance: %0.5Avance: %0.5100000000000051Avance: %0.519999999999996Avance: %0.5300000000000011Avance: %0.539999999999992Avance: %0.5499999999999972Avance: %0.5600000000000023Avance: %0.5700000000000074Avance: %0.5799999999999983Avance: %0.5900000000000034Avance: %0.5999999999999943Avance: %0.6099999999999994Avance: %0.6200000000000045Avance: %0.6299999999999955Avance: %0.6400000000000006Avance: %0.6499999999999915Avance: %0.6600000000000108Avance: %0.6700000000000017Avance: %0.6800000000000068Avance: %0.6899999999999977Avance: %0.7000000000000028Avance: %0.7099999999999937Avance: %0.7199999999999989Avance: %0.730000000000004Avance: %0.7399999999999949Avance: %0.75Avance: %0.7600000000000051Avance: %0.7700000000000102Avance: %0.7800000000000011Avance: %0.7900000000000063Avance: %0.7999999999999972Avance: %0.8100000000000023Avance: %0.8199999999999932Avance: %0.8299999999999983Avance: %0.8400000000000034Avance: %0.8499999999999943Avance: %0.8599999999999994Avance: %0.8700000000000045Avance: %0.8800000000000097Avance: %0.8900000000000006Avance: %0.9000000000000057Avance: %0.9099999999999966Avance: %0.9200000000000017Avance: %0.9299999999999926Avance: %0.9399999999999977Avance: %0.9499999999999886Avance: %0.960000000000008Avance: %0.9699999999999989Avance: %0.980000000000004Avance: %0.9900000000000091Avance: %1.0Avance: %1.0100000000000051Avance: %1.019999999999996Avance: %1.0300000000000011Avance: %1.039999999999992Avance: %1.0499999999999972Avance: %1.0600000000000023Avance: %1.0700000000000074Avance: %1.0799999999999983Avance: %1.0900000000000034Avance: %1.0999999999999943Avance: %1.1099999999999994Avance: %1.1200000000000045Avance: %1.1299999999999955Avance: %1.1400000000000006Avance: %1.1499999999999915Avance: %1.1600000000000108Avance: %1.1700000000000017Avance: %1.1800000000000068Avance: %1.1899999999999977Avance: %1.2000000000000028Avance: %1.2099999999999937Avance: %1.2199999999999989Avance: %1.230000000000004Avance: %1.2399999999999949Avance: %1.25Avance: %1.259999999999991Avance: %1.2700000000000102Avance: %1.2800000000000011Avance: %1.2900000000000063Avance: %1.2999999999999972Avance: %1.3100000000000023Avance: %1.3199999999999932Avance: %1.3299999999999983Avance: %1.3400000000000034Avance: %1.3499999999999943Avance: %1.3599999999999994Avance: %1.3700000000000045Avance: %1.3800000000000097Avance: %1.3900000000000006Avance: %1.4000000000000057Avance: %1.4099999999999966Avance: %1.4200000000000017Avance: %1.4299999999999926Avance: %1.4399999999999977Avance: %1.4499999999999886Avance: %1.4599999999999937Avance: %1.4699999999999989Avance: %1.480000000000004Avance: %1.490000000000009Avance: %1.5Avance: %1.5100000000000051Avance: %1.519999999999996Avance: %1.5300000000000011Avance: %1.539999999999992Avance: %1.5499999999999972Avance: %1.559999999999988Avance: %1.5700000000000074Avance: %1.5799999999999983Avance: %1.5900000000000034Avance: %1.5999999999999943Avance: %1.6099999999999994Avance: %1.6200000000000045Avance: %1.6299999999999955Avance: %1.6400000000000006Avance: %1.6499999999999915Avance: %1.6599999999999966Avance: %1.6700000000000017Avance: %1.6800000000000068Avance: %1.6899999999999977Avance: %1.7000000000000028Avance: %1.7099999999999937Avance: %1.7199999999999989Avance: %1.730000000000004Avance: %1.7399999999999949Avance: %1.75Avance: %1.759999999999991Avance: %1.7700000000000102Avance: %1.7800000000000011Avance: %1.7900000000000063Avance: %1.7999999999999972Avance: %1.8100000000000023Avance: %1.8199999999999932Avance: %1.8299999999999983Avance: %1.8400000000000034Avance: %1.8499999999999943Avance: %1.8599999999999994Avance: %1.8700000000000045Avance: %1.8800000000000097Avance: %1.8900000000000006Avance: %1.9000000000000057Avance: %1.9099999999999966Avance: %1.9200000000000017Avance: %1.9299999999999926Avance: %1.9399999999999977Avance: %1.9500000000000028Avance: %1.9599999999999937Avance: %1.9699999999999989Avance: %1.980000000000004Avance: %1.990000000000009Avance: %2.0Avance: %2.010000000000005Avance: %2.019999999999996Avance: %2.030000000000001Avance: %2.039999999999992Avance: %2.049999999999997Avance: %2.0600000000000023Avance: %2.0700000000000074Avance: %2.0799999999999983Avance: %2.0900000000000034Avance: %2.1000000000000085Avance: %2.1099999999999994Avance: %2.1200000000000045Avance: %2.1299999999999955Avance: %2.1400000000000006Avance: %2.1499999999999915Avance: %2.1599999999999966Avance: %2.1700000000000017Avance: %2.180000000000007Avance: %2.1899999999999977Avance: %2.200000000000003Avance: %2.210000000000008Avance: %2.219999999999999Avance: %2.230000000000004Avance: %2.239999999999995Avance: %2.25Avance: %2.259999999999991Avance: %2.2700000000000102Avance: %2.280000000000001Avance: %2.2900000000000063Avance: %2.299999999999997Avance: %2.3100000000000023Avance: %2.319999999999993Avance: %2.3299999999999983Avance: %2.3400000000000034Avance: %2.3499999999999943Avance: %2.3599999999999994Avance: %2.3700000000000045Avance: %2.3800000000000097Avance: %2.3900000000000006Avance: %2.4000000000000057Avance: %2.4099999999999966Avance: %2.4200000000000017Avance: %2.4299999999999926Avance: %2.4399999999999977Avance: %2.450000000000003Avance: %2.4599999999999937Avance: %2.469999999999999Avance: %2.480000000000004Avance: %2.490000000000009Avance: %2.5Avance: %2.510000000000005Avance: %2.519999999999996Avance: %2.530000000000001Avance: %2.539999999999992Avance: %2.549999999999997Avance: %2.5600000000000023Avance: %2.569999999999993Avance: %2.5799999999999983Avance: %2.5900000000000034Avance: %2.6000000000000085Avance: %2.6099999999999994Avance: %2.6200000000000045Avance: %2.6299999999999955Avance: %2.6400000000000006Avance: %2.6499999999999915Avance: %2.6599999999999966Avance: %2.6699999999999875Avance: %2.680000000000007Avance: %2.6899999999999977Avance: %2.700000000000003Avance: %2.710000000000008Avance: %2.719999999999999Avance: %2.730000000000004Avance: %2.739999999999995Avance: %2.75Avance: %2.759999999999991Avance: %2.769999999999996Avance: %2.780000000000001Avance: %2.7900000000000063Avance: %2.799999999999997Avance: %2.8100000000000023Avance: %2.819999999999993Avance: %2.8299999999999983Avance: %2.8400000000000034Avance: %2.8499999999999943Avance: %2.8599999999999994Avance: %2.8699999999999903Avance: %2.8800000000000097Avance: %2.8900000000000006Avance: %2.9000000000000057Avance: %2.9099999999999966Avance: %2.9200000000000017Avance: %2.9299999999999926Avance: %2.9399999999999977Avance: %2.950000000000003Avance: %2.9599999999999937Avance: %2.969999999999999Avance: %2.980000000000004Avance: %2.990000000000009Avance: %3.0Avance: %3.010000000000005Avance: %3.019999999999996Avance: %3.030000000000001Avance: %3.039999999999992Avance: %3.049999999999997Done Extracting Features 0:02:30.930774
The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.
Try some users for user 75309219
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 326) (2000, 326) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6657
        True       1.00      0.98      0.99       343

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1902
        True       1.00      1.00      1.00        98

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 416721882
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 278) (2000, 278) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6708
        True       1.00      0.95      0.98       292

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.98      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1917
        True       1.00      0.94      0.97        83

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.97      0.98      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 183280034
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 757) (2000, 757) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6994
        True       1.00      1.00      1.00         6

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1998
        True       1.00      1.00      1.00         2

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 35839931
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 474) (2000, 474) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6922
        True       1.00      0.96      0.98        78

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.98      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1978
        True       1.00      1.00      1.00        22

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 76946476
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 431) (2000, 431) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6856
        True       1.00      0.98      0.99       144

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1959
        True       1.00      1.00      1.00        41

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 8150842
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 654) (2000, 654) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6993
        True       1.00      1.00      1.00         7

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1998
        True       1.00      1.00      1.00         2

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 139556774
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 470) (2000, 470) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6674
        True       1.00      0.98      0.99       326

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1907
        True       1.00      0.94      0.97        93

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.97      0.98      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 326807652
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 846) (2000, 846) False False
The number of classes has to be greater than one; got 1 class
Try some users for user 195691612
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 481) (2000, 481) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6982
        True       1.00      1.00      1.00        18

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1995
        True       1.00      1.00      1.00         5

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 70851232
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 493) (2000, 493) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6918
        True       1.00      1.00      1.00        82

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1977
        True       1.00      1.00      1.00        23

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 23608164
Loading df
Done loading df. DF shape is :(12397359, 8) (Original: (12397359, 8)) 		Time delta is: None mins
Loading graph from ../data/graphs/latest_subgraph.gpickle
Fetched 921 level 2 neighbourhs for user 23608164.
Len of own retweets (positive examples) is (1, 2)
done getting tweets universe. Shape is  (12333, 2)
	Dataset was truncated to 10000 tweets
Extracting features Optimized. X shape is : (10000, 921)
Avance: %0.010000000000005116Avance: %0.01999999999999602Avance: %0.030000000000001137Avance: %0.03999999999999204Avance: %0.04999999999999716Avance: %0.060000000000002274Avance: %0.07000000000000739Avance: %0.0799999999999983Avance: %0.09000000000000341Avance: %0.09999999999999432Avance: %0.10999999999999943Avance: %0.12000000000000455Avance: %0.12999999999999545Avance: %0.14000000000000057Avance: %0.14999999999999147Avance: %0.1600000000000108Avance: %0.1700000000000017Avance: %0.18000000000000682Avance: %0.18999999999999773Avance: %0.20000000000000284Avance: %0.20999999999999375Avance: %0.21999999999999886Avance: %0.22999999999998977Avance: %0.23999999999999488Avance: %0.25Avance: %0.2600000000000051Avance: %0.27000000000001023Avance: %0.28000000000000114Avance: %0.29000000000000625Avance: %0.29999999999999716Avance: %0.3100000000000023Avance: %0.3199999999999932Avance: %0.3299999999999983Avance: %0.3399999999999892Avance: %0.3499999999999943Avance: %0.35999999999999943Avance: %0.37000000000000455Avance: %0.37999999999999545Avance: %0.39000000000000057Avance: %0.4000000000000057Avance: %0.4099999999999966Avance: %0.4200000000000017Avance: %0.4299999999999926Avance: %0.4399999999999977Avance: %0.44999999999998863Avance: %0.46000000000000796Avance: %0.46999999999999886Avance: %0.480000000000004Avance: %0.4899999999999949Avance: %0.5Avance: %0.5100000000000051Avance: %0.519999999999996Avance: %0.5300000000000011Avance: %0.539999999999992Avance: %0.5499999999999972Avance: %0.5600000000000023Avance: %0.5700000000000074Avance: %0.5799999999999983Avance: %0.5900000000000034Avance: %0.5999999999999943Avance: %0.6099999999999994Avance: %0.6200000000000045Avance: %0.6299999999999955Avance: %0.6400000000000006Avance: %0.6499999999999915Avance: %0.6600000000000108Avance: %0.6700000000000017Avance: %0.6800000000000068Avance: %0.6899999999999977Avance: %0.7000000000000028Avance: %0.7099999999999937Avance: %0.7199999999999989Avance: %0.730000000000004Avance: %0.7399999999999949Avance: %0.75Avance: %0.7600000000000051Avance: %0.7700000000000102Avance: %0.7800000000000011Avance: %0.7900000000000063Avance: %0.7999999999999972Avance: %0.8100000000000023Avance: %0.8199999999999932Avance: %0.8299999999999983Avance: %0.8400000000000034Avance: %0.8499999999999943Avance: %0.8599999999999994Avance: %0.8700000000000045Avance: %0.8800000000000097Avance: %0.8900000000000006Avance: %0.9000000000000057Avance: %0.9099999999999966Avance: %0.9200000000000017Avance: %0.9299999999999926Avance: %0.9399999999999977Avance: %0.9499999999999886Avance: %0.960000000000008Avance: %0.9699999999999989Avance: %0.980000000000004Avance: %0.9900000000000091Avance: %1.0Avance: %1.0100000000000051Avance: %1.019999999999996Avance: %1.0300000000000011Avance: %1.039999999999992Avance: %1.0499999999999972Avance: %1.0600000000000023Avance: %1.0700000000000074Avance: %1.0799999999999983Avance: %1.0900000000000034Avance: %1.0999999999999943Avance: %1.1099999999999994Avance: %1.1200000000000045Avance: %1.1299999999999955Avance: %1.1400000000000006Avance: %1.1499999999999915Avance: %1.1600000000000108Avance: %1.1700000000000017Avance: %1.1800000000000068Avance: %1.1899999999999977Avance: %1.2000000000000028Avance: %1.2099999999999937Avance: %1.2199999999999989Avance: %1.230000000000004Avance: %1.2399999999999949Avance: %1.25Avance: %1.259999999999991Avance: %1.2700000000000102Avance: %1.2800000000000011Avance: %1.2900000000000063Avance: %1.2999999999999972Avance: %1.3100000000000023Avance: %1.3199999999999932Avance: %1.3299999999999983Avance: %1.3400000000000034Avance: %1.3499999999999943Avance: %1.3599999999999994Avance: %1.3700000000000045Avance: %1.3800000000000097Avance: %1.3900000000000006Avance: %1.4000000000000057Avance: %1.4099999999999966Avance: %1.4200000000000017Avance: %1.4299999999999926Avance: %1.4399999999999977Avance: %1.4499999999999886Avance: %1.4599999999999937Avance: %1.4699999999999989Avance: %1.480000000000004Avance: %1.490000000000009Avance: %1.5Avance: %1.5100000000000051Avance: %1.519999999999996Avance: %1.5300000000000011Avance: %1.539999999999992Avance: %1.5499999999999972Avance: %1.559999999999988Avance: %1.5700000000000074Avance: %1.5799999999999983Avance: %1.5900000000000034Avance: %1.5999999999999943Avance: %1.6099999999999994Avance: %1.6200000000000045Avance: %1.6299999999999955Avance: %1.6400000000000006Avance: %1.6499999999999915Avance: %1.6599999999999966Avance: %1.6700000000000017Avance: %1.6800000000000068Avance: %1.6899999999999977Avance: %1.7000000000000028Avance: %1.7099999999999937Avance: %1.7199999999999989Avance: %1.730000000000004Avance: %1.7399999999999949Avance: %1.75Avance: %1.759999999999991Avance: %1.7700000000000102Avance: %1.7800000000000011Avance: %1.7900000000000063Avance: %1.7999999999999972Avance: %1.8100000000000023Avance: %1.8199999999999932Avance: %1.8299999999999983Avance: %1.8400000000000034Avance: %1.8499999999999943Avance: %1.8599999999999994Avance: %1.8700000000000045Avance: %1.8800000000000097Avance: %1.8900000000000006Avance: %1.9000000000000057Avance: %1.9099999999999966Avance: %1.9200000000000017Avance: %1.9299999999999926Avance: %1.9399999999999977Avance: %1.9500000000000028Avance: %1.9599999999999937Avance: %1.9699999999999989Avance: %1.980000000000004Avance: %1.990000000000009Avance: %2.0Avance: %2.010000000000005Avance: %2.019999999999996Avance: %2.030000000000001Avance: %2.039999999999992Avance: %2.049999999999997Avance: %2.0600000000000023Avance: %2.0700000000000074Avance: %2.0799999999999983Avance: %2.0900000000000034Avance: %2.1000000000000085Avance: %2.1099999999999994Avance: %2.1200000000000045Avance: %2.1299999999999955Avance: %2.1400000000000006Avance: %2.1499999999999915Avance: %2.1599999999999966Avance: %2.1700000000000017Avance: %2.180000000000007Avance: %2.1899999999999977Avance: %2.200000000000003Avance: %2.210000000000008Avance: %2.219999999999999Avance: %2.230000000000004Avance: %2.239999999999995Avance: %2.25Avance: %2.259999999999991Avance: %2.2700000000000102Avance: %2.280000000000001Avance: %2.2900000000000063Avance: %2.299999999999997Avance: %2.3100000000000023Avance: %2.319999999999993Avance: %2.3299999999999983Avance: %2.3400000000000034Avance: %2.3499999999999943Avance: %2.3599999999999994Avance: %2.3700000000000045Avance: %2.3800000000000097Avance: %2.3900000000000006Avance: %2.4000000000000057Avance: %2.4099999999999966Avance: %2.4200000000000017Avance: %2.4299999999999926Avance: %2.4399999999999977Avance: %2.450000000000003Avance: %2.4599999999999937Avance: %2.469999999999999Avance: %2.480000000000004Avance: %2.490000000000009Avance: %2.5Avance: %2.510000000000005Avance: %2.519999999999996Avance: %2.530000000000001Avance: %2.539999999999992Avance: %2.549999999999997Avance: %2.5600000000000023Avance: %2.569999999999993Avance: %2.5799999999999983Avance: %2.5900000000000034Avance: %2.6000000000000085Avance: %2.6099999999999994Avance: %2.6200000000000045Avance: %2.6299999999999955Avance: %2.6400000000000006Avance: %2.6499999999999915Avance: %2.6599999999999966Avance: %2.6699999999999875Avance: %2.680000000000007Avance: %2.6899999999999977Avance: %2.700000000000003Avance: %2.710000000000008Avance: %2.719999999999999Avance: %2.730000000000004Avance: %2.739999999999995Avance: %2.75Avance: %2.759999999999991Avance: %2.769999999999996Avance: %2.780000000000001Avance: %2.7900000000000063Avance: %2.799999999999997Avance: %2.8100000000000023Avance: %2.819999999999993Avance: %2.8299999999999983Avance: %2.8400000000000034Avance: %2.8499999999999943Avance: %2.8599999999999994Avance: %2.8699999999999903Avance: %2.8800000000000097Avance: %2.8900000000000006Avance: %2.9000000000000057Avance: %2.9099999999999966Avance: %2.9200000000000017Avance: %2.9299999999999926Avance: %2.9399999999999977Avance: %2.950000000000003Avance: %2.9599999999999937Avance: %2.969999999999999Avance: %2.980000000000004Avance: %2.990000000000009Avance: %3.0Avance: %3.010000000000005Avance: %3.019999999999996Avance: %3.030000000000001Avance: %3.039999999999992Avance: %3.049999999999997Avance: %3.0600000000000023Avance: %3.069999999999993Avance: %3.0799999999999983Avance: %3.0900000000000034Avance: %3.1000000000000085Avance: %3.1099999999999994Avance: %3.1200000000000045Avance: %3.1299999999999955Avance: %3.1400000000000006Avance: %3.1499999999999915Avance: %3.1599999999999966Avance: %3.1700000000000017Avance: %3.180000000000007Avance: %3.1899999999999977Avance: %3.200000000000003Avance: %3.210000000000008Avance: %3.219999999999999Avance: %3.230000000000004Avance: %3.239999999999995Avance: %3.25Avance: %3.259999999999991Avance: %3.269999999999996Avance: %3.280000000000001Avance: %3.2900000000000063Avance: %3.299999999999997Avance: %3.3100000000000023Avance: %3.3200000000000074Avance: %3.3299999999999983Avance: %3.3400000000000034Avance: %3.3499999999999943Avance: %3.3599999999999994Avance: %3.3699999999999903Avance: %3.3800000000000097Avance: %3.3900000000000006Avance: %3.4000000000000057Avance: %3.4099999999999966Avance: %3.4200000000000017Avance: %3.430000000000007Avance: %3.4399999999999977Avance: %3.450000000000003Avance: %3.4599999999999937Avance: %3.469999999999999Avance: %3.480000000000004Avance: %3.490000000000009Avance: %3.5Avance: %3.510000000000005Avance: %3.519999999999996Avance: %3.530000000000001Avance: %3.539999999999992Avance: %3.549999999999997Avance: %3.5600000000000023Avance: %3.569999999999993Avance: %3.5800000000000125Avance: %3.5900000000000034Avance: %3.6000000000000085Avance: %3.6099999999999994Avance: %3.6200000000000045Avance: %3.6299999999999955Avance: %3.6400000000000006Avance: %3.6499999999999915Avance: %3.6599999999999966Avance: %3.6700000000000017Avance: %3.680000000000007Avance: %3.6899999999999977Avance: %3.700000000000003Avance: %3.710000000000008Avance: %3.719999999999999Avance: %3.730000000000004Avance: %3.739999999999995Avance: %3.75Avance: %3.759999999999991Avance: %3.769999999999996Avance: %3.780000000000001Avance: %3.7900000000000063Avance: %3.799999999999997Avance: %3.8100000000000023Avance: %3.8200000000000074Avance: %3.8299999999999983Avance: %3.8400000000000034Avance: %3.8499999999999943Avance: %3.8599999999999994Avance: %3.8699999999999903Avance: %3.8799999999999955Avance: %3.8900000000000006Avance: %3.9000000000000057Avance: %3.9099999999999966Avance: %3.9200000000000017Avance: %3.930000000000007Avance: %3.9399999999999977Avance: %3.950000000000003Avance: %3.9599999999999937Avance: %3.969999999999999Avance: %3.9799999999999898Avance: %3.990000000000009Avance: %4.0Avance: %4.010000000000005Avance: %4.019999999999996Avance: %4.030000000000001Avance: %4.039999999999992Avance: %4.049999999999997Avance: %4.060000000000002Avance: %4.069999999999993Avance: %4.079999999999998Avance: %4.090000000000003Avance: %4.1000000000000085Avance: %4.109999999999999Avance: %4.1200000000000045Avance: %4.1299999999999955Avance: %4.140000000000001Avance: %4.1499999999999915Avance: %4.159999999999997Avance: %4.170000000000002Avance: %4.179999999999993Avance: %4.189999999999998Avance: %4.200000000000003Avance: %4.210000000000008Avance: %4.219999999999999Avance: %4.230000000000004Avance: %4.239999999999995Avance: %4.25Avance: %4.259999999999991Avance: %4.269999999999996Avance: %4.280000000000001Avance: %4.290000000000006Avance: %4.299999999999997Avance: %4.310000000000002Avance: %4.320000000000007Avance: %4.329999999999998Avance: %4.340000000000003Avance: %4.349999999999994Avance: %4.359999999999999Avance: %4.36999999999999Avance: %4.3799999999999955Avance: %4.390000000000001Avance: %4.400000000000006Avance: %4.409999999999997Avance: %4.420000000000002Avance: %4.430000000000007Avance: %4.439999999999998Avance: %4.450000000000003Avance: %4.459999999999994Avance: %4.469999999999999Avance: %4.47999999999999Avance: %4.490000000000009Avance: %4.5Avance: %4.510000000000005Avance: %4.519999999999996Avance: %4.530000000000001Avance: %4.540000000000006Avance: %4.549999999999997Avance: %4.560000000000002Avance: %4.569999999999993Avance: %4.579999999999998Avance: %4.590000000000003Avance: %4.6000000000000085Avance: %4.609999999999999Avance: %4.6200000000000045Avance: %4.6299999999999955Avance: %4.640000000000001Avance: %4.650000000000006Avance: %4.659999999999997Avance: %4.670000000000002Avance: %4.679999999999993Avance: %4.690000000000012Avance: %4.700000000000003Avance: %4.710000000000008Avance: %4.719999999999999Avance: %4.730000000000004Avance: %4.739999999999995Avance: %4.75Avance: %4.759999999999991Avance: %4.769999999999996Avance: %4.780000000000001Avance: %4.790000000000006Avance: %4.800000000000011Avance: %4.810000000000002Avance: %4.820000000000007Avance: %4.829999999999998Avance: %4.840000000000003Avance: %4.849999999999994Avance: %4.859999999999999Avance: %4.86999999999999Avance: %4.8799999999999955Avance: %4.890000000000001Avance: %4.900000000000006Avance: %4.909999999999997Avance: %4.920000000000002Avance: %4.930000000000007Avance: %4.939999999999998Avance: %4.950000000000003Avance: %4.959999999999994Avance: %4.969999999999999Avance: %4.97999999999999Avance: %4.990000000000009Avance: %5.0Avance: %5.010000000000005Avance: %5.019999999999996Avance: %5.030000000000001Avance: %5.040000000000006Avance: %5.049999999999997Avance: %5.060000000000002Avance: %5.069999999999993Avance: %5.079999999999998Avance: %5.089999999999989Avance: %5.1000000000000085Avance: %5.109999999999999Avance: %5.1200000000000045Avance: %5.1299999999999955Avance: %5.140000000000001Avance: %5.150000000000006Avance: %5.159999999999997Avance: %5.170000000000002Avance: %5.179999999999993Avance: %5.189999999999998Avance: %5.200000000000003Avance: %5.210000000000008Avance: %5.219999999999999Avance: %5.230000000000004Avance: %5.239999999999995Avance: %5.25Avance: %5.259999999999991Avance: %5.269999999999996Avance: %5.280000000000001Avance: %5.289999999999992Avance: %5.300000000000011Avance: %5.310000000000002Avance: %5.320000000000007Avance: %5.329999999999998Avance: %5.340000000000003Avance: %5.349999999999994Avance: %5.359999999999999Avance: %5.36999999999999Avance: %5.3799999999999955Avance: %5.390000000000001Avance: %5.400000000000006Avance: %5.409999999999997Avance: %5.420000000000002Avance: %5.430000000000007Avance: %5.439999999999998Avance: %5.450000000000003Avance: %5.459999999999994Avance: %5.469999999999999Avance: %5.47999999999999Avance: %5.489999999999995Avance: %5.5Avance: %5.510000000000005Avance: %5.519999999999996Avance: %5.530000000000001Avance: %5.540000000000006Avance: %5.549999999999997Avance: %5.560000000000002Avance: %5.569999999999993Avance: %5.579999999999998Avance: %5.589999999999989Avance: %5.6000000000000085Avance: %5.609999999999999Avance: %5.6200000000000045Avance: %5.6299999999999955Avance: %5.640000000000001Avance: %5.650000000000006Avance: %5.659999999999997Avance: %5.670000000000002Avance: %5.679999999999993Avance: %5.689999999999998Avance: %5.700000000000003Avance: %5.710000000000008Avance: %5.719999999999999Avance: %5.730000000000004Avance: %5.739999999999995Avance: %5.75Avance: %5.760000000000005Avance: %5.769999999999996Avance: %5.780000000000001Avance: %5.789999999999992Avance: %5.800000000000011Avance: %5.810000000000002Avance: %5.820000000000007Avance: %5.829999999999998Avance: %5.840000000000003Avance: %5.849999999999994Avance: %5.859999999999999Avance: %5.8700000000000045Avance: %5.8799999999999955Avance: %5.890000000000001Avance: %5.900000000000006Avance: %5.910000000000011Avance: %5.920000000000002Avance: %5.930000000000007Avance: %5.939999999999998Avance: %5.950000000000003Avance: %5.959999999999994Avance: %5.969999999999999Avance: %5.97999999999999Avance: %5.989999999999995Avance: %6.0Avance: %6.010000000000005Avance: %6.02000000000001Avance: %6.030000000000001Avance: %6.040000000000006Avance: %6.049999999999997Avance: %6.060000000000002Avance: %6.069999999999993Avance: %6.079999999999998Avance: %6.089999999999989Avance: %6.1000000000000085Avance: %6.109999999999999Avance: %6.1200000000000045Avance: %6.1299999999999955Avance: %6.140000000000001Avance: %6.150000000000006Avance: %6.159999999999997Avance: %6.170000000000002Avance: %6.179999999999993Avance: %6.189999999999998Avance: %6.200000000000003Avance: %6.210000000000008Avance: %6.219999999999999Avance: %6.230000000000004Avance: %6.239999999999995Avance: %6.25Avance: %6.260000000000005Avance: %6.269999999999996Avance: %6.280000000000001Avance: %6.289999999999992Avance: %6.299999999999997Avance: %6.310000000000002Avance: %6.320000000000007Avance: %6.329999999999998Avance: %6.340000000000003Avance: %6.349999999999994Avance: %6.359999999999999Avance: %6.3700000000000045Avance: %6.3799999999999955Avance: %6.390000000000001Avance: %6.3999999999999915Avance: %6.410000000000011Avance: %6.420000000000002Avance: %6.430000000000007Avance: %6.439999999999998Avance: %6.450000000000003Avance: %6.459999999999994Avance: %6.469999999999999Avance: %6.47999999999999Avance: %6.489999999999995Avance: %6.5Avance: %6.510000000000005Avance: %6.52000000000001Avance: %6.530000000000001Avance: %6.540000000000006Avance: %6.549999999999997Avance: %6.560000000000002Avance: %6.569999999999993Avance: %6.579999999999998Avance: %6.589999999999989Avance: %6.599999999999994Avance: %6.609999999999999Avance: %6.6200000000000045Avance: %6.6299999999999955Avance: %6.640000000000001Avance: %6.650000000000006Avance: %6.659999999999997Avance: %6.670000000000002Avance: %6.679999999999993Avance: %6.689999999999998Avance: %6.699999999999989Avance: %6.710000000000008Avance: %6.719999999999999Avance: %6.730000000000004Avance: %6.739999999999995Avance: %6.75Avance: %6.760000000000005Avance: %6.769999999999996Avance: %6.780000000000001Avance: %6.789999999999992Avance: %6.799999999999997Avance: %6.810000000000002Avance: %6.820000000000007Avance: %6.829999999999998Avance: %6.840000000000003Avance: %6.849999999999994Avance: %6.859999999999999Avance: %6.8700000000000045Avance: %6.8799999999999955Avance: %6.890000000000001Avance: %6.8999999999999915Avance: %6.910000000000011Avance: %6.920000000000002Avance: %6.930000000000007Avance: %6.939999999999998Avance: %6.950000000000003Avance: %6.959999999999994Avance: %6.969999999999999Avance: %6.980000000000004Avance: %6.989999999999995Avance: %7.0Avance: %7.010000000000005Avance: %7.02000000000001Avance: %7.030000000000001Avance: %7.040000000000006Avance: %7.049999999999997Avance: %7.060000000000002Avance: %7.069999999999993Avance: %7.079999999999998Avance: %7.090000000000003Avance: %7.099999999999994Avance: %7.109999999999999Avance: %7.1200000000000045Avance: %7.13000000000001Avance: %7.140000000000001Avance: %7.150000000000006Avance: %7.159999999999997Avance: %7.170000000000002Avance: %7.179999999999993Avance: %7.189999999999998Avance: %7.199999999999989Avance: %7.210000000000008Avance: %7.219999999999999Avance: %7.230000000000004Avance: %7.240000000000009Avance: %7.25Avance: %7.260000000000005Avance: %7.269999999999996Avance: %7.280000000000001Avance: %7.289999999999992Avance: %7.299999999999997Avance: %7.310000000000002Avance: %7.320000000000007Avance: %7.329999999999998Avance: %7.340000000000003Avance: %7.349999999999994Avance: %7.359999999999999Avance: %7.3700000000000045Avance: %7.3799999999999955Avance: %7.390000000000001Avance: %7.3999999999999915Avance: %7.410000000000011Avance: %7.420000000000002Avance: %7.430000000000007Avance: %7.439999999999998Avance: %7.450000000000003Avance: %7.459999999999994Avance: %7.469999999999999Avance: %7.480000000000004Avance: %7.489999999999995Avance: %7.5Avance: %7.509999999999991Avance: %7.52000000000001Avance: %7.530000000000001Avance: %7.540000000000006Avance: %7.549999999999997Avance: %7.560000000000002Avance: %7.569999999999993Avance: %7.579999999999998Avance: %7.590000000000003Avance: %7.599999999999994Avance: %7.609999999999999Avance: %7.6200000000000045Avance: %7.63000000000001Avance: %7.640000000000001Avance: %7.650000000000006Avance: %7.659999999999997Avance: %7.670000000000002Avance: %7.679999999999993Avance: %7.689999999999998Avance: %7.699999999999989Avance: %7.709999999999994Avance: %7.719999999999999Avance: %7.730000000000004Avance: %7.740000000000009Avance: %7.75Avance: %7.760000000000005Avance: %7.769999999999996Avance: %7.780000000000001Avance: %7.789999999999992Avance: %7.799999999999997Avance: %7.809999999999988Avance: %7.820000000000007Avance: %7.829999999999998Avance: %7.840000000000003Avance: %7.849999999999994Avance: %7.859999999999999Avance: %7.8700000000000045Avance: %7.8799999999999955Avance: %7.890000000000001Avance: %7.8999999999999915Avance: %7.909999999999997Avance: %7.920000000000002Avance: %7.930000000000007Avance: %7.939999999999998Avance: %7.950000000000003Avance: %7.959999999999994Avance: %7.969999999999999Avance: %7.980000000000004Avance: %7.989999999999995Avance: %8.0Avance: %8.009999999999991Avance: %8.02000000000001Avance: %8.030000000000001Avance: %8.040000000000006Avance: %8.049999999999997Avance: %8.060000000000002Avance: %8.069999999999993Avance: %8.079999999999998Avance: %8.090000000000003Avance: %8.099999999999994Avance: %8.11Avance: %8.120000000000005Avance: %8.13000000000001Avance: %8.14Avance: %8.150000000000006Avance: %8.159999999999997Avance: %8.170000000000002Avance: %8.179999999999993Avance: %8.189999999999998Avance: %8.200000000000003Avance: %8.209999999999994Avance: %8.219999999999999Avance: %8.230000000000004Avance: %8.240000000000009Avance: %8.25Avance: %8.260000000000005Avance: %8.269999999999996Avance: %8.280000000000001Avance: %8.289999999999992Avance: %8.299999999999997Avance: %8.310000000000002Avance: %8.320000000000007Avance: %8.329999999999998Avance: %8.340000000000003Avance: %8.350000000000009Avance: %8.36Avance: %8.370000000000005Avance: %8.379999999999995Avance: %8.39Avance: %8.399999999999991Avance: %8.409999999999997Avance: %8.420000000000002Avance: %8.430000000000007Avance: %8.439999999999998Avance: %8.450000000000003Avance: %8.460000000000008Avance: %8.469999999999999Avance: %8.480000000000004Avance: %8.489999999999995Avance: %8.5Avance: %8.509999999999991Avance: %8.52000000000001Avance: %8.530000000000001Avance: %8.540000000000006Avance: %8.549999999999997Avance: %8.560000000000002Avance: %8.569999999999993Avance: %8.579999999999998Avance: %8.590000000000003Avance: %8.599999999999994Avance: %8.61Avance: %8.620000000000005Avance: %8.63000000000001Avance: %8.64Avance: %8.650000000000006Avance: %8.659999999999997Avance: %8.670000000000002Avance: %8.679999999999993Avance: %8.689999999999998Avance: %8.700000000000003Avance: %8.709999999999994Avance: %8.719999999999999Avance: %8.730000000000004Avance: %8.740000000000009Avance: %8.75Avance: %8.760000000000005Avance: %8.769999999999996Avance: %8.780000000000001Avance: %8.789999999999992Avance: %8.799999999999997Avance: %8.810000000000002Avance: %8.819999999999993Avance: %8.829999999999998Avance: %8.840000000000003Avance: %8.850000000000009Avance: %8.86Avance: %8.870000000000005Avance: %8.879999999999995Avance: %8.89Avance: %8.899999999999991Avance: %8.909999999999997Avance: %8.919999999999987Avance: %8.930000000000007Avance: %8.939999999999998Avance: %8.950000000000003Avance: %8.960000000000008Avance: %8.969999999999999Avance: %8.980000000000004Avance: %8.989999999999995Avance: %9.0Avance: %9.009999999999991Avance: %9.019999999999996Avance: %9.030000000000001Avance: %9.040000000000006Avance: %9.049999999999997Avance: %9.060000000000002Avance: %9.069999999999993Avance: %9.079999999999998Avance: %9.090000000000003Avance: %9.099999999999994Avance: %9.11Avance: %9.11999999999999Avance: %9.13000000000001Avance: %9.14Avance: %9.150000000000006Avance: %9.159999999999997Avance: %9.170000000000002Avance: %9.179999999999993Avance: %9.189999999999998Avance: %9.200000000000003Avance: %9.209999999999994Done Extracting Features 0:08:17.792021
The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.
Try some users for user 7664892
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 820) (2000, 820) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6922
        True       1.00      0.99      0.99        78

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1978
        True       1.00      1.00      1.00        22

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 14265389
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 451) (2000, 451) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6931
        True       1.00      0.99      0.99        69

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1981
        True       1.00      1.00      1.00        19

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 1864359570
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 200) (2000, 200) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6989
        True       1.00      1.00      1.00        11

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1997
        True       1.00      1.00      1.00         3

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 364528766
Loading df
Done loading df. DF shape is :(12397359, 8) (Original: (12397359, 8)) 		Time delta is: None mins
Loading graph from ../data/graphs/latest_subgraph.gpickle
Fetched 500 level 2 neighbourhs for user 364528766.
Len of own retweets (positive examples) is (4, 2)
done getting tweets universe. Shape is  (10266, 2)
	Dataset was truncated to 10000 tweets
Extracting features Optimized. X shape is : (10000, 500)
Avance: %0.010000000000005116Avance: %0.01999999999999602Avance: %0.030000000000001137Avance: %0.03999999999999204Avance: %0.04999999999999716Avance: %0.060000000000002274Avance: %0.07000000000000739Avance: %0.0799999999999983Avance: %0.09000000000000341Avance: %0.09999999999999432Avance: %0.10999999999999943Avance: %0.12000000000000455Avance: %0.12999999999999545Avance: %0.14000000000000057Avance: %0.14999999999999147Avance: %0.1600000000000108Avance: %0.1700000000000017Avance: %0.18000000000000682Avance: %0.18999999999999773Avance: %0.20000000000000284Avance: %0.20999999999999375Avance: %0.21999999999999886Avance: %0.22999999999998977Avance: %0.23999999999999488Avance: %0.25Avance: %0.2600000000000051Avance: %0.27000000000001023Avance: %0.28000000000000114Avance: %0.29000000000000625Avance: %0.29999999999999716Avance: %0.3100000000000023Avance: %0.3199999999999932Avance: %0.3299999999999983Avance: %0.3399999999999892Avance: %0.3499999999999943Avance: %0.35999999999999943Avance: %0.37000000000000455Avance: %0.37999999999999545Avance: %0.39000000000000057Avance: %0.4000000000000057Avance: %0.4099999999999966Avance: %0.4200000000000017Avance: %0.4299999999999926Avance: %0.4399999999999977Avance: %0.44999999999998863Avance: %0.46000000000000796Avance: %0.46999999999999886Avance: %0.480000000000004Avance: %0.4899999999999949Avance: %0.5Avance: %0.5100000000000051Avance: %0.519999999999996Avance: %0.5300000000000011Avance: %0.539999999999992Avance: %0.5499999999999972Avance: %0.5600000000000023Avance: %0.5700000000000074Avance: %0.5799999999999983Avance: %0.5900000000000034Avance: %0.5999999999999943Avance: %0.6099999999999994Avance: %0.6200000000000045Avance: %0.6299999999999955Avance: %0.6400000000000006Avance: %0.6499999999999915Avance: %0.6600000000000108Avance: %0.6700000000000017Avance: %0.6800000000000068Avance: %0.6899999999999977Avance: %0.7000000000000028Avance: %0.7099999999999937Avance: %0.7199999999999989Avance: %0.730000000000004Avance: %0.7399999999999949Avance: %0.75Avance: %0.7600000000000051Avance: %0.7700000000000102Avance: %0.7800000000000011Avance: %0.7900000000000063Avance: %0.7999999999999972Avance: %0.8100000000000023Avance: %0.8199999999999932Avance: %0.8299999999999983Avance: %0.8400000000000034Avance: %0.8499999999999943Avance: %0.8599999999999994Avance: %0.8700000000000045Avance: %0.8800000000000097Avance: %0.8900000000000006Avance: %0.9000000000000057Avance: %0.9099999999999966Avance: %0.9200000000000017Avance: %0.9299999999999926Avance: %0.9399999999999977Avance: %0.9499999999999886Avance: %0.960000000000008Avance: %0.9699999999999989Avance: %0.980000000000004Avance: %0.9900000000000091Avance: %1.0Avance: %1.0100000000000051Avance: %1.019999999999996Avance: %1.0300000000000011Avance: %1.039999999999992Avance: %1.0499999999999972Avance: %1.0600000000000023Avance: %1.0700000000000074Avance: %1.0799999999999983Avance: %1.0900000000000034Avance: %1.0999999999999943Avance: %1.1099999999999994Avance: %1.1200000000000045Avance: %1.1299999999999955Avance: %1.1400000000000006Avance: %1.1499999999999915Avance: %1.1600000000000108Avance: %1.1700000000000017Avance: %1.1800000000000068Avance: %1.1899999999999977Avance: %1.2000000000000028Avance: %1.2099999999999937Avance: %1.2199999999999989Avance: %1.230000000000004Avance: %1.2399999999999949Avance: %1.25Avance: %1.259999999999991Avance: %1.2700000000000102Avance: %1.2800000000000011Avance: %1.2900000000000063Avance: %1.2999999999999972Avance: %1.3100000000000023Avance: %1.3199999999999932Avance: %1.3299999999999983Avance: %1.3400000000000034Avance: %1.3499999999999943Avance: %1.3599999999999994Avance: %1.3700000000000045Avance: %1.3800000000000097Avance: %1.3900000000000006Avance: %1.4000000000000057Avance: %1.4099999999999966Avance: %1.4200000000000017Avance: %1.4299999999999926Avance: %1.4399999999999977Avance: %1.4499999999999886Avance: %1.4599999999999937Avance: %1.4699999999999989Avance: %1.480000000000004Avance: %1.490000000000009Avance: %1.5Avance: %1.5100000000000051Avance: %1.519999999999996Avance: %1.5300000000000011Avance: %1.539999999999992Avance: %1.5499999999999972Avance: %1.559999999999988Avance: %1.5700000000000074Avance: %1.5799999999999983Avance: %1.5900000000000034Avance: %1.5999999999999943Avance: %1.6099999999999994Avance: %1.6200000000000045Avance: %1.6299999999999955Avance: %1.6400000000000006Avance: %1.6499999999999915Avance: %1.6599999999999966Avance: %1.6700000000000017Avance: %1.6800000000000068Avance: %1.6899999999999977Avance: %1.7000000000000028Avance: %1.7099999999999937Avance: %1.7199999999999989Avance: %1.730000000000004Avance: %1.7399999999999949Avance: %1.75Avance: %1.759999999999991Avance: %1.7700000000000102Avance: %1.7800000000000011Avance: %1.7900000000000063Avance: %1.7999999999999972Avance: %1.8100000000000023Avance: %1.8199999999999932Avance: %1.8299999999999983Avance: %1.8400000000000034Avance: %1.8499999999999943Avance: %1.8599999999999994Avance: %1.8700000000000045Avance: %1.8800000000000097Avance: %1.8900000000000006Avance: %1.9000000000000057Avance: %1.9099999999999966Avance: %1.9200000000000017Avance: %1.9299999999999926Avance: %1.9399999999999977Avance: %1.9500000000000028Avance: %1.9599999999999937Avance: %1.9699999999999989Avance: %1.980000000000004Avance: %1.990000000000009Avance: %2.0Avance: %2.010000000000005Avance: %2.019999999999996Avance: %2.030000000000001Avance: %2.039999999999992Avance: %2.049999999999997Avance: %2.0600000000000023Avance: %2.0700000000000074Avance: %2.0799999999999983Avance: %2.0900000000000034Avance: %2.1000000000000085Avance: %2.1099999999999994Avance: %2.1200000000000045Avance: %2.1299999999999955Avance: %2.1400000000000006Avance: %2.1499999999999915Avance: %2.1599999999999966Avance: %2.1700000000000017Avance: %2.180000000000007Avance: %2.1899999999999977Avance: %2.200000000000003Avance: %2.210000000000008Avance: %2.219999999999999Avance: %2.230000000000004Avance: %2.239999999999995Avance: %2.25Avance: %2.259999999999991Avance: %2.2700000000000102Avance: %2.280000000000001Avance: %2.2900000000000063Avance: %2.299999999999997Avance: %2.3100000000000023Avance: %2.319999999999993Avance: %2.3299999999999983Avance: %2.3400000000000034Avance: %2.3499999999999943Avance: %2.3599999999999994Avance: %2.3700000000000045Avance: %2.3800000000000097Avance: %2.3900000000000006Avance: %2.4000000000000057Avance: %2.4099999999999966Avance: %2.4200000000000017Avance: %2.4299999999999926Avance: %2.4399999999999977Avance: %2.450000000000003Avance: %2.4599999999999937Avance: %2.469999999999999Avance: %2.480000000000004Avance: %2.490000000000009Avance: %2.5Avance: %2.510000000000005Avance: %2.519999999999996Avance: %2.530000000000001Avance: %2.539999999999992Avance: %2.549999999999997Avance: %2.5600000000000023Avance: %2.569999999999993Avance: %2.5799999999999983Avance: %2.5900000000000034Avance: %2.6000000000000085Avance: %2.6099999999999994Avance: %2.6200000000000045Avance: %2.6299999999999955Avance: %2.6400000000000006Avance: %2.6499999999999915Avance: %2.6599999999999966Avance: %2.6699999999999875Avance: %2.680000000000007Avance: %2.6899999999999977Avance: %2.700000000000003Avance: %2.710000000000008Avance: %2.719999999999999Avance: %2.730000000000004Avance: %2.739999999999995Avance: %2.75Avance: %2.759999999999991Avance: %2.769999999999996Avance: %2.780000000000001Avance: %2.7900000000000063Avance: %2.799999999999997Avance: %2.8100000000000023Avance: %2.819999999999993Avance: %2.8299999999999983Avance: %2.8400000000000034Avance: %2.8499999999999943Avance: %2.8599999999999994Avance: %2.8699999999999903Avance: %2.8800000000000097Avance: %2.8900000000000006Avance: %2.9000000000000057Avance: %2.9099999999999966Avance: %2.9200000000000017Avance: %2.9299999999999926Avance: %2.9399999999999977Avance: %2.950000000000003Avance: %2.9599999999999937Avance: %2.969999999999999Avance: %2.980000000000004Avance: %2.990000000000009Avance: %3.0Avance: %3.010000000000005Avance: %3.019999999999996Avance: %3.030000000000001Avance: %3.039999999999992Avance: %3.049999999999997Avance: %3.0600000000000023Avance: %3.069999999999993Avance: %3.0799999999999983Avance: %3.0900000000000034Avance: %3.1000000000000085Avance: %3.1099999999999994Avance: %3.1200000000000045Avance: %3.1299999999999955Avance: %3.1400000000000006Avance: %3.1499999999999915Avance: %3.1599999999999966Avance: %3.1700000000000017Avance: %3.180000000000007Avance: %3.1899999999999977Avance: %3.200000000000003Avance: %3.210000000000008Avance: %3.219999999999999Avance: %3.230000000000004Avance: %3.239999999999995Avance: %3.25Avance: %3.259999999999991Avance: %3.269999999999996Avance: %3.280000000000001Avance: %3.2900000000000063Avance: %3.299999999999997Avance: %3.3100000000000023Avance: %3.3200000000000074Avance: %3.3299999999999983Avance: %3.3400000000000034Avance: %3.3499999999999943Avance: %3.3599999999999994Avance: %3.3699999999999903Avance: %3.3800000000000097Avance: %3.3900000000000006Avance: %3.4000000000000057Avance: %3.4099999999999966Avance: %3.4200000000000017Avance: %3.430000000000007Avance: %3.4399999999999977Avance: %3.450000000000003Avance: %3.4599999999999937Avance: %3.469999999999999Avance: %3.480000000000004Avance: %3.490000000000009Avance: %3.5Avance: %3.510000000000005Avance: %3.519999999999996Avance: %3.530000000000001Avance: %3.539999999999992Avance: %3.549999999999997Avance: %3.5600000000000023Avance: %3.569999999999993Avance: %3.5800000000000125Avance: %3.5900000000000034Avance: %3.6000000000000085Avance: %3.6099999999999994Avance: %3.6200000000000045Avance: %3.6299999999999955Avance: %3.6400000000000006Avance: %3.6499999999999915Avance: %3.6599999999999966Avance: %3.6700000000000017Avance: %3.680000000000007Avance: %3.6899999999999977Avance: %3.700000000000003Avance: %3.710000000000008Avance: %3.719999999999999Avance: %3.730000000000004Avance: %3.739999999999995Avance: %3.75Avance: %3.759999999999991Avance: %3.769999999999996Avance: %3.780000000000001Avance: %3.7900000000000063Avance: %3.799999999999997Avance: %3.8100000000000023Avance: %3.8200000000000074Avance: %3.8299999999999983Avance: %3.8400000000000034Avance: %3.8499999999999943Avance: %3.8599999999999994Avance: %3.8699999999999903Avance: %3.8799999999999955Avance: %3.8900000000000006Avance: %3.9000000000000057Avance: %3.9099999999999966Avance: %3.9200000000000017Avance: %3.930000000000007Avance: %3.9399999999999977Avance: %3.950000000000003Avance: %3.9599999999999937Avance: %3.969999999999999Avance: %3.9799999999999898Avance: %3.990000000000009Avance: %4.0Avance: %4.010000000000005Avance: %4.019999999999996Avance: %4.030000000000001Avance: %4.039999999999992Avance: %4.049999999999997Avance: %4.060000000000002Avance: %4.069999999999993Avance: %4.079999999999998Avance: %4.090000000000003Avance: %4.1000000000000085Avance: %4.109999999999999Avance: %4.1200000000000045Avance: %4.1299999999999955Avance: %4.140000000000001Avance: %4.1499999999999915Avance: %4.159999999999997Avance: %4.170000000000002Avance: %4.179999999999993Avance: %4.189999999999998Avance: %4.200000000000003Avance: %4.210000000000008Avance: %4.219999999999999Avance: %4.230000000000004Avance: %4.239999999999995Avance: %4.25Avance: %4.259999999999991Avance: %4.269999999999996Avance: %4.280000000000001Avance: %4.290000000000006Avance: %4.299999999999997Avance: %4.310000000000002Avance: %4.320000000000007Avance: %4.329999999999998Avance: %4.340000000000003Avance: %4.349999999999994Avance: %4.359999999999999Avance: %4.36999999999999Avance: %4.3799999999999955Avance: %4.390000000000001Avance: %4.400000000000006Avance: %4.409999999999997Avance: %4.420000000000002Avance: %4.430000000000007Avance: %4.439999999999998Avance: %4.450000000000003Avance: %4.459999999999994Avance: %4.469999999999999Avance: %4.47999999999999Avance: %4.490000000000009Avance: %4.5Avance: %4.510000000000005Avance: %4.519999999999996Avance: %4.530000000000001Avance: %4.540000000000006Avance: %4.549999999999997Avance: %4.560000000000002Avance: %4.569999999999993Avance: %4.579999999999998Avance: %4.590000000000003Avance: %4.6000000000000085Avance: %4.609999999999999Avance: %4.6200000000000045Avance: %4.6299999999999955Avance: %4.640000000000001Avance: %4.650000000000006Avance: %4.659999999999997Avance: %4.670000000000002Avance: %4.679999999999993Avance: %4.690000000000012Avance: %4.700000000000003Avance: %4.710000000000008Avance: %4.719999999999999Avance: %4.730000000000004Avance: %4.739999999999995Avance: %4.75Avance: %4.759999999999991Avance: %4.769999999999996Avance: %4.780000000000001Avance: %4.790000000000006Avance: %4.800000000000011Avance: %4.810000000000002Avance: %4.820000000000007Avance: %4.829999999999998Avance: %4.840000000000003Avance: %4.849999999999994Avance: %4.859999999999999Avance: %4.86999999999999Avance: %4.8799999999999955Avance: %4.890000000000001Avance: %4.900000000000006Avance: %4.909999999999997Avance: %4.920000000000002Avance: %4.930000000000007Avance: %4.939999999999998Avance: %4.950000000000003Avance: %4.959999999999994Avance: %4.969999999999999Avance: %4.97999999999999Avance: %4.990000000000009Avance: %5.0Done Extracting Features 0:04:08.343679
The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.
Try some users for user 19722366
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 670) (2000, 670) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6990
        True       1.00      1.00      1.00        10

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1997
        True       1.00      1.00      1.00         3

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 506434809
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 676) (2000, 676) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6906
        True       1.00      1.00      1.00        94

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1973
        True       1.00      1.00      1.00        27

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 35040142
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 658) (2000, 658) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6995
        True       1.00      1.00      1.00         5

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1999
        True       1.00      1.00      1.00         1

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 139792947
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 521) (2000, 521) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6882
        True       1.00      0.98      0.99       118

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1966
        True       1.00      1.00      1.00        34

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 17309892
Loading df
Done loading df. DF shape is :(12397359, 8) (Original: (12397359, 8)) 		Time delta is: None mins
Loading graph from ../data/graphs/latest_subgraph.gpickle
Fetched 745 level 2 neighbourhs for user 17309892.
Len of own retweets (positive examples) is (3, 2)
done getting tweets universe. Shape is  (11172, 2)
	Dataset was truncated to 10000 tweets
Extracting features Optimized. X shape is : (10000, 745)
Avance: %0.010000000000005116Avance: %0.01999999999999602Avance: %0.030000000000001137Avance: %0.03999999999999204Avance: %0.04999999999999716Avance: %0.060000000000002274Avance: %0.07000000000000739Avance: %0.0799999999999983Avance: %0.09000000000000341Avance: %0.09999999999999432Avance: %0.10999999999999943Avance: %0.12000000000000455Avance: %0.12999999999999545Avance: %0.14000000000000057Avance: %0.14999999999999147Avance: %0.1600000000000108Avance: %0.1700000000000017Avance: %0.18000000000000682Avance: %0.18999999999999773Avance: %0.20000000000000284Avance: %0.20999999999999375Avance: %0.21999999999999886Avance: %0.22999999999998977Avance: %0.23999999999999488Avance: %0.25Avance: %0.2600000000000051Avance: %0.27000000000001023Avance: %0.28000000000000114Avance: %0.29000000000000625Avance: %0.29999999999999716Avance: %0.3100000000000023Avance: %0.3199999999999932Avance: %0.3299999999999983Avance: %0.3399999999999892Avance: %0.3499999999999943Avance: %0.35999999999999943Avance: %0.37000000000000455Avance: %0.37999999999999545Avance: %0.39000000000000057Avance: %0.4000000000000057Avance: %0.4099999999999966Avance: %0.4200000000000017Avance: %0.4299999999999926Avance: %0.4399999999999977Avance: %0.44999999999998863Avance: %0.46000000000000796Avance: %0.46999999999999886Avance: %0.480000000000004Avance: %0.4899999999999949Avance: %0.5Avance: %0.5100000000000051Avance: %0.519999999999996Avance: %0.5300000000000011Avance: %0.539999999999992Avance: %0.5499999999999972Avance: %0.5600000000000023Avance: %0.5700000000000074Avance: %0.5799999999999983Avance: %0.5900000000000034Avance: %0.5999999999999943Avance: %0.6099999999999994Avance: %0.6200000000000045Avance: %0.6299999999999955Avance: %0.6400000000000006Avance: %0.6499999999999915Avance: %0.6600000000000108Avance: %0.6700000000000017Avance: %0.6800000000000068Avance: %0.6899999999999977Avance: %0.7000000000000028Avance: %0.7099999999999937Avance: %0.7199999999999989Avance: %0.730000000000004Avance: %0.7399999999999949Avance: %0.75Avance: %0.7600000000000051Avance: %0.7700000000000102Avance: %0.7800000000000011Avance: %0.7900000000000063Avance: %0.7999999999999972Avance: %0.8100000000000023Avance: %0.8199999999999932Avance: %0.8299999999999983Avance: %0.8400000000000034Avance: %0.8499999999999943Avance: %0.8599999999999994Avance: %0.8700000000000045Avance: %0.8800000000000097Avance: %0.8900000000000006Avance: %0.9000000000000057Avance: %0.9099999999999966Avance: %0.9200000000000017Avance: %0.9299999999999926Avance: %0.9399999999999977Avance: %0.9499999999999886Avance: %0.960000000000008Avance: %0.9699999999999989Avance: %0.980000000000004Avance: %0.9900000000000091Avance: %1.0Avance: %1.0100000000000051Avance: %1.019999999999996Avance: %1.0300000000000011Avance: %1.039999999999992Avance: %1.0499999999999972Avance: %1.0600000000000023Avance: %1.0700000000000074Avance: %1.0799999999999983Avance: %1.0900000000000034Avance: %1.0999999999999943Avance: %1.1099999999999994Avance: %1.1200000000000045Avance: %1.1299999999999955Avance: %1.1400000000000006Avance: %1.1499999999999915Avance: %1.1600000000000108Avance: %1.1700000000000017Avance: %1.1800000000000068Avance: %1.1899999999999977Avance: %1.2000000000000028Avance: %1.2099999999999937Avance: %1.2199999999999989Avance: %1.230000000000004Avance: %1.2399999999999949Avance: %1.25Avance: %1.259999999999991Avance: %1.2700000000000102Avance: %1.2800000000000011Avance: %1.2900000000000063Avance: %1.2999999999999972Avance: %1.3100000000000023Avance: %1.3199999999999932Avance: %1.3299999999999983Avance: %1.3400000000000034Avance: %1.3499999999999943Avance: %1.3599999999999994Avance: %1.3700000000000045Avance: %1.3800000000000097Avance: %1.3900000000000006Avance: %1.4000000000000057Avance: %1.4099999999999966Avance: %1.4200000000000017Avance: %1.4299999999999926Avance: %1.4399999999999977Avance: %1.4499999999999886Avance: %1.4599999999999937Avance: %1.4699999999999989Avance: %1.480000000000004Avance: %1.490000000000009Avance: %1.5Avance: %1.5100000000000051Avance: %1.519999999999996Avance: %1.5300000000000011Avance: %1.539999999999992Avance: %1.5499999999999972Avance: %1.559999999999988Avance: %1.5700000000000074Avance: %1.5799999999999983Avance: %1.5900000000000034Avance: %1.5999999999999943Avance: %1.6099999999999994Avance: %1.6200000000000045Avance: %1.6299999999999955Avance: %1.6400000000000006Avance: %1.6499999999999915Avance: %1.6599999999999966Avance: %1.6700000000000017Avance: %1.6800000000000068Avance: %1.6899999999999977Avance: %1.7000000000000028Avance: %1.7099999999999937Avance: %1.7199999999999989Avance: %1.730000000000004Avance: %1.7399999999999949Avance: %1.75Avance: %1.759999999999991Avance: %1.7700000000000102Avance: %1.7800000000000011Avance: %1.7900000000000063Avance: %1.7999999999999972Avance: %1.8100000000000023Avance: %1.8199999999999932Avance: %1.8299999999999983Avance: %1.8400000000000034Avance: %1.8499999999999943Avance: %1.8599999999999994Avance: %1.8700000000000045Avance: %1.8800000000000097Avance: %1.8900000000000006Avance: %1.9000000000000057Avance: %1.9099999999999966Avance: %1.9200000000000017Avance: %1.9299999999999926Avance: %1.9399999999999977Avance: %1.9500000000000028Avance: %1.9599999999999937Avance: %1.9699999999999989Avance: %1.980000000000004Avance: %1.990000000000009Avance: %2.0Avance: %2.010000000000005Avance: %2.019999999999996Avance: %2.030000000000001Avance: %2.039999999999992Avance: %2.049999999999997Avance: %2.0600000000000023Avance: %2.0700000000000074Avance: %2.0799999999999983Avance: %2.0900000000000034Avance: %2.1000000000000085Avance: %2.1099999999999994Avance: %2.1200000000000045Avance: %2.1299999999999955Avance: %2.1400000000000006Avance: %2.1499999999999915Avance: %2.1599999999999966Avance: %2.1700000000000017Avance: %2.180000000000007Avance: %2.1899999999999977Avance: %2.200000000000003Avance: %2.210000000000008Avance: %2.219999999999999Avance: %2.230000000000004Avance: %2.239999999999995Avance: %2.25Avance: %2.259999999999991Avance: %2.2700000000000102Avance: %2.280000000000001Avance: %2.2900000000000063Avance: %2.299999999999997Avance: %2.3100000000000023Avance: %2.319999999999993Avance: %2.3299999999999983Avance: %2.3400000000000034Avance: %2.3499999999999943Avance: %2.3599999999999994Avance: %2.3700000000000045Avance: %2.3800000000000097Avance: %2.3900000000000006Avance: %2.4000000000000057Avance: %2.4099999999999966Avance: %2.4200000000000017Avance: %2.4299999999999926Avance: %2.4399999999999977Avance: %2.450000000000003Avance: %2.4599999999999937Avance: %2.469999999999999Avance: %2.480000000000004Avance: %2.490000000000009Avance: %2.5Avance: %2.510000000000005Avance: %2.519999999999996Avance: %2.530000000000001Avance: %2.539999999999992Avance: %2.549999999999997Avance: %2.5600000000000023Avance: %2.569999999999993Avance: %2.5799999999999983Avance: %2.5900000000000034Avance: %2.6000000000000085Avance: %2.6099999999999994Avance: %2.6200000000000045Avance: %2.6299999999999955Avance: %2.6400000000000006Avance: %2.6499999999999915Avance: %2.6599999999999966Avance: %2.6699999999999875Avance: %2.680000000000007Avance: %2.6899999999999977Avance: %2.700000000000003Avance: %2.710000000000008Avance: %2.719999999999999Avance: %2.730000000000004Avance: %2.739999999999995Avance: %2.75Avance: %2.759999999999991Avance: %2.769999999999996Avance: %2.780000000000001Avance: %2.7900000000000063Avance: %2.799999999999997Avance: %2.8100000000000023Avance: %2.819999999999993Avance: %2.8299999999999983Avance: %2.8400000000000034Avance: %2.8499999999999943Avance: %2.8599999999999994Avance: %2.8699999999999903Avance: %2.8800000000000097Avance: %2.8900000000000006Avance: %2.9000000000000057Avance: %2.9099999999999966Avance: %2.9200000000000017Avance: %2.9299999999999926Avance: %2.9399999999999977Avance: %2.950000000000003Avance: %2.9599999999999937Avance: %2.969999999999999Avance: %2.980000000000004Avance: %2.990000000000009Avance: %3.0Avance: %3.010000000000005Avance: %3.019999999999996Avance: %3.030000000000001Avance: %3.039999999999992Avance: %3.049999999999997Avance: %3.0600000000000023Avance: %3.069999999999993Avance: %3.0799999999999983Avance: %3.0900000000000034Avance: %3.1000000000000085Avance: %3.1099999999999994Avance: %3.1200000000000045Avance: %3.1299999999999955Avance: %3.1400000000000006Avance: %3.1499999999999915Avance: %3.1599999999999966Avance: %3.1700000000000017Avance: %3.180000000000007Avance: %3.1899999999999977Avance: %3.200000000000003Avance: %3.210000000000008Avance: %3.219999999999999Avance: %3.230000000000004Avance: %3.239999999999995Avance: %3.25Avance: %3.259999999999991Avance: %3.269999999999996Avance: %3.280000000000001Avance: %3.2900000000000063Avance: %3.299999999999997Avance: %3.3100000000000023Avance: %3.3200000000000074Avance: %3.3299999999999983Avance: %3.3400000000000034Avance: %3.3499999999999943Avance: %3.3599999999999994Avance: %3.3699999999999903Avance: %3.3800000000000097Avance: %3.3900000000000006Avance: %3.4000000000000057Avance: %3.4099999999999966Avance: %3.4200000000000017Avance: %3.430000000000007Avance: %3.4399999999999977Avance: %3.450000000000003Avance: %3.4599999999999937Avance: %3.469999999999999Avance: %3.480000000000004Avance: %3.490000000000009Avance: %3.5Avance: %3.510000000000005Avance: %3.519999999999996Avance: %3.530000000000001Avance: %3.539999999999992Avance: %3.549999999999997Avance: %3.5600000000000023Avance: %3.569999999999993Avance: %3.5800000000000125Avance: %3.5900000000000034Avance: %3.6000000000000085Avance: %3.6099999999999994Avance: %3.6200000000000045Avance: %3.6299999999999955Avance: %3.6400000000000006Avance: %3.6499999999999915Avance: %3.6599999999999966Avance: %3.6700000000000017Avance: %3.680000000000007Avance: %3.6899999999999977Avance: %3.700000000000003Avance: %3.710000000000008Avance: %3.719999999999999Avance: %3.730000000000004Avance: %3.739999999999995Avance: %3.75Avance: %3.759999999999991Avance: %3.769999999999996Avance: %3.780000000000001Avance: %3.7900000000000063Avance: %3.799999999999997Avance: %3.8100000000000023Avance: %3.8200000000000074Avance: %3.8299999999999983Avance: %3.8400000000000034Avance: %3.8499999999999943Avance: %3.8599999999999994Avance: %3.8699999999999903Avance: %3.8799999999999955Avance: %3.8900000000000006Avance: %3.9000000000000057Avance: %3.9099999999999966Avance: %3.9200000000000017Avance: %3.930000000000007Avance: %3.9399999999999977Avance: %3.950000000000003Avance: %3.9599999999999937Avance: %3.969999999999999Avance: %3.9799999999999898Avance: %3.990000000000009Avance: %4.0Avance: %4.010000000000005Avance: %4.019999999999996Avance: %4.030000000000001Avance: %4.039999999999992Avance: %4.049999999999997Avance: %4.060000000000002Avance: %4.069999999999993Avance: %4.079999999999998Avance: %4.090000000000003Avance: %4.1000000000000085Avance: %4.109999999999999Avance: %4.1200000000000045Avance: %4.1299999999999955Avance: %4.140000000000001Avance: %4.1499999999999915Avance: %4.159999999999997Avance: %4.170000000000002Avance: %4.179999999999993Avance: %4.189999999999998Avance: %4.200000000000003Avance: %4.210000000000008Avance: %4.219999999999999Avance: %4.230000000000004Avance: %4.239999999999995Avance: %4.25Avance: %4.259999999999991Avance: %4.269999999999996Avance: %4.280000000000001Avance: %4.290000000000006Avance: %4.299999999999997Avance: %4.310000000000002Avance: %4.320000000000007Avance: %4.329999999999998Avance: %4.340000000000003Avance: %4.349999999999994Avance: %4.359999999999999Avance: %4.36999999999999Avance: %4.3799999999999955Avance: %4.390000000000001Avance: %4.400000000000006Avance: %4.409999999999997Avance: %4.420000000000002Avance: %4.430000000000007Avance: %4.439999999999998Avance: %4.450000000000003Avance: %4.459999999999994Avance: %4.469999999999999Avance: %4.47999999999999Avance: %4.490000000000009Avance: %4.5Avance: %4.510000000000005Avance: %4.519999999999996Avance: %4.530000000000001Avance: %4.540000000000006Avance: %4.549999999999997Avance: %4.560000000000002Avance: %4.569999999999993Avance: %4.579999999999998Avance: %4.590000000000003Avance: %4.6000000000000085Avance: %4.609999999999999Avance: %4.6200000000000045Avance: %4.6299999999999955Avance: %4.640000000000001Avance: %4.650000000000006Avance: %4.659999999999997Avance: %4.670000000000002Avance: %4.679999999999993Avance: %4.690000000000012Avance: %4.700000000000003Avance: %4.710000000000008Avance: %4.719999999999999Avance: %4.730000000000004Avance: %4.739999999999995Avance: %4.75Avance: %4.759999999999991Avance: %4.769999999999996Avance: %4.780000000000001Avance: %4.790000000000006Avance: %4.800000000000011Avance: %4.810000000000002Avance: %4.820000000000007Avance: %4.829999999999998Avance: %4.840000000000003Avance: %4.849999999999994Avance: %4.859999999999999Avance: %4.86999999999999Avance: %4.8799999999999955Avance: %4.890000000000001Avance: %4.900000000000006Avance: %4.909999999999997Avance: %4.920000000000002Avance: %4.930000000000007Avance: %4.939999999999998Avance: %4.950000000000003Avance: %4.959999999999994Avance: %4.969999999999999Avance: %4.97999999999999Avance: %4.990000000000009Avance: %5.0Avance: %5.010000000000005Avance: %5.019999999999996Avance: %5.030000000000001Avance: %5.040000000000006Avance: %5.049999999999997Avance: %5.060000000000002Avance: %5.069999999999993Avance: %5.079999999999998Avance: %5.089999999999989Avance: %5.1000000000000085Avance: %5.109999999999999Avance: %5.1200000000000045Avance: %5.1299999999999955Avance: %5.140000000000001Avance: %5.150000000000006Avance: %5.159999999999997Avance: %5.170000000000002Avance: %5.179999999999993Avance: %5.189999999999998Avance: %5.200000000000003Avance: %5.210000000000008Avance: %5.219999999999999Avance: %5.230000000000004Avance: %5.239999999999995Avance: %5.25Avance: %5.259999999999991Avance: %5.269999999999996Avance: %5.280000000000001Avance: %5.289999999999992Avance: %5.300000000000011Avance: %5.310000000000002Avance: %5.320000000000007Avance: %5.329999999999998Avance: %5.340000000000003Avance: %5.349999999999994Avance: %5.359999999999999Avance: %5.36999999999999Avance: %5.3799999999999955Avance: %5.390000000000001Avance: %5.400000000000006Avance: %5.409999999999997Avance: %5.420000000000002Avance: %5.430000000000007Avance: %5.439999999999998Avance: %5.450000000000003Avance: %5.459999999999994Avance: %5.469999999999999Avance: %5.47999999999999Avance: %5.489999999999995Avance: %5.5Avance: %5.510000000000005Avance: %5.519999999999996Avance: %5.530000000000001Avance: %5.540000000000006Avance: %5.549999999999997Avance: %5.560000000000002Avance: %5.569999999999993Avance: %5.579999999999998Avance: %5.589999999999989Avance: %5.6000000000000085Avance: %5.609999999999999Avance: %5.6200000000000045Avance: %5.6299999999999955Avance: %5.640000000000001Avance: %5.650000000000006Avance: %5.659999999999997Avance: %5.670000000000002Avance: %5.679999999999993Avance: %5.689999999999998Avance: %5.700000000000003Avance: %5.710000000000008Avance: %5.719999999999999Avance: %5.730000000000004Avance: %5.739999999999995Avance: %5.75Avance: %5.760000000000005Avance: %5.769999999999996Avance: %5.780000000000001Avance: %5.789999999999992Avance: %5.800000000000011Avance: %5.810000000000002Avance: %5.820000000000007Avance: %5.829999999999998Avance: %5.840000000000003Avance: %5.849999999999994Avance: %5.859999999999999Avance: %5.8700000000000045Avance: %5.8799999999999955Avance: %5.890000000000001Avance: %5.900000000000006Avance: %5.910000000000011Avance: %5.920000000000002Avance: %5.930000000000007Avance: %5.939999999999998Avance: %5.950000000000003Avance: %5.959999999999994Avance: %5.969999999999999Avance: %5.97999999999999Avance: %5.989999999999995Avance: %6.0Avance: %6.010000000000005Avance: %6.02000000000001Avance: %6.030000000000001Avance: %6.040000000000006Avance: %6.049999999999997Avance: %6.060000000000002Avance: %6.069999999999993Avance: %6.079999999999998Avance: %6.089999999999989Avance: %6.1000000000000085Avance: %6.109999999999999Avance: %6.1200000000000045Avance: %6.1299999999999955Avance: %6.140000000000001Avance: %6.150000000000006Avance: %6.159999999999997Avance: %6.170000000000002Avance: %6.179999999999993Avance: %6.189999999999998Avance: %6.200000000000003Avance: %6.210000000000008Avance: %6.219999999999999Avance: %6.230000000000004Avance: %6.239999999999995Avance: %6.25Avance: %6.260000000000005Avance: %6.269999999999996Avance: %6.280000000000001Avance: %6.289999999999992Avance: %6.299999999999997Avance: %6.310000000000002Avance: %6.320000000000007Avance: %6.329999999999998Avance: %6.340000000000003Avance: %6.349999999999994Avance: %6.359999999999999Avance: %6.3700000000000045Avance: %6.3799999999999955Avance: %6.390000000000001Avance: %6.3999999999999915Avance: %6.410000000000011Avance: %6.420000000000002Avance: %6.430000000000007Avance: %6.439999999999998Avance: %6.450000000000003Avance: %6.459999999999994Avance: %6.469999999999999Avance: %6.47999999999999Avance: %6.489999999999995Avance: %6.5Avance: %6.510000000000005Avance: %6.52000000000001Avance: %6.530000000000001Avance: %6.540000000000006Avance: %6.549999999999997Avance: %6.560000000000002Avance: %6.569999999999993Avance: %6.579999999999998Avance: %6.589999999999989Avance: %6.599999999999994Avance: %6.609999999999999Avance: %6.6200000000000045Avance: %6.6299999999999955Avance: %6.640000000000001Avance: %6.650000000000006Avance: %6.659999999999997Avance: %6.670000000000002Avance: %6.679999999999993Avance: %6.689999999999998Avance: %6.699999999999989Avance: %6.710000000000008Avance: %6.719999999999999Avance: %6.730000000000004Avance: %6.739999999999995Avance: %6.75Avance: %6.760000000000005Avance: %6.769999999999996Avance: %6.780000000000001Avance: %6.789999999999992Avance: %6.799999999999997Avance: %6.810000000000002Avance: %6.820000000000007Avance: %6.829999999999998Avance: %6.840000000000003Avance: %6.849999999999994Avance: %6.859999999999999Avance: %6.8700000000000045Avance: %6.8799999999999955Avance: %6.890000000000001Avance: %6.8999999999999915Avance: %6.910000000000011Avance: %6.920000000000002Avance: %6.930000000000007Avance: %6.939999999999998Avance: %6.950000000000003Avance: %6.959999999999994Avance: %6.969999999999999Avance: %6.980000000000004Avance: %6.989999999999995Avance: %7.0Avance: %7.010000000000005Avance: %7.02000000000001Avance: %7.030000000000001Avance: %7.040000000000006Avance: %7.049999999999997Avance: %7.060000000000002Avance: %7.069999999999993Avance: %7.079999999999998Avance: %7.090000000000003Avance: %7.099999999999994Avance: %7.109999999999999Avance: %7.1200000000000045Avance: %7.13000000000001Avance: %7.140000000000001Avance: %7.150000000000006Avance: %7.159999999999997Avance: %7.170000000000002Avance: %7.179999999999993Avance: %7.189999999999998Avance: %7.199999999999989Avance: %7.210000000000008Avance: %7.219999999999999Avance: %7.230000000000004Avance: %7.240000000000009Avance: %7.25Avance: %7.260000000000005Avance: %7.269999999999996Avance: %7.280000000000001Avance: %7.289999999999992Avance: %7.299999999999997Avance: %7.310000000000002Avance: %7.320000000000007Avance: %7.329999999999998Avance: %7.340000000000003Avance: %7.349999999999994Avance: %7.359999999999999Avance: %7.3700000000000045Avance: %7.3799999999999955Avance: %7.390000000000001Avance: %7.3999999999999915Avance: %7.410000000000011Avance: %7.420000000000002Avance: %7.430000000000007Avance: %7.439999999999998Avance: %7.450000000000003Done Extracting Features 0:06:14.597180
The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.
Try some users for user 164791463
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 370) (2000, 370) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6916
        True       1.00      0.96      0.98        84

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.98      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1976
        True       1.00      1.00      1.00        24

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 189278621
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 407) (2000, 407) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6735
        True       1.00      1.00      1.00       265

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1925
        True       1.00      1.00      1.00        75

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 42046651
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 363) (2000, 363) False False
The number of classes has to be greater than one; got 1 class
Try some users for user 7658762
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 454) (2000, 454) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6896
        True       1.00      0.99      1.00       104

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1971
        True       1.00      1.00      1.00        29

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 71013695
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 785) (2000, 785) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6920
        True       1.00      0.99      0.99        80

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1977
        True       1.00      1.00      1.00        23

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 17960797
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 507) (2000, 507) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6842
        True       1.00      1.00      1.00       158

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1955
        True       1.00      0.98      0.99        45

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.99      0.99      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 126746564
Loading df
Done loading df. DF shape is :(12397359, 8) (Original: (12397359, 8)) 		Time delta is: None mins
Loading graph from ../data/graphs/latest_subgraph.gpickle
Fetched 749 level 2 neighbourhs for user 126746564.
Len of own retweets (positive examples) is (1, 2)
done getting tweets universe. Shape is  (11000, 2)
	Dataset was truncated to 10000 tweets
Extracting features Optimized. X shape is : (10000, 749)
Avance: %0.010000000000005116Avance: %0.01999999999999602Avance: %0.030000000000001137Avance: %0.03999999999999204Avance: %0.04999999999999716Avance: %0.060000000000002274Avance: %0.07000000000000739Avance: %0.0799999999999983Avance: %0.09000000000000341Avance: %0.09999999999999432Avance: %0.10999999999999943Avance: %0.12000000000000455Avance: %0.12999999999999545Avance: %0.14000000000000057Avance: %0.14999999999999147Avance: %0.1600000000000108Avance: %0.1700000000000017Avance: %0.18000000000000682Avance: %0.18999999999999773Avance: %0.20000000000000284Avance: %0.20999999999999375Avance: %0.21999999999999886Avance: %0.22999999999998977Avance: %0.23999999999999488Avance: %0.25Avance: %0.2600000000000051Avance: %0.27000000000001023Avance: %0.28000000000000114Avance: %0.29000000000000625Avance: %0.29999999999999716Avance: %0.3100000000000023Avance: %0.3199999999999932Avance: %0.3299999999999983Avance: %0.3399999999999892Avance: %0.3499999999999943Avance: %0.35999999999999943Avance: %0.37000000000000455Avance: %0.37999999999999545Avance: %0.39000000000000057Avance: %0.4000000000000057Avance: %0.4099999999999966Avance: %0.4200000000000017Avance: %0.4299999999999926Avance: %0.4399999999999977Avance: %0.44999999999998863Avance: %0.46000000000000796Avance: %0.46999999999999886Avance: %0.480000000000004Avance: %0.4899999999999949Avance: %0.5Avance: %0.5100000000000051Avance: %0.519999999999996Avance: %0.5300000000000011Avance: %0.539999999999992Avance: %0.5499999999999972Avance: %0.5600000000000023Avance: %0.5700000000000074Avance: %0.5799999999999983Avance: %0.5900000000000034Avance: %0.5999999999999943Avance: %0.6099999999999994Avance: %0.6200000000000045Avance: %0.6299999999999955Avance: %0.6400000000000006Avance: %0.6499999999999915Avance: %0.6600000000000108Avance: %0.6700000000000017Avance: %0.6800000000000068Avance: %0.6899999999999977Avance: %0.7000000000000028Avance: %0.7099999999999937Avance: %0.7199999999999989Avance: %0.730000000000004Avance: %0.7399999999999949Avance: %0.75Avance: %0.7600000000000051Avance: %0.7700000000000102Avance: %0.7800000000000011Avance: %0.7900000000000063Avance: %0.7999999999999972Avance: %0.8100000000000023Avance: %0.8199999999999932Avance: %0.8299999999999983Avance: %0.8400000000000034Avance: %0.8499999999999943Avance: %0.8599999999999994Avance: %0.8700000000000045Avance: %0.8800000000000097Avance: %0.8900000000000006Avance: %0.9000000000000057Avance: %0.9099999999999966Avance: %0.9200000000000017Avance: %0.9299999999999926Avance: %0.9399999999999977Avance: %0.9499999999999886Avance: %0.960000000000008Avance: %0.9699999999999989Avance: %0.980000000000004Avance: %0.9900000000000091Avance: %1.0Avance: %1.0100000000000051Avance: %1.019999999999996Avance: %1.0300000000000011Avance: %1.039999999999992Avance: %1.0499999999999972Avance: %1.0600000000000023Avance: %1.0700000000000074Avance: %1.0799999999999983Avance: %1.0900000000000034Avance: %1.0999999999999943Avance: %1.1099999999999994Avance: %1.1200000000000045Avance: %1.1299999999999955Avance: %1.1400000000000006Avance: %1.1499999999999915Avance: %1.1600000000000108Avance: %1.1700000000000017Avance: %1.1800000000000068Avance: %1.1899999999999977Avance: %1.2000000000000028Avance: %1.2099999999999937Avance: %1.2199999999999989Avance: %1.230000000000004Avance: %1.2399999999999949Avance: %1.25Avance: %1.259999999999991Avance: %1.2700000000000102Avance: %1.2800000000000011Avance: %1.2900000000000063Avance: %1.2999999999999972Avance: %1.3100000000000023Avance: %1.3199999999999932Avance: %1.3299999999999983Avance: %1.3400000000000034Avance: %1.3499999999999943Avance: %1.3599999999999994Avance: %1.3700000000000045Avance: %1.3800000000000097Avance: %1.3900000000000006Avance: %1.4000000000000057Avance: %1.4099999999999966Avance: %1.4200000000000017Avance: %1.4299999999999926Avance: %1.4399999999999977Avance: %1.4499999999999886Avance: %1.4599999999999937Avance: %1.4699999999999989Avance: %1.480000000000004Avance: %1.490000000000009Avance: %1.5Avance: %1.5100000000000051Avance: %1.519999999999996Avance: %1.5300000000000011Avance: %1.539999999999992Avance: %1.5499999999999972Avance: %1.559999999999988Avance: %1.5700000000000074Avance: %1.5799999999999983Avance: %1.5900000000000034Avance: %1.5999999999999943Avance: %1.6099999999999994Avance: %1.6200000000000045Avance: %1.6299999999999955Avance: %1.6400000000000006Avance: %1.6499999999999915Avance: %1.6599999999999966Avance: %1.6700000000000017Avance: %1.6800000000000068Avance: %1.6899999999999977Avance: %1.7000000000000028Avance: %1.7099999999999937Avance: %1.7199999999999989Avance: %1.730000000000004Avance: %1.7399999999999949Avance: %1.75Avance: %1.759999999999991Avance: %1.7700000000000102Avance: %1.7800000000000011Avance: %1.7900000000000063Avance: %1.7999999999999972Avance: %1.8100000000000023Avance: %1.8199999999999932Avance: %1.8299999999999983Avance: %1.8400000000000034Avance: %1.8499999999999943Avance: %1.8599999999999994Avance: %1.8700000000000045Avance: %1.8800000000000097Avance: %1.8900000000000006Avance: %1.9000000000000057Avance: %1.9099999999999966Avance: %1.9200000000000017Avance: %1.9299999999999926Avance: %1.9399999999999977Avance: %1.9500000000000028Avance: %1.9599999999999937Avance: %1.9699999999999989Avance: %1.980000000000004Avance: %1.990000000000009Avance: %2.0Avance: %2.010000000000005Avance: %2.019999999999996Avance: %2.030000000000001Avance: %2.039999999999992Avance: %2.049999999999997Avance: %2.0600000000000023Avance: %2.0700000000000074Avance: %2.0799999999999983Avance: %2.0900000000000034Avance: %2.1000000000000085Avance: %2.1099999999999994Avance: %2.1200000000000045Avance: %2.1299999999999955Avance: %2.1400000000000006Avance: %2.1499999999999915Avance: %2.1599999999999966Avance: %2.1700000000000017Avance: %2.180000000000007Avance: %2.1899999999999977Avance: %2.200000000000003Avance: %2.210000000000008Avance: %2.219999999999999Avance: %2.230000000000004Avance: %2.239999999999995Avance: %2.25Avance: %2.259999999999991Avance: %2.2700000000000102Avance: %2.280000000000001Avance: %2.2900000000000063Avance: %2.299999999999997Avance: %2.3100000000000023Avance: %2.319999999999993Avance: %2.3299999999999983Avance: %2.3400000000000034Avance: %2.3499999999999943Avance: %2.3599999999999994Avance: %2.3700000000000045Avance: %2.3800000000000097Avance: %2.3900000000000006Avance: %2.4000000000000057Avance: %2.4099999999999966Avance: %2.4200000000000017Avance: %2.4299999999999926Avance: %2.4399999999999977Avance: %2.450000000000003Avance: %2.4599999999999937Avance: %2.469999999999999Avance: %2.480000000000004Avance: %2.490000000000009Avance: %2.5Avance: %2.510000000000005Avance: %2.519999999999996Avance: %2.530000000000001Avance: %2.539999999999992Avance: %2.549999999999997Avance: %2.5600000000000023Avance: %2.569999999999993Avance: %2.5799999999999983Avance: %2.5900000000000034Avance: %2.6000000000000085Avance: %2.6099999999999994Avance: %2.6200000000000045Avance: %2.6299999999999955Avance: %2.6400000000000006Avance: %2.6499999999999915Avance: %2.6599999999999966Avance: %2.6699999999999875Avance: %2.680000000000007Avance: %2.6899999999999977Avance: %2.700000000000003Avance: %2.710000000000008Avance: %2.719999999999999Avance: %2.730000000000004Avance: %2.739999999999995Avance: %2.75Avance: %2.759999999999991Avance: %2.769999999999996Avance: %2.780000000000001Avance: %2.7900000000000063Avance: %2.799999999999997Avance: %2.8100000000000023Avance: %2.819999999999993Avance: %2.8299999999999983Avance: %2.8400000000000034Avance: %2.8499999999999943Avance: %2.8599999999999994Avance: %2.8699999999999903Avance: %2.8800000000000097Avance: %2.8900000000000006Avance: %2.9000000000000057Avance: %2.9099999999999966Avance: %2.9200000000000017Avance: %2.9299999999999926Avance: %2.9399999999999977Avance: %2.950000000000003Avance: %2.9599999999999937Avance: %2.969999999999999Avance: %2.980000000000004Avance: %2.990000000000009Avance: %3.0Avance: %3.010000000000005Avance: %3.019999999999996Avance: %3.030000000000001Avance: %3.039999999999992Avance: %3.049999999999997Avance: %3.0600000000000023Avance: %3.069999999999993Avance: %3.0799999999999983Avance: %3.0900000000000034Avance: %3.1000000000000085Avance: %3.1099999999999994Avance: %3.1200000000000045Avance: %3.1299999999999955Avance: %3.1400000000000006Avance: %3.1499999999999915Avance: %3.1599999999999966Avance: %3.1700000000000017Avance: %3.180000000000007Avance: %3.1899999999999977Avance: %3.200000000000003Avance: %3.210000000000008Avance: %3.219999999999999Avance: %3.230000000000004Avance: %3.239999999999995Avance: %3.25Avance: %3.259999999999991Avance: %3.269999999999996Avance: %3.280000000000001Avance: %3.2900000000000063Avance: %3.299999999999997Avance: %3.3100000000000023Avance: %3.3200000000000074Avance: %3.3299999999999983Avance: %3.3400000000000034Avance: %3.3499999999999943Avance: %3.3599999999999994Avance: %3.3699999999999903Avance: %3.3800000000000097Avance: %3.3900000000000006Avance: %3.4000000000000057Avance: %3.4099999999999966Avance: %3.4200000000000017Avance: %3.430000000000007Avance: %3.4399999999999977Avance: %3.450000000000003Avance: %3.4599999999999937Avance: %3.469999999999999Avance: %3.480000000000004Avance: %3.490000000000009Avance: %3.5Avance: %3.510000000000005Avance: %3.519999999999996Avance: %3.530000000000001Avance: %3.539999999999992Avance: %3.549999999999997Avance: %3.5600000000000023Avance: %3.569999999999993Avance: %3.5800000000000125Avance: %3.5900000000000034Avance: %3.6000000000000085Avance: %3.6099999999999994Avance: %3.6200000000000045Avance: %3.6299999999999955Avance: %3.6400000000000006Avance: %3.6499999999999915Avance: %3.6599999999999966Avance: %3.6700000000000017Avance: %3.680000000000007Avance: %3.6899999999999977Avance: %3.700000000000003Avance: %3.710000000000008Avance: %3.719999999999999Avance: %3.730000000000004Avance: %3.739999999999995Avance: %3.75Avance: %3.759999999999991Avance: %3.769999999999996Avance: %3.780000000000001Avance: %3.7900000000000063Avance: %3.799999999999997Avance: %3.8100000000000023Avance: %3.8200000000000074Avance: %3.8299999999999983Avance: %3.8400000000000034Avance: %3.8499999999999943Avance: %3.8599999999999994Avance: %3.8699999999999903Avance: %3.8799999999999955Avance: %3.8900000000000006Avance: %3.9000000000000057Avance: %3.9099999999999966Avance: %3.9200000000000017Avance: %3.930000000000007Avance: %3.9399999999999977Avance: %3.950000000000003Avance: %3.9599999999999937Avance: %3.969999999999999Avance: %3.9799999999999898Avance: %3.990000000000009Avance: %4.0Avance: %4.010000000000005Avance: %4.019999999999996Avance: %4.030000000000001Avance: %4.039999999999992Avance: %4.049999999999997Avance: %4.060000000000002Avance: %4.069999999999993Avance: %4.079999999999998Avance: %4.090000000000003Avance: %4.1000000000000085Avance: %4.109999999999999Avance: %4.1200000000000045Avance: %4.1299999999999955Avance: %4.140000000000001Avance: %4.1499999999999915Avance: %4.159999999999997Avance: %4.170000000000002Avance: %4.179999999999993Avance: %4.189999999999998Avance: %4.200000000000003Avance: %4.210000000000008Avance: %4.219999999999999Avance: %4.230000000000004Avance: %4.239999999999995Avance: %4.25Avance: %4.259999999999991Avance: %4.269999999999996Avance: %4.280000000000001Avance: %4.290000000000006Avance: %4.299999999999997Avance: %4.310000000000002Avance: %4.320000000000007Avance: %4.329999999999998Avance: %4.340000000000003Avance: %4.349999999999994Avance: %4.359999999999999Avance: %4.36999999999999Avance: %4.3799999999999955Avance: %4.390000000000001Avance: %4.400000000000006Avance: %4.409999999999997Avance: %4.420000000000002Avance: %4.430000000000007Avance: %4.439999999999998Avance: %4.450000000000003Avance: %4.459999999999994Avance: %4.469999999999999Avance: %4.47999999999999Avance: %4.490000000000009Avance: %4.5Avance: %4.510000000000005Avance: %4.519999999999996Avance: %4.530000000000001Avance: %4.540000000000006Avance: %4.549999999999997Avance: %4.560000000000002Avance: %4.569999999999993Avance: %4.579999999999998Avance: %4.590000000000003Avance: %4.6000000000000085Avance: %4.609999999999999Avance: %4.6200000000000045Avance: %4.6299999999999955Avance: %4.640000000000001Avance: %4.650000000000006Avance: %4.659999999999997Avance: %4.670000000000002Avance: %4.679999999999993Avance: %4.690000000000012Avance: %4.700000000000003Avance: %4.710000000000008Avance: %4.719999999999999Avance: %4.730000000000004Avance: %4.739999999999995Avance: %4.75Avance: %4.759999999999991Avance: %4.769999999999996Avance: %4.780000000000001Avance: %4.790000000000006Avance: %4.800000000000011Avance: %4.810000000000002Avance: %4.820000000000007Avance: %4.829999999999998Avance: %4.840000000000003Avance: %4.849999999999994Avance: %4.859999999999999Avance: %4.86999999999999Avance: %4.8799999999999955Avance: %4.890000000000001Avance: %4.900000000000006Avance: %4.909999999999997Avance: %4.920000000000002Avance: %4.930000000000007Avance: %4.939999999999998Avance: %4.950000000000003Avance: %4.959999999999994Avance: %4.969999999999999Avance: %4.97999999999999Avance: %4.990000000000009Avance: %5.0Avance: %5.010000000000005Avance: %5.019999999999996Avance: %5.030000000000001Avance: %5.040000000000006Avance: %5.049999999999997Avance: %5.060000000000002Avance: %5.069999999999993Avance: %5.079999999999998Avance: %5.089999999999989Avance: %5.1000000000000085Avance: %5.109999999999999Avance: %5.1200000000000045Avance: %5.1299999999999955Avance: %5.140000000000001Avance: %5.150000000000006Avance: %5.159999999999997Avance: %5.170000000000002Avance: %5.179999999999993Avance: %5.189999999999998Avance: %5.200000000000003Avance: %5.210000000000008Avance: %5.219999999999999Avance: %5.230000000000004Avance: %5.239999999999995Avance: %5.25Avance: %5.259999999999991Avance: %5.269999999999996Avance: %5.280000000000001Avance: %5.289999999999992Avance: %5.300000000000011Avance: %5.310000000000002Avance: %5.320000000000007Avance: %5.329999999999998Avance: %5.340000000000003Avance: %5.349999999999994Avance: %5.359999999999999Avance: %5.36999999999999Avance: %5.3799999999999955Avance: %5.390000000000001Avance: %5.400000000000006Avance: %5.409999999999997Avance: %5.420000000000002Avance: %5.430000000000007Avance: %5.439999999999998Avance: %5.450000000000003Avance: %5.459999999999994Avance: %5.469999999999999Avance: %5.47999999999999Avance: %5.489999999999995Avance: %5.5Avance: %5.510000000000005Avance: %5.519999999999996Avance: %5.530000000000001Avance: %5.540000000000006Avance: %5.549999999999997Avance: %5.560000000000002Avance: %5.569999999999993Avance: %5.579999999999998Avance: %5.589999999999989Avance: %5.6000000000000085Avance: %5.609999999999999Avance: %5.6200000000000045Avance: %5.6299999999999955Avance: %5.640000000000001Avance: %5.650000000000006Avance: %5.659999999999997Avance: %5.670000000000002Avance: %5.679999999999993Avance: %5.689999999999998Avance: %5.700000000000003Avance: %5.710000000000008Avance: %5.719999999999999Avance: %5.730000000000004Avance: %5.739999999999995Avance: %5.75Avance: %5.760000000000005Avance: %5.769999999999996Avance: %5.780000000000001Avance: %5.789999999999992Avance: %5.800000000000011Avance: %5.810000000000002Avance: %5.820000000000007Avance: %5.829999999999998Avance: %5.840000000000003Avance: %5.849999999999994Avance: %5.859999999999999Avance: %5.8700000000000045Avance: %5.8799999999999955Avance: %5.890000000000001Avance: %5.900000000000006Avance: %5.910000000000011Avance: %5.920000000000002Avance: %5.930000000000007Avance: %5.939999999999998Avance: %5.950000000000003Avance: %5.959999999999994Avance: %5.969999999999999Avance: %5.97999999999999Avance: %5.989999999999995Avance: %6.0Avance: %6.010000000000005Avance: %6.02000000000001Avance: %6.030000000000001Avance: %6.040000000000006Avance: %6.049999999999997Avance: %6.060000000000002Avance: %6.069999999999993Avance: %6.079999999999998Avance: %6.089999999999989Avance: %6.1000000000000085Avance: %6.109999999999999Avance: %6.1200000000000045Avance: %6.1299999999999955Avance: %6.140000000000001Avance: %6.150000000000006Avance: %6.159999999999997Avance: %6.170000000000002Avance: %6.179999999999993Avance: %6.189999999999998Avance: %6.200000000000003Avance: %6.210000000000008Avance: %6.219999999999999Avance: %6.230000000000004Avance: %6.239999999999995Avance: %6.25Avance: %6.260000000000005Avance: %6.269999999999996Avance: %6.280000000000001Avance: %6.289999999999992Avance: %6.299999999999997Avance: %6.310000000000002Avance: %6.320000000000007Avance: %6.329999999999998Avance: %6.340000000000003Avance: %6.349999999999994Avance: %6.359999999999999Avance: %6.3700000000000045Avance: %6.3799999999999955Avance: %6.390000000000001Avance: %6.3999999999999915Avance: %6.410000000000011Avance: %6.420000000000002Avance: %6.430000000000007Avance: %6.439999999999998Avance: %6.450000000000003Avance: %6.459999999999994Avance: %6.469999999999999Avance: %6.47999999999999Avance: %6.489999999999995Avance: %6.5Avance: %6.510000000000005Avance: %6.52000000000001Avance: %6.530000000000001Avance: %6.540000000000006Avance: %6.549999999999997Avance: %6.560000000000002Avance: %6.569999999999993Avance: %6.579999999999998Avance: %6.589999999999989Avance: %6.599999999999994Avance: %6.609999999999999Avance: %6.6200000000000045Avance: %6.6299999999999955Avance: %6.640000000000001Avance: %6.650000000000006Avance: %6.659999999999997Avance: %6.670000000000002Avance: %6.679999999999993Avance: %6.689999999999998Avance: %6.699999999999989Avance: %6.710000000000008Avance: %6.719999999999999Avance: %6.730000000000004Avance: %6.739999999999995Avance: %6.75Avance: %6.760000000000005Avance: %6.769999999999996Avance: %6.780000000000001Avance: %6.789999999999992Avance: %6.799999999999997Avance: %6.810000000000002Avance: %6.820000000000007Avance: %6.829999999999998Avance: %6.840000000000003Avance: %6.849999999999994Avance: %6.859999999999999Avance: %6.8700000000000045Avance: %6.8799999999999955Avance: %6.890000000000001Avance: %6.8999999999999915Avance: %6.910000000000011Avance: %6.920000000000002Avance: %6.930000000000007Avance: %6.939999999999998Avance: %6.950000000000003Avance: %6.959999999999994Avance: %6.969999999999999Avance: %6.980000000000004Avance: %6.989999999999995Avance: %7.0Avance: %7.010000000000005Avance: %7.02000000000001Avance: %7.030000000000001Avance: %7.040000000000006Avance: %7.049999999999997Avance: %7.060000000000002Avance: %7.069999999999993Avance: %7.079999999999998Avance: %7.090000000000003Avance: %7.099999999999994Avance: %7.109999999999999Avance: %7.1200000000000045Avance: %7.13000000000001Avance: %7.140000000000001Avance: %7.150000000000006Avance: %7.159999999999997Avance: %7.170000000000002Avance: %7.179999999999993Avance: %7.189999999999998Avance: %7.199999999999989Avance: %7.210000000000008Avance: %7.219999999999999Avance: %7.230000000000004Avance: %7.240000000000009Avance: %7.25Avance: %7.260000000000005Avance: %7.269999999999996Avance: %7.280000000000001Avance: %7.289999999999992Avance: %7.299999999999997Avance: %7.310000000000002Avance: %7.320000000000007Avance: %7.329999999999998Avance: %7.340000000000003Avance: %7.349999999999994Avance: %7.359999999999999Avance: %7.3700000000000045Avance: %7.3799999999999955Avance: %7.390000000000001Avance: %7.3999999999999915Avance: %7.410000000000011Avance: %7.420000000000002Avance: %7.430000000000007Avance: %7.439999999999998Avance: %7.450000000000003Avance: %7.459999999999994Avance: %7.469999999999999Avance: %7.480000000000004Avance: %7.489999999999995Done Extracting Features 0:06:39.688122
The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.
Try some users for user 114537953
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 710) (2000, 710) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6956
        True       1.00      1.00      1.00        44

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1987
        True       1.00      1.00      1.00        13

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 120477698
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 455) (2000, 455) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6828
        True       1.00      0.99      1.00       172

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1951
        True       1.00      0.98      0.99        49

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.99      0.99      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 29557642
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 360) (2000, 360) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6975
        True       1.00      1.00      1.00        25

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1993
        True       1.00      1.00      1.00         7

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 15187065
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 741) (2000, 741) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6985
        True       1.00      1.00      1.00        15

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1995
        True       1.00      1.00      1.00         5

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 8259752
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 654) (2000, 654) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6985
        True       1.00      1.00      1.00        15

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1995
        True       1.00      1.00      1.00         5

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 115717145
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 524) (2000, 524) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6858
        True       1.00      0.97      0.99       142

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1959
        True       1.00      1.00      1.00        41

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 87818409
Loading df
Done loading df. DF shape is :(12397359, 8) (Original: (12397359, 8)) 		Time delta is: None mins
Loading graph from ../data/graphs/latest_subgraph.gpickle
Fetched 331 level 2 neighbourhs for user 87818409.
Len of own retweets (positive examples) is (1, 2)
done getting tweets universe. Shape is  (10977, 2)
	Dataset was truncated to 10000 tweets
Extracting features Optimized. X shape is : (10000, 331)
Avance: %0.010000000000005116Avance: %0.01999999999999602Avance: %0.030000000000001137Avance: %0.03999999999999204Avance: %0.04999999999999716Avance: %0.060000000000002274Avance: %0.07000000000000739Avance: %0.0799999999999983Avance: %0.09000000000000341Avance: %0.09999999999999432Avance: %0.10999999999999943Avance: %0.12000000000000455Avance: %0.12999999999999545Avance: %0.14000000000000057Avance: %0.14999999999999147Avance: %0.1600000000000108Avance: %0.1700000000000017Avance: %0.18000000000000682Avance: %0.18999999999999773Avance: %0.20000000000000284Avance: %0.20999999999999375Avance: %0.21999999999999886Avance: %0.22999999999998977Avance: %0.23999999999999488Avance: %0.25Avance: %0.2600000000000051Avance: %0.27000000000001023Avance: %0.28000000000000114Avance: %0.29000000000000625Avance: %0.29999999999999716Avance: %0.3100000000000023Avance: %0.3199999999999932Avance: %0.3299999999999983Avance: %0.3399999999999892Avance: %0.3499999999999943Avance: %0.35999999999999943Avance: %0.37000000000000455Avance: %0.37999999999999545Avance: %0.39000000000000057Avance: %0.4000000000000057Avance: %0.4099999999999966Avance: %0.4200000000000017Avance: %0.4299999999999926Avance: %0.4399999999999977Avance: %0.44999999999998863Avance: %0.46000000000000796Avance: %0.46999999999999886Avance: %0.480000000000004Avance: %0.4899999999999949Avance: %0.5Avance: %0.5100000000000051Avance: %0.519999999999996Avance: %0.5300000000000011Avance: %0.539999999999992Avance: %0.5499999999999972Avance: %0.5600000000000023Avance: %0.5700000000000074Avance: %0.5799999999999983Avance: %0.5900000000000034Avance: %0.5999999999999943Avance: %0.6099999999999994Avance: %0.6200000000000045Avance: %0.6299999999999955Avance: %0.6400000000000006Avance: %0.6499999999999915Avance: %0.6600000000000108Avance: %0.6700000000000017Avance: %0.6800000000000068Avance: %0.6899999999999977Avance: %0.7000000000000028Avance: %0.7099999999999937Avance: %0.7199999999999989Avance: %0.730000000000004Avance: %0.7399999999999949Avance: %0.75Avance: %0.7600000000000051Avance: %0.7700000000000102Avance: %0.7800000000000011Avance: %0.7900000000000063Avance: %0.7999999999999972Avance: %0.8100000000000023Avance: %0.8199999999999932Avance: %0.8299999999999983Avance: %0.8400000000000034Avance: %0.8499999999999943Avance: %0.8599999999999994Avance: %0.8700000000000045Avance: %0.8800000000000097Avance: %0.8900000000000006Avance: %0.9000000000000057Avance: %0.9099999999999966Avance: %0.9200000000000017Avance: %0.9299999999999926Avance: %0.9399999999999977Avance: %0.9499999999999886Avance: %0.960000000000008Avance: %0.9699999999999989Avance: %0.980000000000004Avance: %0.9900000000000091Avance: %1.0Avance: %1.0100000000000051Avance: %1.019999999999996Avance: %1.0300000000000011Avance: %1.039999999999992Avance: %1.0499999999999972Avance: %1.0600000000000023Avance: %1.0700000000000074Avance: %1.0799999999999983Avance: %1.0900000000000034Avance: %1.0999999999999943Avance: %1.1099999999999994Avance: %1.1200000000000045Avance: %1.1299999999999955Avance: %1.1400000000000006Avance: %1.1499999999999915Avance: %1.1600000000000108Avance: %1.1700000000000017Avance: %1.1800000000000068Avance: %1.1899999999999977Avance: %1.2000000000000028Avance: %1.2099999999999937Avance: %1.2199999999999989Avance: %1.230000000000004Avance: %1.2399999999999949Avance: %1.25Avance: %1.259999999999991Avance: %1.2700000000000102Avance: %1.2800000000000011Avance: %1.2900000000000063Avance: %1.2999999999999972Avance: %1.3100000000000023Avance: %1.3199999999999932Avance: %1.3299999999999983Avance: %1.3400000000000034Avance: %1.3499999999999943Avance: %1.3599999999999994Avance: %1.3700000000000045Avance: %1.3800000000000097Avance: %1.3900000000000006Avance: %1.4000000000000057Avance: %1.4099999999999966Avance: %1.4200000000000017Avance: %1.4299999999999926Avance: %1.4399999999999977Avance: %1.4499999999999886Avance: %1.4599999999999937Avance: %1.4699999999999989Avance: %1.480000000000004Avance: %1.490000000000009Avance: %1.5Avance: %1.5100000000000051Avance: %1.519999999999996Avance: %1.5300000000000011Avance: %1.539999999999992Avance: %1.5499999999999972Avance: %1.559999999999988Avance: %1.5700000000000074Avance: %1.5799999999999983Avance: %1.5900000000000034Avance: %1.5999999999999943Avance: %1.6099999999999994Avance: %1.6200000000000045Avance: %1.6299999999999955Avance: %1.6400000000000006Avance: %1.6499999999999915Avance: %1.6599999999999966Avance: %1.6700000000000017Avance: %1.6800000000000068Avance: %1.6899999999999977Avance: %1.7000000000000028Avance: %1.7099999999999937Avance: %1.7199999999999989Avance: %1.730000000000004Avance: %1.7399999999999949Avance: %1.75Avance: %1.759999999999991Avance: %1.7700000000000102Avance: %1.7800000000000011Avance: %1.7900000000000063Avance: %1.7999999999999972Avance: %1.8100000000000023Avance: %1.8199999999999932Avance: %1.8299999999999983Avance: %1.8400000000000034Avance: %1.8499999999999943Avance: %1.8599999999999994Avance: %1.8700000000000045Avance: %1.8800000000000097Avance: %1.8900000000000006Avance: %1.9000000000000057Avance: %1.9099999999999966Avance: %1.9200000000000017Avance: %1.9299999999999926Avance: %1.9399999999999977Avance: %1.9500000000000028Avance: %1.9599999999999937Avance: %1.9699999999999989Avance: %1.980000000000004Avance: %1.990000000000009Avance: %2.0Avance: %2.010000000000005Avance: %2.019999999999996Avance: %2.030000000000001Avance: %2.039999999999992Avance: %2.049999999999997Avance: %2.0600000000000023Avance: %2.0700000000000074Avance: %2.0799999999999983Avance: %2.0900000000000034Avance: %2.1000000000000085Avance: %2.1099999999999994Avance: %2.1200000000000045Avance: %2.1299999999999955Avance: %2.1400000000000006Avance: %2.1499999999999915Avance: %2.1599999999999966Avance: %2.1700000000000017Avance: %2.180000000000007Avance: %2.1899999999999977Avance: %2.200000000000003Avance: %2.210000000000008Avance: %2.219999999999999Avance: %2.230000000000004Avance: %2.239999999999995Avance: %2.25Avance: %2.259999999999991Avance: %2.2700000000000102Avance: %2.280000000000001Avance: %2.2900000000000063Avance: %2.299999999999997Avance: %2.3100000000000023Avance: %2.319999999999993Avance: %2.3299999999999983Avance: %2.3400000000000034Avance: %2.3499999999999943Avance: %2.3599999999999994Avance: %2.3700000000000045Avance: %2.3800000000000097Avance: %2.3900000000000006Avance: %2.4000000000000057Avance: %2.4099999999999966Avance: %2.4200000000000017Avance: %2.4299999999999926Avance: %2.4399999999999977Avance: %2.450000000000003Avance: %2.4599999999999937Avance: %2.469999999999999Avance: %2.480000000000004Avance: %2.490000000000009Avance: %2.5Avance: %2.510000000000005Avance: %2.519999999999996Avance: %2.530000000000001Avance: %2.539999999999992Avance: %2.549999999999997Avance: %2.5600000000000023Avance: %2.569999999999993Avance: %2.5799999999999983Avance: %2.5900000000000034Avance: %2.6000000000000085Avance: %2.6099999999999994Avance: %2.6200000000000045Avance: %2.6299999999999955Avance: %2.6400000000000006Avance: %2.6499999999999915Avance: %2.6599999999999966Avance: %2.6699999999999875Avance: %2.680000000000007Avance: %2.6899999999999977Avance: %2.700000000000003Avance: %2.710000000000008Avance: %2.719999999999999Avance: %2.730000000000004Avance: %2.739999999999995Avance: %2.75Avance: %2.759999999999991Avance: %2.769999999999996Avance: %2.780000000000001Avance: %2.7900000000000063Avance: %2.799999999999997Avance: %2.8100000000000023Avance: %2.819999999999993Avance: %2.8299999999999983Avance: %2.8400000000000034Avance: %2.8499999999999943Avance: %2.8599999999999994Avance: %2.8699999999999903Avance: %2.8800000000000097Avance: %2.8900000000000006Avance: %2.9000000000000057Avance: %2.9099999999999966Avance: %2.9200000000000017Avance: %2.9299999999999926Avance: %2.9399999999999977Avance: %2.950000000000003Avance: %2.9599999999999937Avance: %2.969999999999999Avance: %2.980000000000004Avance: %2.990000000000009Avance: %3.0Avance: %3.010000000000005Avance: %3.019999999999996Avance: %3.030000000000001Avance: %3.039999999999992Avance: %3.049999999999997Avance: %3.0600000000000023Avance: %3.069999999999993Avance: %3.0799999999999983Avance: %3.0900000000000034Avance: %3.1000000000000085Avance: %3.1099999999999994Avance: %3.1200000000000045Avance: %3.1299999999999955Avance: %3.1400000000000006Avance: %3.1499999999999915Avance: %3.1599999999999966Avance: %3.1700000000000017Avance: %3.180000000000007Avance: %3.1899999999999977Avance: %3.200000000000003Avance: %3.210000000000008Avance: %3.219999999999999Avance: %3.230000000000004Avance: %3.239999999999995Avance: %3.25Avance: %3.259999999999991Avance: %3.269999999999996Avance: %3.280000000000001Avance: %3.2900000000000063Avance: %3.299999999999997Avance: %3.3100000000000023Done Extracting Features 0:02:44.750023
The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.
Try some users for user 186815164
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 452) (2000, 452) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6915
        True       1.00      0.91      0.95        85

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.95      0.98      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1975
        True       1.00      0.96      0.98        25

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.98      0.99      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 7230812
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 752) (2000, 752) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6996
        True       1.00      1.00      1.00         4

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1999
        True       1.00      1.00      1.00         1

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 59881845
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 461) (2000, 461) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6977
        True       1.00      1.00      1.00        23

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1993
        True       1.00      1.00      1.00         7

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 196362385
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 574) (2000, 574) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6904
        True       1.00      1.00      1.00        96

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1973
        True       1.00      1.00      1.00        27

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 52693558
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 739) (2000, 739) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6971
        True       1.00      1.00      1.00        29

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1992
        True       1.00      1.00      1.00         8

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 9041082
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 637) (2000, 637) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6943
        True       1.00      0.98      0.99        57

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1984
        True       1.00      1.00      1.00        16

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 244328058
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 554) (2000, 554) False False
The number of classes has to be greater than one; got 1 class
Try some users for user 322172045
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 707) (2000, 707) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6977
        True       1.00      1.00      1.00        23

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1993
        True       1.00      1.00      1.00         7

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 8932062
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 637) (2000, 637) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6966
        True       1.00      1.00      1.00        34

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1991
        True       1.00      1.00      1.00         9

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 18717943
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 561) (2000, 561) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6994
        True       1.00      1.00      1.00         6

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1999
        True       1.00      1.00      1.00         1

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 25584941
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 565) (2000, 565) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6953
        True       1.00      0.89      0.94        47

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.95      0.97      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1987
        True       1.00      1.00      1.00        13

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 1724268187
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 716) (2000, 716) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6992
        True       1.00      1.00      1.00         8

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1997
        True       1.00      1.00      1.00         3

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 50778394
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 491) (2000, 491) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6789
        True       1.00      0.98      0.99       211

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1939
        True       1.00      0.92      0.96        61

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.96      0.98      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 185443156
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 462) (2000, 462) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6996
        True       1.00      1.00      1.00         4

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1999
        True       1.00      1.00      1.00         1

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 83989828
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 798) (2000, 798) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6959
        True       1.00      1.00      1.00        41

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1988
        True       1.00      1.00      1.00        12

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 1679653772
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 534) (2000, 534) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6936
        True       1.00      0.97      0.98        64

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.98      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1982
        True       1.00      0.94      0.97        18

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.97      0.99      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 42915657
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 605) (2000, 605) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6909
        True       1.00      0.98      0.99        91

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1974
        True       1.00      1.00      1.00        26

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 46733972
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 491) (2000, 491) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6905
        True       1.00      0.98      0.99        95

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1973
        True       1.00      1.00      1.00        27

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 349106585
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 1006) (2000, 1006) False False
The number of classes has to be greater than one; got 1 class
Try some users for user 33189599
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 423) (2000, 423) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6971
        True       1.00      1.00      1.00        29

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1991
        True       1.00      1.00      1.00         9

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 14104882
Loading df
Done loading df. DF shape is :(12397359, 8) (Original: (12397359, 8)) 		Time delta is: None mins
Loading graph from ../data/graphs/latest_subgraph.gpickle
Fetched 759 level 2 neighbourhs for user 14104882.
Len of own retweets (positive examples) is (4, 2)
done getting tweets universe. Shape is  (13142, 2)
	Dataset was truncated to 10000 tweets
Extracting features Optimized. X shape is : (10000, 759)
Avance: %0.010000000000005116Avance: %0.01999999999999602Avance: %0.030000000000001137Avance: %0.03999999999999204Avance: %0.04999999999999716Avance: %0.060000000000002274Avance: %0.07000000000000739Avance: %0.0799999999999983Avance: %0.09000000000000341Avance: %0.09999999999999432Avance: %0.10999999999999943Avance: %0.12000000000000455Avance: %0.12999999999999545Avance: %0.14000000000000057Avance: %0.14999999999999147Avance: %0.1600000000000108Avance: %0.1700000000000017Avance: %0.18000000000000682Avance: %0.18999999999999773Avance: %0.20000000000000284Avance: %0.20999999999999375Avance: %0.21999999999999886Avance: %0.22999999999998977Avance: %0.23999999999999488Avance: %0.25Avance: %0.2600000000000051Avance: %0.27000000000001023Avance: %0.28000000000000114Avance: %0.29000000000000625Avance: %0.29999999999999716Avance: %0.3100000000000023Avance: %0.3199999999999932Avance: %0.3299999999999983Avance: %0.3399999999999892Avance: %0.3499999999999943Avance: %0.35999999999999943Avance: %0.37000000000000455Avance: %0.37999999999999545Avance: %0.39000000000000057Avance: %0.4000000000000057Avance: %0.4099999999999966Avance: %0.4200000000000017Avance: %0.4299999999999926Avance: %0.4399999999999977Avance: %0.44999999999998863Avance: %0.46000000000000796Avance: %0.46999999999999886Avance: %0.480000000000004Avance: %0.4899999999999949Avance: %0.5Avance: %0.5100000000000051Avance: %0.519999999999996Avance: %0.5300000000000011Avance: %0.539999999999992Avance: %0.5499999999999972Avance: %0.5600000000000023Avance: %0.5700000000000074Avance: %0.5799999999999983Avance: %0.5900000000000034Avance: %0.5999999999999943Avance: %0.6099999999999994Avance: %0.6200000000000045Avance: %0.6299999999999955Avance: %0.6400000000000006Avance: %0.6499999999999915Avance: %0.6600000000000108Avance: %0.6700000000000017Avance: %0.6800000000000068Avance: %0.6899999999999977Avance: %0.7000000000000028Avance: %0.7099999999999937Avance: %0.7199999999999989Avance: %0.730000000000004Avance: %0.7399999999999949Avance: %0.75Avance: %0.7600000000000051Avance: %0.7700000000000102Avance: %0.7800000000000011Avance: %0.7900000000000063Avance: %0.7999999999999972Avance: %0.8100000000000023Avance: %0.8199999999999932Avance: %0.8299999999999983Avance: %0.8400000000000034Avance: %0.8499999999999943Avance: %0.8599999999999994Avance: %0.8700000000000045Avance: %0.8800000000000097Avance: %0.8900000000000006Avance: %0.9000000000000057Avance: %0.9099999999999966Avance: %0.9200000000000017Avance: %0.9299999999999926Avance: %0.9399999999999977Avance: %0.9499999999999886Avance: %0.960000000000008Avance: %0.9699999999999989Avance: %0.980000000000004Avance: %0.9900000000000091Avance: %1.0Avance: %1.0100000000000051Avance: %1.019999999999996Avance: %1.0300000000000011Avance: %1.039999999999992Avance: %1.0499999999999972Avance: %1.0600000000000023Avance: %1.0700000000000074Avance: %1.0799999999999983Avance: %1.0900000000000034Avance: %1.0999999999999943Avance: %1.1099999999999994Avance: %1.1200000000000045Avance: %1.1299999999999955Avance: %1.1400000000000006Avance: %1.1499999999999915Avance: %1.1600000000000108Avance: %1.1700000000000017Avance: %1.1800000000000068Avance: %1.1899999999999977Avance: %1.2000000000000028Avance: %1.2099999999999937Avance: %1.2199999999999989Avance: %1.230000000000004Avance: %1.2399999999999949Avance: %1.25Avance: %1.259999999999991Avance: %1.2700000000000102Avance: %1.2800000000000011Avance: %1.2900000000000063Avance: %1.2999999999999972Avance: %1.3100000000000023Avance: %1.3199999999999932Avance: %1.3299999999999983Avance: %1.3400000000000034Avance: %1.3499999999999943Avance: %1.3599999999999994Avance: %1.3700000000000045Avance: %1.3800000000000097Avance: %1.3900000000000006Avance: %1.4000000000000057Avance: %1.4099999999999966Avance: %1.4200000000000017Avance: %1.4299999999999926Avance: %1.4399999999999977Avance: %1.4499999999999886Avance: %1.4599999999999937Avance: %1.4699999999999989Avance: %1.480000000000004Avance: %1.490000000000009Avance: %1.5Avance: %1.5100000000000051Avance: %1.519999999999996Avance: %1.5300000000000011Avance: %1.539999999999992Avance: %1.5499999999999972Avance: %1.559999999999988Avance: %1.5700000000000074Avance: %1.5799999999999983Avance: %1.5900000000000034Avance: %1.5999999999999943Avance: %1.6099999999999994Avance: %1.6200000000000045Avance: %1.6299999999999955Avance: %1.6400000000000006Avance: %1.6499999999999915Avance: %1.6599999999999966Avance: %1.6700000000000017Avance: %1.6800000000000068Avance: %1.6899999999999977Avance: %1.7000000000000028Avance: %1.7099999999999937Avance: %1.7199999999999989Avance: %1.730000000000004Avance: %1.7399999999999949Avance: %1.75Avance: %1.759999999999991Avance: %1.7700000000000102Avance: %1.7800000000000011Avance: %1.7900000000000063Avance: %1.7999999999999972Avance: %1.8100000000000023Avance: %1.8199999999999932Avance: %1.8299999999999983Avance: %1.8400000000000034Avance: %1.8499999999999943Avance: %1.8599999999999994Avance: %1.8700000000000045Avance: %1.8800000000000097Avance: %1.8900000000000006Avance: %1.9000000000000057Avance: %1.9099999999999966Avance: %1.9200000000000017Avance: %1.9299999999999926Avance: %1.9399999999999977Avance: %1.9500000000000028Avance: %1.9599999999999937Avance: %1.9699999999999989Avance: %1.980000000000004Avance: %1.990000000000009Avance: %2.0Avance: %2.010000000000005Avance: %2.019999999999996Avance: %2.030000000000001Avance: %2.039999999999992Avance: %2.049999999999997Avance: %2.0600000000000023Avance: %2.0700000000000074Avance: %2.0799999999999983Avance: %2.0900000000000034Avance: %2.1000000000000085Avance: %2.1099999999999994Avance: %2.1200000000000045Avance: %2.1299999999999955Avance: %2.1400000000000006Avance: %2.1499999999999915Avance: %2.1599999999999966Avance: %2.1700000000000017Avance: %2.180000000000007Avance: %2.1899999999999977Avance: %2.200000000000003Avance: %2.210000000000008Avance: %2.219999999999999Avance: %2.230000000000004Avance: %2.239999999999995Avance: %2.25Avance: %2.259999999999991Avance: %2.2700000000000102Avance: %2.280000000000001Avance: %2.2900000000000063Avance: %2.299999999999997Avance: %2.3100000000000023Avance: %2.319999999999993Avance: %2.3299999999999983Avance: %2.3400000000000034Avance: %2.3499999999999943Avance: %2.3599999999999994Avance: %2.3700000000000045Avance: %2.3800000000000097Avance: %2.3900000000000006Avance: %2.4000000000000057Avance: %2.4099999999999966Avance: %2.4200000000000017Avance: %2.4299999999999926Avance: %2.4399999999999977Avance: %2.450000000000003Avance: %2.4599999999999937Avance: %2.469999999999999Avance: %2.480000000000004Avance: %2.490000000000009Avance: %2.5Avance: %2.510000000000005Avance: %2.519999999999996Avance: %2.530000000000001Avance: %2.539999999999992Avance: %2.549999999999997Avance: %2.5600000000000023Avance: %2.569999999999993Avance: %2.5799999999999983Avance: %2.5900000000000034Avance: %2.6000000000000085Avance: %2.6099999999999994Avance: %2.6200000000000045Avance: %2.6299999999999955Avance: %2.6400000000000006Avance: %2.6499999999999915Avance: %2.6599999999999966Avance: %2.6699999999999875Avance: %2.680000000000007Avance: %2.6899999999999977Avance: %2.700000000000003Avance: %2.710000000000008Avance: %2.719999999999999Avance: %2.730000000000004Avance: %2.739999999999995Avance: %2.75Avance: %2.759999999999991Avance: %2.769999999999996Avance: %2.780000000000001Avance: %2.7900000000000063Avance: %2.799999999999997Avance: %2.8100000000000023Avance: %2.819999999999993Avance: %2.8299999999999983Avance: %2.8400000000000034Avance: %2.8499999999999943Avance: %2.8599999999999994Avance: %2.8699999999999903Avance: %2.8800000000000097Avance: %2.8900000000000006Avance: %2.9000000000000057Avance: %2.9099999999999966Avance: %2.9200000000000017Avance: %2.9299999999999926Avance: %2.9399999999999977Avance: %2.950000000000003Avance: %2.9599999999999937Avance: %2.969999999999999Avance: %2.980000000000004Avance: %2.990000000000009Avance: %3.0Avance: %3.010000000000005Avance: %3.019999999999996Avance: %3.030000000000001Avance: %3.039999999999992Avance: %3.049999999999997Avance: %3.0600000000000023Avance: %3.069999999999993Avance: %3.0799999999999983Avance: %3.0900000000000034Avance: %3.1000000000000085Avance: %3.1099999999999994Avance: %3.1200000000000045Avance: %3.1299999999999955Avance: %3.1400000000000006Avance: %3.1499999999999915Avance: %3.1599999999999966Avance: %3.1700000000000017Avance: %3.180000000000007Avance: %3.1899999999999977Avance: %3.200000000000003Avance: %3.210000000000008Avance: %3.219999999999999Avance: %3.230000000000004Avance: %3.239999999999995Avance: %3.25Avance: %3.259999999999991Avance: %3.269999999999996Avance: %3.280000000000001Avance: %3.2900000000000063Avance: %3.299999999999997Avance: %3.3100000000000023Avance: %3.3200000000000074Avance: %3.3299999999999983Avance: %3.3400000000000034Avance: %3.3499999999999943Avance: %3.3599999999999994Avance: %3.3699999999999903Avance: %3.3800000000000097Avance: %3.3900000000000006Avance: %3.4000000000000057Avance: %3.4099999999999966Avance: %3.4200000000000017Avance: %3.430000000000007Avance: %3.4399999999999977Avance: %3.450000000000003Avance: %3.4599999999999937Avance: %3.469999999999999Avance: %3.480000000000004Avance: %3.490000000000009Avance: %3.5Avance: %3.510000000000005Avance: %3.519999999999996Avance: %3.530000000000001Avance: %3.539999999999992Avance: %3.549999999999997Avance: %3.5600000000000023Avance: %3.569999999999993Avance: %3.5800000000000125Avance: %3.5900000000000034Avance: %3.6000000000000085Avance: %3.6099999999999994Avance: %3.6200000000000045Avance: %3.6299999999999955Avance: %3.6400000000000006Avance: %3.6499999999999915Avance: %3.6599999999999966Avance: %3.6700000000000017Avance: %3.680000000000007Avance: %3.6899999999999977Avance: %3.700000000000003Avance: %3.710000000000008Avance: %3.719999999999999Avance: %3.730000000000004Avance: %3.739999999999995Avance: %3.75Avance: %3.759999999999991Avance: %3.769999999999996Avance: %3.780000000000001Avance: %3.7900000000000063Avance: %3.799999999999997Avance: %3.8100000000000023Avance: %3.8200000000000074Avance: %3.8299999999999983Avance: %3.8400000000000034Avance: %3.8499999999999943Avance: %3.8599999999999994Avance: %3.8699999999999903Avance: %3.8799999999999955Avance: %3.8900000000000006Avance: %3.9000000000000057Avance: %3.9099999999999966Avance: %3.9200000000000017Avance: %3.930000000000007Avance: %3.9399999999999977Avance: %3.950000000000003Avance: %3.9599999999999937Avance: %3.969999999999999Avance: %3.9799999999999898Avance: %3.990000000000009Avance: %4.0Avance: %4.010000000000005Avance: %4.019999999999996Avance: %4.030000000000001Avance: %4.039999999999992Avance: %4.049999999999997Avance: %4.060000000000002Avance: %4.069999999999993Avance: %4.079999999999998Avance: %4.090000000000003Avance: %4.1000000000000085Avance: %4.109999999999999Avance: %4.1200000000000045Avance: %4.1299999999999955Avance: %4.140000000000001Avance: %4.1499999999999915Avance: %4.159999999999997Avance: %4.170000000000002Avance: %4.179999999999993Avance: %4.189999999999998Avance: %4.200000000000003Avance: %4.210000000000008Avance: %4.219999999999999Avance: %4.230000000000004Avance: %4.239999999999995Avance: %4.25Avance: %4.259999999999991Avance: %4.269999999999996Avance: %4.280000000000001Avance: %4.290000000000006Avance: %4.299999999999997Avance: %4.310000000000002Avance: %4.320000000000007Avance: %4.329999999999998Avance: %4.340000000000003Avance: %4.349999999999994Avance: %4.359999999999999Avance: %4.36999999999999Avance: %4.3799999999999955Avance: %4.390000000000001Avance: %4.400000000000006Avance: %4.409999999999997Avance: %4.420000000000002Avance: %4.430000000000007Avance: %4.439999999999998Avance: %4.450000000000003Avance: %4.459999999999994Avance: %4.469999999999999Avance: %4.47999999999999Avance: %4.490000000000009Avance: %4.5Avance: %4.510000000000005Avance: %4.519999999999996Avance: %4.530000000000001Avance: %4.540000000000006Avance: %4.549999999999997Avance: %4.560000000000002Avance: %4.569999999999993Avance: %4.579999999999998Avance: %4.590000000000003Avance: %4.6000000000000085Avance: %4.609999999999999Avance: %4.6200000000000045Avance: %4.6299999999999955Avance: %4.640000000000001Avance: %4.650000000000006Avance: %4.659999999999997Avance: %4.670000000000002Avance: %4.679999999999993Avance: %4.690000000000012Avance: %4.700000000000003Avance: %4.710000000000008Avance: %4.719999999999999Avance: %4.730000000000004Avance: %4.739999999999995Avance: %4.75Avance: %4.759999999999991Avance: %4.769999999999996Avance: %4.780000000000001Avance: %4.790000000000006Avance: %4.800000000000011Avance: %4.810000000000002Avance: %4.820000000000007Avance: %4.829999999999998Avance: %4.840000000000003Avance: %4.849999999999994Avance: %4.859999999999999Avance: %4.86999999999999Avance: %4.8799999999999955Avance: %4.890000000000001Avance: %4.900000000000006Avance: %4.909999999999997Avance: %4.920000000000002Avance: %4.930000000000007Avance: %4.939999999999998Avance: %4.950000000000003Avance: %4.959999999999994Avance: %4.969999999999999Avance: %4.97999999999999Avance: %4.990000000000009Avance: %5.0Avance: %5.010000000000005Avance: %5.019999999999996Avance: %5.030000000000001Avance: %5.040000000000006Avance: %5.049999999999997Avance: %5.060000000000002Avance: %5.069999999999993Avance: %5.079999999999998Avance: %5.089999999999989Avance: %5.1000000000000085Avance: %5.109999999999999Avance: %5.1200000000000045Avance: %5.1299999999999955Avance: %5.140000000000001Avance: %5.150000000000006Avance: %5.159999999999997Avance: %5.170000000000002Avance: %5.179999999999993Avance: %5.189999999999998Avance: %5.200000000000003Avance: %5.210000000000008Avance: %5.219999999999999Avance: %5.230000000000004Avance: %5.239999999999995Avance: %5.25Avance: %5.259999999999991Avance: %5.269999999999996Avance: %5.280000000000001Avance: %5.289999999999992Avance: %5.300000000000011Avance: %5.310000000000002Avance: %5.320000000000007Avance: %5.329999999999998Avance: %5.340000000000003Avance: %5.349999999999994Avance: %5.359999999999999Avance: %5.36999999999999Avance: %5.3799999999999955Avance: %5.390000000000001Avance: %5.400000000000006Avance: %5.409999999999997Avance: %5.420000000000002Avance: %5.430000000000007Avance: %5.439999999999998Avance: %5.450000000000003Avance: %5.459999999999994Avance: %5.469999999999999Avance: %5.47999999999999Avance: %5.489999999999995Avance: %5.5Avance: %5.510000000000005Avance: %5.519999999999996Avance: %5.530000000000001Avance: %5.540000000000006Avance: %5.549999999999997Avance: %5.560000000000002Avance: %5.569999999999993Avance: %5.579999999999998Avance: %5.589999999999989Avance: %5.6000000000000085Avance: %5.609999999999999Avance: %5.6200000000000045Avance: %5.6299999999999955Avance: %5.640000000000001Avance: %5.650000000000006Avance: %5.659999999999997Avance: %5.670000000000002Avance: %5.679999999999993Avance: %5.689999999999998Avance: %5.700000000000003Avance: %5.710000000000008Avance: %5.719999999999999Avance: %5.730000000000004Avance: %5.739999999999995Avance: %5.75Avance: %5.760000000000005Avance: %5.769999999999996Avance: %5.780000000000001Avance: %5.789999999999992Avance: %5.800000000000011Avance: %5.810000000000002Avance: %5.820000000000007Avance: %5.829999999999998Avance: %5.840000000000003Avance: %5.849999999999994Avance: %5.859999999999999Avance: %5.8700000000000045Avance: %5.8799999999999955Avance: %5.890000000000001Avance: %5.900000000000006Avance: %5.910000000000011Avance: %5.920000000000002Avance: %5.930000000000007Avance: %5.939999999999998Avance: %5.950000000000003Avance: %5.959999999999994Avance: %5.969999999999999Avance: %5.97999999999999Avance: %5.989999999999995Avance: %6.0Avance: %6.010000000000005Avance: %6.02000000000001Avance: %6.030000000000001Avance: %6.040000000000006Avance: %6.049999999999997Avance: %6.060000000000002Avance: %6.069999999999993Avance: %6.079999999999998Avance: %6.089999999999989Avance: %6.1000000000000085Avance: %6.109999999999999Avance: %6.1200000000000045Avance: %6.1299999999999955Avance: %6.140000000000001Avance: %6.150000000000006Avance: %6.159999999999997Avance: %6.170000000000002Avance: %6.179999999999993Avance: %6.189999999999998Avance: %6.200000000000003Avance: %6.210000000000008Avance: %6.219999999999999Avance: %6.230000000000004Avance: %6.239999999999995Avance: %6.25Avance: %6.260000000000005Avance: %6.269999999999996Avance: %6.280000000000001Avance: %6.289999999999992Avance: %6.299999999999997Avance: %6.310000000000002Avance: %6.320000000000007Avance: %6.329999999999998Avance: %6.340000000000003Avance: %6.349999999999994Avance: %6.359999999999999Avance: %6.3700000000000045Avance: %6.3799999999999955Avance: %6.390000000000001Avance: %6.3999999999999915Avance: %6.410000000000011Avance: %6.420000000000002Avance: %6.430000000000007Avance: %6.439999999999998Avance: %6.450000000000003Avance: %6.459999999999994Avance: %6.469999999999999Avance: %6.47999999999999Avance: %6.489999999999995Avance: %6.5Avance: %6.510000000000005Avance: %6.52000000000001Avance: %6.530000000000001Avance: %6.540000000000006Avance: %6.549999999999997Avance: %6.560000000000002Avance: %6.569999999999993Avance: %6.579999999999998Avance: %6.589999999999989Avance: %6.599999999999994Avance: %6.609999999999999Avance: %6.6200000000000045Avance: %6.6299999999999955Avance: %6.640000000000001Avance: %6.650000000000006Avance: %6.659999999999997Avance: %6.670000000000002Avance: %6.679999999999993Avance: %6.689999999999998Avance: %6.699999999999989Avance: %6.710000000000008Avance: %6.719999999999999Avance: %6.730000000000004Avance: %6.739999999999995Avance: %6.75Avance: %6.760000000000005Avance: %6.769999999999996Avance: %6.780000000000001Avance: %6.789999999999992Avance: %6.799999999999997Avance: %6.810000000000002Avance: %6.820000000000007Avance: %6.829999999999998Avance: %6.840000000000003Avance: %6.849999999999994Avance: %6.859999999999999Avance: %6.8700000000000045Avance: %6.8799999999999955Avance: %6.890000000000001Avance: %6.8999999999999915Avance: %6.910000000000011Avance: %6.920000000000002Avance: %6.930000000000007Avance: %6.939999999999998Avance: %6.950000000000003Avance: %6.959999999999994Avance: %6.969999999999999Avance: %6.980000000000004Avance: %6.989999999999995Avance: %7.0Avance: %7.010000000000005Avance: %7.02000000000001Avance: %7.030000000000001Avance: %7.040000000000006Avance: %7.049999999999997Avance: %7.060000000000002Avance: %7.069999999999993Avance: %7.079999999999998Avance: %7.090000000000003Avance: %7.099999999999994Avance: %7.109999999999999Avance: %7.1200000000000045Avance: %7.13000000000001Avance: %7.140000000000001Avance: %7.150000000000006Avance: %7.159999999999997Avance: %7.170000000000002Avance: %7.179999999999993Avance: %7.189999999999998Avance: %7.199999999999989Avance: %7.210000000000008Avance: %7.219999999999999Avance: %7.230000000000004Avance: %7.240000000000009Avance: %7.25Avance: %7.260000000000005Avance: %7.269999999999996Avance: %7.280000000000001Avance: %7.289999999999992Avance: %7.299999999999997Avance: %7.310000000000002Avance: %7.320000000000007Avance: %7.329999999999998Avance: %7.340000000000003Avance: %7.349999999999994Avance: %7.359999999999999Avance: %7.3700000000000045Avance: %7.3799999999999955Avance: %7.390000000000001Avance: %7.3999999999999915Avance: %7.410000000000011Avance: %7.420000000000002Avance: %7.430000000000007Avance: %7.439999999999998Avance: %7.450000000000003Avance: %7.459999999999994Avance: %7.469999999999999Avance: %7.480000000000004Avance: %7.489999999999995Avance: %7.5Avance: %7.509999999999991Avance: %7.52000000000001Avance: %7.530000000000001Avance: %7.540000000000006Avance: %7.549999999999997Avance: %7.560000000000002Avance: %7.569999999999993Avance: %7.579999999999998Avance: %7.590000000000003Done Extracting Features 0:06:47.746928
The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.
Try some users for user 19923515
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 395) (2000, 395) False False
The number of classes has to be greater than one; got 1 class
Try some users for user 9165882
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 757) (2000, 757) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6947
        True       1.00      1.00      1.00        53

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1985
        True       1.00      1.00      1.00        15

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 92794844
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 522) (2000, 522) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6974
        True       1.00      1.00      1.00        26

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1993
        True       1.00      0.86      0.92         7

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.93      0.96      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 70414730
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 568) (2000, 568) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6733
        True       1.00      0.96      0.98       267

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.98      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1923
        True       1.00      0.92      0.96        77

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.96      0.98      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 160609570
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 441) (2000, 441) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6928
        True       1.00      1.00      1.00        72

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1979
        True       1.00      1.00      1.00        21

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 108968018
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 478) (2000, 478) True True
Best parameters set found on training set:

{'C': 1, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      0.73      0.84      6991
        True       0.00      1.00      0.01         9

   micro avg       0.73      0.73      0.73      7000
   macro avg       0.50      0.87      0.43      7000
weighted avg       1.00      0.73      0.84      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      0.72      0.84      1997
        True       0.01      1.00      0.01         3

   micro avg       0.72      0.72      0.72      2000
   macro avg       0.50      0.86      0.42      2000
weighted avg       1.00      0.72      0.84      2000


			END SVC
Try some users for user 135091201
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 366) (2000, 366) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6972
        True       1.00      1.00      1.00        28

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1992
        True       1.00      1.00      1.00         8

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 2859085672
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 732) (2000, 732) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': None, 'gamma': 10, 'kernel': 'poly'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6995
        True       1.00      0.60      0.75         5

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.80      0.87      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1999
        True       1.00      1.00      1.00         1

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 149286609
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 638) (2000, 638) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6992
        True       1.00      1.00      1.00         8

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1998
        True       1.00      1.00      1.00         2

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 9732212
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 389) (2000, 389) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6761
        True       1.00      0.99      0.99       239

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1932
        True       1.00      0.97      0.99        68

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.99      0.99      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 24735713
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 423) (2000, 423) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6885
        True       1.00      0.99      1.00       115

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1967
        True       1.00      1.00      1.00        33

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 1307745198
Loading df
Done loading df. DF shape is :(12397359, 8) (Original: (12397359, 8)) 		Time delta is: None mins
Loading graph from ../data/graphs/latest_subgraph.gpickle
Fetched 497 level 2 neighbourhs for user 1307745198.
Len of own retweets (positive examples) is (3, 2)
done getting tweets universe. Shape is  (12498, 2)
	Dataset was truncated to 10000 tweets
Extracting features Optimized. X shape is : (10000, 497)
Avance: %0.010000000000005116Avance: %0.01999999999999602Avance: %0.030000000000001137Avance: %0.03999999999999204Avance: %0.04999999999999716Avance: %0.060000000000002274Avance: %0.07000000000000739Avance: %0.0799999999999983Avance: %0.09000000000000341Avance: %0.09999999999999432Avance: %0.10999999999999943Avance: %0.12000000000000455Avance: %0.12999999999999545Avance: %0.14000000000000057Avance: %0.14999999999999147Avance: %0.1600000000000108Avance: %0.1700000000000017Avance: %0.18000000000000682Avance: %0.18999999999999773Avance: %0.20000000000000284Avance: %0.20999999999999375Avance: %0.21999999999999886Avance: %0.22999999999998977Avance: %0.23999999999999488Avance: %0.25Avance: %0.2600000000000051Avance: %0.27000000000001023Avance: %0.28000000000000114Avance: %0.29000000000000625Avance: %0.29999999999999716Avance: %0.3100000000000023Avance: %0.3199999999999932Avance: %0.3299999999999983Avance: %0.3399999999999892Avance: %0.3499999999999943Avance: %0.35999999999999943Avance: %0.37000000000000455Avance: %0.37999999999999545Avance: %0.39000000000000057Avance: %0.4000000000000057Avance: %0.4099999999999966Avance: %0.4200000000000017Avance: %0.4299999999999926Avance: %0.4399999999999977Avance: %0.44999999999998863Avance: %0.46000000000000796Avance: %0.46999999999999886Avance: %0.480000000000004Avance: %0.4899999999999949Avance: %0.5Avance: %0.5100000000000051Avance: %0.519999999999996Avance: %0.5300000000000011Avance: %0.539999999999992Avance: %0.5499999999999972Avance: %0.5600000000000023Avance: %0.5700000000000074Avance: %0.5799999999999983Avance: %0.5900000000000034Avance: %0.5999999999999943Avance: %0.6099999999999994Avance: %0.6200000000000045Avance: %0.6299999999999955Avance: %0.6400000000000006Avance: %0.6499999999999915Avance: %0.6600000000000108Avance: %0.6700000000000017Avance: %0.6800000000000068Avance: %0.6899999999999977Avance: %0.7000000000000028Avance: %0.7099999999999937Avance: %0.7199999999999989Avance: %0.730000000000004Avance: %0.7399999999999949Avance: %0.75Avance: %0.7600000000000051Avance: %0.7700000000000102Avance: %0.7800000000000011Avance: %0.7900000000000063Avance: %0.7999999999999972Avance: %0.8100000000000023Avance: %0.8199999999999932Avance: %0.8299999999999983Avance: %0.8400000000000034Avance: %0.8499999999999943Avance: %0.8599999999999994Avance: %0.8700000000000045Avance: %0.8800000000000097Avance: %0.8900000000000006Avance: %0.9000000000000057Avance: %0.9099999999999966Avance: %0.9200000000000017Avance: %0.9299999999999926Avance: %0.9399999999999977Avance: %0.9499999999999886Avance: %0.960000000000008Avance: %0.9699999999999989Avance: %0.980000000000004Avance: %0.9900000000000091Avance: %1.0Avance: %1.0100000000000051Avance: %1.019999999999996Avance: %1.0300000000000011Avance: %1.039999999999992Avance: %1.0499999999999972Avance: %1.0600000000000023Avance: %1.0700000000000074Avance: %1.0799999999999983Avance: %1.0900000000000034Avance: %1.0999999999999943Avance: %1.1099999999999994Avance: %1.1200000000000045Avance: %1.1299999999999955Avance: %1.1400000000000006Avance: %1.1499999999999915Avance: %1.1600000000000108Avance: %1.1700000000000017Avance: %1.1800000000000068Avance: %1.1899999999999977Avance: %1.2000000000000028Avance: %1.2099999999999937Avance: %1.2199999999999989Avance: %1.230000000000004Avance: %1.2399999999999949Avance: %1.25Avance: %1.259999999999991Avance: %1.2700000000000102Avance: %1.2800000000000011Avance: %1.2900000000000063Avance: %1.2999999999999972Avance: %1.3100000000000023Avance: %1.3199999999999932Avance: %1.3299999999999983Avance: %1.3400000000000034Avance: %1.3499999999999943Avance: %1.3599999999999994Avance: %1.3700000000000045Avance: %1.3800000000000097Avance: %1.3900000000000006Avance: %1.4000000000000057Avance: %1.4099999999999966Avance: %1.4200000000000017Avance: %1.4299999999999926Avance: %1.4399999999999977Avance: %1.4499999999999886Avance: %1.4599999999999937Avance: %1.4699999999999989Avance: %1.480000000000004Avance: %1.490000000000009Avance: %1.5Avance: %1.5100000000000051Avance: %1.519999999999996Avance: %1.5300000000000011Avance: %1.539999999999992Avance: %1.5499999999999972Avance: %1.559999999999988Avance: %1.5700000000000074Avance: %1.5799999999999983Avance: %1.5900000000000034Avance: %1.5999999999999943Avance: %1.6099999999999994Avance: %1.6200000000000045Avance: %1.6299999999999955Avance: %1.6400000000000006Avance: %1.6499999999999915Avance: %1.6599999999999966Avance: %1.6700000000000017Avance: %1.6800000000000068Avance: %1.6899999999999977Avance: %1.7000000000000028Avance: %1.7099999999999937Avance: %1.7199999999999989Avance: %1.730000000000004Avance: %1.7399999999999949Avance: %1.75Avance: %1.759999999999991Avance: %1.7700000000000102Avance: %1.7800000000000011Avance: %1.7900000000000063Avance: %1.7999999999999972Avance: %1.8100000000000023Avance: %1.8199999999999932Avance: %1.8299999999999983Avance: %1.8400000000000034Avance: %1.8499999999999943Avance: %1.8599999999999994Avance: %1.8700000000000045Avance: %1.8800000000000097Avance: %1.8900000000000006Avance: %1.9000000000000057Avance: %1.9099999999999966Avance: %1.9200000000000017Avance: %1.9299999999999926Avance: %1.9399999999999977Avance: %1.9500000000000028Avance: %1.9599999999999937Avance: %1.9699999999999989Avance: %1.980000000000004Avance: %1.990000000000009Avance: %2.0Avance: %2.010000000000005Avance: %2.019999999999996Avance: %2.030000000000001Avance: %2.039999999999992Avance: %2.049999999999997Avance: %2.0600000000000023Avance: %2.0700000000000074Avance: %2.0799999999999983Avance: %2.0900000000000034Avance: %2.1000000000000085Avance: %2.1099999999999994Avance: %2.1200000000000045Avance: %2.1299999999999955Avance: %2.1400000000000006Avance: %2.1499999999999915Avance: %2.1599999999999966Avance: %2.1700000000000017Avance: %2.180000000000007Avance: %2.1899999999999977Avance: %2.200000000000003Avance: %2.210000000000008Avance: %2.219999999999999Avance: %2.230000000000004Avance: %2.239999999999995Avance: %2.25Avance: %2.259999999999991Avance: %2.2700000000000102Avance: %2.280000000000001Avance: %2.2900000000000063Avance: %2.299999999999997Avance: %2.3100000000000023Avance: %2.319999999999993Avance: %2.3299999999999983Avance: %2.3400000000000034Avance: %2.3499999999999943Avance: %2.3599999999999994Avance: %2.3700000000000045Avance: %2.3800000000000097Avance: %2.3900000000000006Avance: %2.4000000000000057Avance: %2.4099999999999966Avance: %2.4200000000000017Avance: %2.4299999999999926Avance: %2.4399999999999977Avance: %2.450000000000003Avance: %2.4599999999999937Avance: %2.469999999999999Avance: %2.480000000000004Avance: %2.490000000000009Avance: %2.5Avance: %2.510000000000005Avance: %2.519999999999996Avance: %2.530000000000001Avance: %2.539999999999992Avance: %2.549999999999997Avance: %2.5600000000000023Avance: %2.569999999999993Avance: %2.5799999999999983Avance: %2.5900000000000034Avance: %2.6000000000000085Avance: %2.6099999999999994Avance: %2.6200000000000045Avance: %2.6299999999999955Avance: %2.6400000000000006Avance: %2.6499999999999915Avance: %2.6599999999999966Avance: %2.6699999999999875Avance: %2.680000000000007Avance: %2.6899999999999977Avance: %2.700000000000003Avance: %2.710000000000008Avance: %2.719999999999999Avance: %2.730000000000004Avance: %2.739999999999995Avance: %2.75Avance: %2.759999999999991Avance: %2.769999999999996Avance: %2.780000000000001Avance: %2.7900000000000063Avance: %2.799999999999997Avance: %2.8100000000000023Avance: %2.819999999999993Avance: %2.8299999999999983Avance: %2.8400000000000034Avance: %2.8499999999999943Avance: %2.8599999999999994Avance: %2.8699999999999903Avance: %2.8800000000000097Avance: %2.8900000000000006Avance: %2.9000000000000057Avance: %2.9099999999999966Avance: %2.9200000000000017Avance: %2.9299999999999926Avance: %2.9399999999999977Avance: %2.950000000000003Avance: %2.9599999999999937Avance: %2.969999999999999Avance: %2.980000000000004Avance: %2.990000000000009Avance: %3.0Avance: %3.010000000000005Avance: %3.019999999999996Avance: %3.030000000000001Avance: %3.039999999999992Avance: %3.049999999999997Avance: %3.0600000000000023Avance: %3.069999999999993Avance: %3.0799999999999983Avance: %3.0900000000000034Avance: %3.1000000000000085Avance: %3.1099999999999994Avance: %3.1200000000000045Avance: %3.1299999999999955Avance: %3.1400000000000006Avance: %3.1499999999999915Avance: %3.1599999999999966Avance: %3.1700000000000017Avance: %3.180000000000007Avance: %3.1899999999999977Avance: %3.200000000000003Avance: %3.210000000000008Avance: %3.219999999999999Avance: %3.230000000000004Avance: %3.239999999999995Avance: %3.25Avance: %3.259999999999991Avance: %3.269999999999996Avance: %3.280000000000001Avance: %3.2900000000000063Avance: %3.299999999999997Avance: %3.3100000000000023Avance: %3.3200000000000074Avance: %3.3299999999999983Avance: %3.3400000000000034Avance: %3.3499999999999943Avance: %3.3599999999999994Avance: %3.3699999999999903Avance: %3.3800000000000097Avance: %3.3900000000000006Avance: %3.4000000000000057Avance: %3.4099999999999966Avance: %3.4200000000000017Avance: %3.430000000000007Avance: %3.4399999999999977Avance: %3.450000000000003Avance: %3.4599999999999937Avance: %3.469999999999999Avance: %3.480000000000004Avance: %3.490000000000009Avance: %3.5Avance: %3.510000000000005Avance: %3.519999999999996Avance: %3.530000000000001Avance: %3.539999999999992Avance: %3.549999999999997Avance: %3.5600000000000023Avance: %3.569999999999993Avance: %3.5800000000000125Avance: %3.5900000000000034Avance: %3.6000000000000085Avance: %3.6099999999999994Avance: %3.6200000000000045Avance: %3.6299999999999955Avance: %3.6400000000000006Avance: %3.6499999999999915Avance: %3.6599999999999966Avance: %3.6700000000000017Avance: %3.680000000000007Avance: %3.6899999999999977Avance: %3.700000000000003Avance: %3.710000000000008Avance: %3.719999999999999Avance: %3.730000000000004Avance: %3.739999999999995Avance: %3.75Avance: %3.759999999999991Avance: %3.769999999999996Avance: %3.780000000000001Avance: %3.7900000000000063Avance: %3.799999999999997Avance: %3.8100000000000023Avance: %3.8200000000000074Avance: %3.8299999999999983Avance: %3.8400000000000034Avance: %3.8499999999999943Avance: %3.8599999999999994Avance: %3.8699999999999903Avance: %3.8799999999999955Avance: %3.8900000000000006Avance: %3.9000000000000057Avance: %3.9099999999999966Avance: %3.9200000000000017Avance: %3.930000000000007Avance: %3.9399999999999977Avance: %3.950000000000003Avance: %3.9599999999999937Avance: %3.969999999999999Avance: %3.9799999999999898Avance: %3.990000000000009Avance: %4.0Avance: %4.010000000000005Avance: %4.019999999999996Avance: %4.030000000000001Avance: %4.039999999999992Avance: %4.049999999999997Avance: %4.060000000000002Avance: %4.069999999999993Avance: %4.079999999999998Avance: %4.090000000000003Avance: %4.1000000000000085Avance: %4.109999999999999Avance: %4.1200000000000045Avance: %4.1299999999999955Avance: %4.140000000000001Avance: %4.1499999999999915Avance: %4.159999999999997Avance: %4.170000000000002Avance: %4.179999999999993Avance: %4.189999999999998Avance: %4.200000000000003Avance: %4.210000000000008Avance: %4.219999999999999Avance: %4.230000000000004Avance: %4.239999999999995Avance: %4.25Avance: %4.259999999999991Avance: %4.269999999999996Avance: %4.280000000000001Avance: %4.290000000000006Avance: %4.299999999999997Avance: %4.310000000000002Avance: %4.320000000000007Avance: %4.329999999999998Avance: %4.340000000000003Avance: %4.349999999999994Avance: %4.359999999999999Avance: %4.36999999999999Avance: %4.3799999999999955Avance: %4.390000000000001Avance: %4.400000000000006Avance: %4.409999999999997Avance: %4.420000000000002Avance: %4.430000000000007Avance: %4.439999999999998Avance: %4.450000000000003Avance: %4.459999999999994Avance: %4.469999999999999Avance: %4.47999999999999Avance: %4.490000000000009Avance: %4.5Avance: %4.510000000000005Avance: %4.519999999999996Avance: %4.530000000000001Avance: %4.540000000000006Avance: %4.549999999999997Avance: %4.560000000000002Avance: %4.569999999999993Avance: %4.579999999999998Avance: %4.590000000000003Avance: %4.6000000000000085Avance: %4.609999999999999Avance: %4.6200000000000045Avance: %4.6299999999999955Avance: %4.640000000000001Avance: %4.650000000000006Avance: %4.659999999999997Avance: %4.670000000000002Avance: %4.679999999999993Avance: %4.690000000000012Avance: %4.700000000000003Avance: %4.710000000000008Avance: %4.719999999999999Avance: %4.730000000000004Avance: %4.739999999999995Avance: %4.75Avance: %4.759999999999991Avance: %4.769999999999996Avance: %4.780000000000001Avance: %4.790000000000006Avance: %4.800000000000011Avance: %4.810000000000002Avance: %4.820000000000007Avance: %4.829999999999998Avance: %4.840000000000003Avance: %4.849999999999994Avance: %4.859999999999999Avance: %4.86999999999999Avance: %4.8799999999999955Avance: %4.890000000000001Avance: %4.900000000000006Avance: %4.909999999999997Avance: %4.920000000000002Avance: %4.930000000000007Avance: %4.939999999999998Avance: %4.950000000000003Avance: %4.959999999999994Avance: %4.969999999999999Done Extracting Features 0:04:38.742445
The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.
Try some users for user 143564478
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 537) (2000, 537) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6954
        True       1.00      1.00      1.00        46

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1987
        True       1.00      1.00      1.00        13

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 37507486
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 395) (2000, 395) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6792
        True       1.00      0.99      0.99       208

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1941
        True       1.00      0.98      0.99        59

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.99      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 151058509
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 219) (2000, 219) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6704
        True       1.00      1.00      1.00       296

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1915
        True       1.00      0.99      0.99        85

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.99      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 269820826
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 667) (2000, 667) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6996
        True       1.00      1.00      1.00         4

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1999
        True       1.00      1.00      1.00         1

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 2340581
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 628) (2000, 628) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6805
        True       1.00      1.00      1.00       195

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1944
        True       1.00      1.00      1.00        56

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 160004720
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 461) (2000, 461) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6950
        True       1.00      1.00      1.00        50

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1985
        True       1.00      1.00      1.00        15

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 131728769
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 413) (2000, 413) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6942
        True       1.00      1.00      1.00        58

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1983
        True       1.00      1.00      1.00        17

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 62903173
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 656) (2000, 656) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6923
        True       1.00      0.99      0.99        77

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1978
        True       1.00      1.00      1.00        22

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 801202
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 833) (2000, 833) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6891
        True       1.00      1.00      1.00       109

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1969
        True       1.00      1.00      1.00        31

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 131068673
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 540) (2000, 540) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6757
        True       1.00      1.00      1.00       243

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1931
        True       1.00      1.00      1.00        69

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 312146187
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 337) (2000, 337) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6996
        True       1.00      1.00      1.00         4

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1999
        True       1.00      1.00      1.00         1

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 1652541
Loading df
Done loading df. DF shape is :(12397359, 8) (Original: (12397359, 8)) 		Time delta is: None mins
Loading graph from ../data/graphs/latest_subgraph.gpickle
Fetched 309 level 2 neighbourhs for user 1652541.
Len of own retweets (positive examples) is (2, 2)
done getting tweets universe. Shape is  (12717, 2)
	Dataset was truncated to 10000 tweets
Extracting features Optimized. X shape is : (10000, 309)
Avance: %0.010000000000005116Avance: %0.01999999999999602Avance: %0.030000000000001137Avance: %0.03999999999999204Avance: %0.04999999999999716Avance: %0.060000000000002274Avance: %0.07000000000000739Avance: %0.0799999999999983Avance: %0.09000000000000341Avance: %0.09999999999999432Avance: %0.10999999999999943Avance: %0.12000000000000455Avance: %0.12999999999999545Avance: %0.14000000000000057Avance: %0.14999999999999147Avance: %0.1600000000000108Avance: %0.1700000000000017Avance: %0.18000000000000682Avance: %0.18999999999999773Avance: %0.20000000000000284Avance: %0.20999999999999375Avance: %0.21999999999999886Avance: %0.22999999999998977Avance: %0.23999999999999488Avance: %0.25Avance: %0.2600000000000051Avance: %0.27000000000001023Avance: %0.28000000000000114Avance: %0.29000000000000625Avance: %0.29999999999999716Avance: %0.3100000000000023Avance: %0.3199999999999932Avance: %0.3299999999999983Avance: %0.3399999999999892Avance: %0.3499999999999943Avance: %0.35999999999999943Avance: %0.37000000000000455Avance: %0.37999999999999545Avance: %0.39000000000000057Avance: %0.4000000000000057Avance: %0.4099999999999966Avance: %0.4200000000000017Avance: %0.4299999999999926Avance: %0.4399999999999977Avance: %0.44999999999998863Avance: %0.46000000000000796Avance: %0.46999999999999886Avance: %0.480000000000004Avance: %0.4899999999999949Avance: %0.5Avance: %0.5100000000000051Avance: %0.519999999999996Avance: %0.5300000000000011Avance: %0.539999999999992Avance: %0.5499999999999972Avance: %0.5600000000000023Avance: %0.5700000000000074Avance: %0.5799999999999983Avance: %0.5900000000000034Avance: %0.5999999999999943Avance: %0.6099999999999994Avance: %0.6200000000000045Avance: %0.6299999999999955Avance: %0.6400000000000006Avance: %0.6499999999999915Avance: %0.6600000000000108Avance: %0.6700000000000017Avance: %0.6800000000000068Avance: %0.6899999999999977Avance: %0.7000000000000028Avance: %0.7099999999999937Avance: %0.7199999999999989Avance: %0.730000000000004Avance: %0.7399999999999949Avance: %0.75Avance: %0.7600000000000051Avance: %0.7700000000000102Avance: %0.7800000000000011Avance: %0.7900000000000063Avance: %0.7999999999999972Avance: %0.8100000000000023Avance: %0.8199999999999932Avance: %0.8299999999999983Avance: %0.8400000000000034Avance: %0.8499999999999943Avance: %0.8599999999999994Avance: %0.8700000000000045Avance: %0.8800000000000097Avance: %0.8900000000000006Avance: %0.9000000000000057Avance: %0.9099999999999966Avance: %0.9200000000000017Avance: %0.9299999999999926Avance: %0.9399999999999977Avance: %0.9499999999999886Avance: %0.960000000000008Avance: %0.9699999999999989Avance: %0.980000000000004Avance: %0.9900000000000091Avance: %1.0Avance: %1.0100000000000051Avance: %1.019999999999996Avance: %1.0300000000000011Avance: %1.039999999999992Avance: %1.0499999999999972Avance: %1.0600000000000023Avance: %1.0700000000000074Avance: %1.0799999999999983Avance: %1.0900000000000034Avance: %1.0999999999999943Avance: %1.1099999999999994Avance: %1.1200000000000045Avance: %1.1299999999999955Avance: %1.1400000000000006Avance: %1.1499999999999915Avance: %1.1600000000000108Avance: %1.1700000000000017Avance: %1.1800000000000068Avance: %1.1899999999999977Avance: %1.2000000000000028Avance: %1.2099999999999937Avance: %1.2199999999999989Avance: %1.230000000000004Avance: %1.2399999999999949Avance: %1.25Avance: %1.259999999999991Avance: %1.2700000000000102Avance: %1.2800000000000011Avance: %1.2900000000000063Avance: %1.2999999999999972Avance: %1.3100000000000023Avance: %1.3199999999999932Avance: %1.3299999999999983Avance: %1.3400000000000034Avance: %1.3499999999999943Avance: %1.3599999999999994Avance: %1.3700000000000045Avance: %1.3800000000000097Avance: %1.3900000000000006Avance: %1.4000000000000057Avance: %1.4099999999999966Avance: %1.4200000000000017Avance: %1.4299999999999926Avance: %1.4399999999999977Avance: %1.4499999999999886Avance: %1.4599999999999937Avance: %1.4699999999999989Avance: %1.480000000000004Avance: %1.490000000000009Avance: %1.5Avance: %1.5100000000000051Avance: %1.519999999999996Avance: %1.5300000000000011Avance: %1.539999999999992Avance: %1.5499999999999972Avance: %1.559999999999988Avance: %1.5700000000000074Avance: %1.5799999999999983Avance: %1.5900000000000034Avance: %1.5999999999999943Avance: %1.6099999999999994Avance: %1.6200000000000045Avance: %1.6299999999999955Avance: %1.6400000000000006Avance: %1.6499999999999915Avance: %1.6599999999999966Avance: %1.6700000000000017Avance: %1.6800000000000068Avance: %1.6899999999999977Avance: %1.7000000000000028Avance: %1.7099999999999937Avance: %1.7199999999999989Avance: %1.730000000000004Avance: %1.7399999999999949Avance: %1.75Avance: %1.759999999999991Avance: %1.7700000000000102Avance: %1.7800000000000011Avance: %1.7900000000000063Avance: %1.7999999999999972Avance: %1.8100000000000023Avance: %1.8199999999999932Avance: %1.8299999999999983Avance: %1.8400000000000034Avance: %1.8499999999999943Avance: %1.8599999999999994Avance: %1.8700000000000045Avance: %1.8800000000000097Avance: %1.8900000000000006Avance: %1.9000000000000057Avance: %1.9099999999999966Avance: %1.9200000000000017Avance: %1.9299999999999926Avance: %1.9399999999999977Avance: %1.9500000000000028Avance: %1.9599999999999937Avance: %1.9699999999999989Avance: %1.980000000000004Avance: %1.990000000000009Avance: %2.0Avance: %2.010000000000005Avance: %2.019999999999996Avance: %2.030000000000001Avance: %2.039999999999992Avance: %2.049999999999997Avance: %2.0600000000000023Avance: %2.0700000000000074Avance: %2.0799999999999983Avance: %2.0900000000000034Avance: %2.1000000000000085Avance: %2.1099999999999994Avance: %2.1200000000000045Avance: %2.1299999999999955Avance: %2.1400000000000006Avance: %2.1499999999999915Avance: %2.1599999999999966Avance: %2.1700000000000017Avance: %2.180000000000007Avance: %2.1899999999999977Avance: %2.200000000000003Avance: %2.210000000000008Avance: %2.219999999999999Avance: %2.230000000000004Avance: %2.239999999999995Avance: %2.25Avance: %2.259999999999991Avance: %2.2700000000000102Avance: %2.280000000000001Avance: %2.2900000000000063Avance: %2.299999999999997Avance: %2.3100000000000023Avance: %2.319999999999993Avance: %2.3299999999999983Avance: %2.3400000000000034Avance: %2.3499999999999943Avance: %2.3599999999999994Avance: %2.3700000000000045Avance: %2.3800000000000097Avance: %2.3900000000000006Avance: %2.4000000000000057Avance: %2.4099999999999966Avance: %2.4200000000000017Avance: %2.4299999999999926Avance: %2.4399999999999977Avance: %2.450000000000003Avance: %2.4599999999999937Avance: %2.469999999999999Avance: %2.480000000000004Avance: %2.490000000000009Avance: %2.5Avance: %2.510000000000005Avance: %2.519999999999996Avance: %2.530000000000001Avance: %2.539999999999992Avance: %2.549999999999997Avance: %2.5600000000000023Avance: %2.569999999999993Avance: %2.5799999999999983Avance: %2.5900000000000034Avance: %2.6000000000000085Avance: %2.6099999999999994Avance: %2.6200000000000045Avance: %2.6299999999999955Avance: %2.6400000000000006Avance: %2.6499999999999915Avance: %2.6599999999999966Avance: %2.6699999999999875Avance: %2.680000000000007Avance: %2.6899999999999977Avance: %2.700000000000003Avance: %2.710000000000008Avance: %2.719999999999999Avance: %2.730000000000004Avance: %2.739999999999995Avance: %2.75Avance: %2.759999999999991Avance: %2.769999999999996Avance: %2.780000000000001Avance: %2.7900000000000063Avance: %2.799999999999997Avance: %2.8100000000000023Avance: %2.819999999999993Avance: %2.8299999999999983Avance: %2.8400000000000034Avance: %2.8499999999999943Avance: %2.8599999999999994Avance: %2.8699999999999903Avance: %2.8800000000000097Avance: %2.8900000000000006Avance: %2.9000000000000057Avance: %2.9099999999999966Avance: %2.9200000000000017Avance: %2.9299999999999926Avance: %2.9399999999999977Avance: %2.950000000000003Avance: %2.9599999999999937Avance: %2.969999999999999Avance: %2.980000000000004Avance: %2.990000000000009Avance: %3.0Avance: %3.010000000000005Avance: %3.019999999999996Avance: %3.030000000000001Avance: %3.039999999999992Avance: %3.049999999999997Avance: %3.0600000000000023Avance: %3.069999999999993Avance: %3.0799999999999983Avance: %3.0900000000000034Done Extracting Features 0:02:34.002866
The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.
Try some users for user 146953504
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 648) (2000, 648) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6852
        True       1.00      1.00      1.00       148

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1957
        True       1.00      1.00      1.00        43

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 72925674
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 576) (2000, 576) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6996
        True       1.00      1.00      1.00         4

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1999
        True       1.00      1.00      1.00         1

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 7840542
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 663) (2000, 663) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6965
        True       1.00      1.00      1.00        35

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1990
        True       1.00      1.00      1.00        10

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 182916952
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 1020) (2000, 1020) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6954
        True       1.00      0.96      0.98        46

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.98      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1987
        True       1.00      1.00      1.00        13

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 154753258
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 316) (2000, 316) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6871
        True       1.00      0.95      0.97       129

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.97      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1963
        True       1.00      0.92      0.96        37

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.96      0.98      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 69628317
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 485) (2000, 485) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6949
        True       1.00      1.00      1.00        51

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1985
        True       1.00      1.00      1.00        15

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 517662064
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 701) (2000, 701) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6993
        True       1.00      1.00      1.00         7

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1998
        True       1.00      1.00      1.00         2

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 182512109
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 391) (2000, 391) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6926
        True       1.00      0.99      0.99        74

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1979
        True       1.00      1.00      1.00        21

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 192728611
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 573) (2000, 573) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6965
        True       1.00      1.00      1.00        35

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1990
        True       1.00      1.00      1.00        10

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 63039879
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 489) (2000, 489) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6986
        True       1.00      1.00      1.00        14

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1996
        True       1.00      1.00      1.00         4

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 149254440
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 419) (2000, 419) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       0.99      1.00      1.00      6572
        True       1.00      0.91      0.95       428

   micro avg       0.99      0.99      0.99      7000
   macro avg       1.00      0.96      0.98      7000
weighted avg       0.99      0.99      0.99      7000


Scores on test set.

              precision    recall  f1-score   support

       False       0.99      1.00      1.00      1878
        True       1.00      0.90      0.95       122

   micro avg       0.99      0.99      0.99      2000
   macro avg       1.00      0.95      0.97      2000
weighted avg       0.99      0.99      0.99      2000


			END SVC
Try some users for user 258860022
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 490) (2000, 490) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6832
        True       1.00      1.00      1.00       168

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1952
        True       1.00      0.98      0.99        48

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.99      0.99      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 8010502
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 845) (2000, 845) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6895
        True       1.00      0.99      1.00       105

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1970
        True       1.00      1.00      1.00        30

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 914367366
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 601) (2000, 601) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6989
        True       1.00      1.00      1.00        11

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1997
        True       1.00      1.00      1.00         3

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 322780110
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 389) (2000, 389) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6913
        True       1.00      0.85      0.92        87

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.93      0.96      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1975
        True       1.00      1.00      1.00        25

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 9656262
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 421) (2000, 421) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6882
        True       1.00      0.97      0.98       118

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.98      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1967
        True       1.00      1.00      1.00        33

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 20450173
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 396) (2000, 396) False False
The number of classes has to be greater than one; got 1 class
Try some users for user 85130724
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 624) (2000, 624) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6973
        True       1.00      0.96      0.98        27

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.98      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1992
        True       1.00      1.00      1.00         8

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 284811417
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 775) (2000, 775) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6982
        True       1.00      1.00      1.00        18

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1995
        True       1.00      1.00      1.00         5

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 73190286
Loading df
Done loading df. DF shape is :(12397359, 8) (Original: (12397359, 8)) 		Time delta is: None mins
Loading graph from ../data/graphs/latest_subgraph.gpickle
Fetched 359 level 2 neighbourhs for user 73190286.
Len of own retweets (positive examples) is (4, 2)
done getting tweets universe. Shape is  (11734, 2)
	Dataset was truncated to 10000 tweets
Extracting features Optimized. X shape is : (10000, 359)
Avance: %0.010000000000005116Avance: %0.01999999999999602Avance: %0.030000000000001137Avance: %0.03999999999999204Avance: %0.04999999999999716Avance: %0.060000000000002274Avance: %0.07000000000000739Avance: %0.0799999999999983Avance: %0.09000000000000341Avance: %0.09999999999999432Avance: %0.10999999999999943Avance: %0.12000000000000455Avance: %0.12999999999999545Avance: %0.14000000000000057Avance: %0.14999999999999147Avance: %0.1600000000000108Avance: %0.1700000000000017Avance: %0.18000000000000682Avance: %0.18999999999999773Avance: %0.20000000000000284Avance: %0.20999999999999375Avance: %0.21999999999999886Avance: %0.22999999999998977Avance: %0.23999999999999488Avance: %0.25Avance: %0.2600000000000051Avance: %0.27000000000001023Avance: %0.28000000000000114Avance: %0.29000000000000625Avance: %0.29999999999999716Avance: %0.3100000000000023Avance: %0.3199999999999932Avance: %0.3299999999999983Avance: %0.3399999999999892Avance: %0.3499999999999943Avance: %0.35999999999999943Avance: %0.37000000000000455Avance: %0.37999999999999545Avance: %0.39000000000000057Avance: %0.4000000000000057Avance: %0.4099999999999966Avance: %0.4200000000000017Avance: %0.4299999999999926Avance: %0.4399999999999977Avance: %0.44999999999998863Avance: %0.46000000000000796Avance: %0.46999999999999886Avance: %0.480000000000004Avance: %0.4899999999999949Avance: %0.5Avance: %0.5100000000000051Avance: %0.519999999999996Avance: %0.5300000000000011Avance: %0.539999999999992Avance: %0.5499999999999972Avance: %0.5600000000000023Avance: %0.5700000000000074Avance: %0.5799999999999983Avance: %0.5900000000000034Avance: %0.5999999999999943Avance: %0.6099999999999994Avance: %0.6200000000000045Avance: %0.6299999999999955Avance: %0.6400000000000006Avance: %0.6499999999999915Avance: %0.6600000000000108Avance: %0.6700000000000017Avance: %0.6800000000000068Avance: %0.6899999999999977Avance: %0.7000000000000028Avance: %0.7099999999999937Avance: %0.7199999999999989Avance: %0.730000000000004Avance: %0.7399999999999949Avance: %0.75Avance: %0.7600000000000051Avance: %0.7700000000000102Avance: %0.7800000000000011Avance: %0.7900000000000063Avance: %0.7999999999999972Avance: %0.8100000000000023Avance: %0.8199999999999932Avance: %0.8299999999999983Avance: %0.8400000000000034Avance: %0.8499999999999943Avance: %0.8599999999999994Avance: %0.8700000000000045Avance: %0.8800000000000097Avance: %0.8900000000000006Avance: %0.9000000000000057Avance: %0.9099999999999966Avance: %0.9200000000000017Avance: %0.9299999999999926Avance: %0.9399999999999977Avance: %0.9499999999999886Avance: %0.960000000000008Avance: %0.9699999999999989Avance: %0.980000000000004Avance: %0.9900000000000091Avance: %1.0Avance: %1.0100000000000051Avance: %1.019999999999996Avance: %1.0300000000000011Avance: %1.039999999999992Avance: %1.0499999999999972Avance: %1.0600000000000023Avance: %1.0700000000000074Avance: %1.0799999999999983Avance: %1.0900000000000034Avance: %1.0999999999999943Avance: %1.1099999999999994Avance: %1.1200000000000045Avance: %1.1299999999999955Avance: %1.1400000000000006Avance: %1.1499999999999915Avance: %1.1600000000000108Avance: %1.1700000000000017Avance: %1.1800000000000068Avance: %1.1899999999999977Avance: %1.2000000000000028Avance: %1.2099999999999937Avance: %1.2199999999999989Avance: %1.230000000000004Avance: %1.2399999999999949Avance: %1.25Avance: %1.259999999999991Avance: %1.2700000000000102Avance: %1.2800000000000011Avance: %1.2900000000000063Avance: %1.2999999999999972Avance: %1.3100000000000023Avance: %1.3199999999999932Avance: %1.3299999999999983Avance: %1.3400000000000034Avance: %1.3499999999999943Avance: %1.3599999999999994Avance: %1.3700000000000045Avance: %1.3800000000000097Avance: %1.3900000000000006Avance: %1.4000000000000057Avance: %1.4099999999999966Avance: %1.4200000000000017Avance: %1.4299999999999926Avance: %1.4399999999999977Avance: %1.4499999999999886Avance: %1.4599999999999937Avance: %1.4699999999999989Avance: %1.480000000000004Avance: %1.490000000000009Avance: %1.5Avance: %1.5100000000000051Avance: %1.519999999999996Avance: %1.5300000000000011Avance: %1.539999999999992Avance: %1.5499999999999972Avance: %1.559999999999988Avance: %1.5700000000000074Avance: %1.5799999999999983Avance: %1.5900000000000034Avance: %1.5999999999999943Avance: %1.6099999999999994Avance: %1.6200000000000045Avance: %1.6299999999999955Avance: %1.6400000000000006Avance: %1.6499999999999915Avance: %1.6599999999999966Avance: %1.6700000000000017Avance: %1.6800000000000068Avance: %1.6899999999999977Avance: %1.7000000000000028Avance: %1.7099999999999937Avance: %1.7199999999999989Avance: %1.730000000000004Avance: %1.7399999999999949Avance: %1.75Avance: %1.759999999999991Avance: %1.7700000000000102Avance: %1.7800000000000011Avance: %1.7900000000000063Avance: %1.7999999999999972Avance: %1.8100000000000023Avance: %1.8199999999999932Avance: %1.8299999999999983Avance: %1.8400000000000034Avance: %1.8499999999999943Avance: %1.8599999999999994Avance: %1.8700000000000045Avance: %1.8800000000000097Avance: %1.8900000000000006Avance: %1.9000000000000057Avance: %1.9099999999999966Avance: %1.9200000000000017Avance: %1.9299999999999926Avance: %1.9399999999999977Avance: %1.9500000000000028Avance: %1.9599999999999937Avance: %1.9699999999999989Avance: %1.980000000000004Avance: %1.990000000000009Avance: %2.0Avance: %2.010000000000005Avance: %2.019999999999996Avance: %2.030000000000001Avance: %2.039999999999992Avance: %2.049999999999997Avance: %2.0600000000000023Avance: %2.0700000000000074Avance: %2.0799999999999983Avance: %2.0900000000000034Avance: %2.1000000000000085Avance: %2.1099999999999994Avance: %2.1200000000000045Avance: %2.1299999999999955Avance: %2.1400000000000006Avance: %2.1499999999999915Avance: %2.1599999999999966Avance: %2.1700000000000017Avance: %2.180000000000007Avance: %2.1899999999999977Avance: %2.200000000000003Avance: %2.210000000000008Avance: %2.219999999999999Avance: %2.230000000000004Avance: %2.239999999999995Avance: %2.25Avance: %2.259999999999991Avance: %2.2700000000000102Avance: %2.280000000000001Avance: %2.2900000000000063Avance: %2.299999999999997Avance: %2.3100000000000023Avance: %2.319999999999993Avance: %2.3299999999999983Avance: %2.3400000000000034Avance: %2.3499999999999943Avance: %2.3599999999999994Avance: %2.3700000000000045Avance: %2.3800000000000097Avance: %2.3900000000000006Avance: %2.4000000000000057Avance: %2.4099999999999966Avance: %2.4200000000000017Avance: %2.4299999999999926Avance: %2.4399999999999977Avance: %2.450000000000003Avance: %2.4599999999999937Avance: %2.469999999999999Avance: %2.480000000000004Avance: %2.490000000000009Avance: %2.5Avance: %2.510000000000005Avance: %2.519999999999996Avance: %2.530000000000001Avance: %2.539999999999992Avance: %2.549999999999997Avance: %2.5600000000000023Avance: %2.569999999999993Avance: %2.5799999999999983Avance: %2.5900000000000034Avance: %2.6000000000000085Avance: %2.6099999999999994Avance: %2.6200000000000045Avance: %2.6299999999999955Avance: %2.6400000000000006Avance: %2.6499999999999915Avance: %2.6599999999999966Avance: %2.6699999999999875Avance: %2.680000000000007Avance: %2.6899999999999977Avance: %2.700000000000003Avance: %2.710000000000008Avance: %2.719999999999999Avance: %2.730000000000004Avance: %2.739999999999995Avance: %2.75Avance: %2.759999999999991Avance: %2.769999999999996Avance: %2.780000000000001Avance: %2.7900000000000063Avance: %2.799999999999997Avance: %2.8100000000000023Avance: %2.819999999999993Avance: %2.8299999999999983Avance: %2.8400000000000034Avance: %2.8499999999999943Avance: %2.8599999999999994Avance: %2.8699999999999903Avance: %2.8800000000000097Avance: %2.8900000000000006Avance: %2.9000000000000057Avance: %2.9099999999999966Avance: %2.9200000000000017Avance: %2.9299999999999926Avance: %2.9399999999999977Avance: %2.950000000000003Avance: %2.9599999999999937Avance: %2.969999999999999Avance: %2.980000000000004Avance: %2.990000000000009Avance: %3.0Avance: %3.010000000000005Avance: %3.019999999999996Avance: %3.030000000000001Avance: %3.039999999999992Avance: %3.049999999999997Avance: %3.0600000000000023Avance: %3.069999999999993Avance: %3.0799999999999983Avance: %3.0900000000000034Avance: %3.1000000000000085Avance: %3.1099999999999994Avance: %3.1200000000000045Avance: %3.1299999999999955Avance: %3.1400000000000006Avance: %3.1499999999999915Avance: %3.1599999999999966Avance: %3.1700000000000017Avance: %3.180000000000007Avance: %3.1899999999999977Avance: %3.200000000000003Avance: %3.210000000000008Avance: %3.219999999999999Avance: %3.230000000000004Avance: %3.239999999999995Avance: %3.25Avance: %3.259999999999991Avance: %3.269999999999996Avance: %3.280000000000001Avance: %3.2900000000000063Avance: %3.299999999999997Avance: %3.3100000000000023Avance: %3.3200000000000074Avance: %3.3299999999999983Avance: %3.3400000000000034Avance: %3.3499999999999943Avance: %3.3599999999999994Avance: %3.3699999999999903Avance: %3.3800000000000097Avance: %3.3900000000000006Avance: %3.4000000000000057Avance: %3.4099999999999966Avance: %3.4200000000000017Avance: %3.430000000000007Avance: %3.4399999999999977Avance: %3.450000000000003Avance: %3.4599999999999937Avance: %3.469999999999999Avance: %3.480000000000004Avance: %3.490000000000009Avance: %3.5Avance: %3.510000000000005Avance: %3.519999999999996Avance: %3.530000000000001Avance: %3.539999999999992Avance: %3.549999999999997Avance: %3.5600000000000023Avance: %3.569999999999993Avance: %3.5800000000000125Avance: %3.5900000000000034Done Extracting Features 0:02:59.155246
The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.
Try some users for user 183184266
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 247) (2000, 247) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6756
        True       1.00      1.00      1.00       244

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1930
        True       1.00      1.00      1.00        70

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 631374320
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 237) (2000, 237) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6974
        True       1.00      1.00      1.00        26

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1993
        True       1.00      1.00      1.00         7

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 38456951
Loading df
Done loading df. DF shape is :(12397359, 8) (Original: (12397359, 8)) 		Time delta is: None mins
Loading graph from ../data/graphs/latest_subgraph.gpickle
Fetched 673 level 2 neighbourhs for user 38456951.
Len of own retweets (positive examples) is (3, 2)
done getting tweets universe. Shape is  (11500, 2)
	Dataset was truncated to 10000 tweets
Extracting features Optimized. X shape is : (10000, 673)
Avance: %0.010000000000005116Avance: %0.01999999999999602Avance: %0.030000000000001137Avance: %0.03999999999999204Avance: %0.04999999999999716Avance: %0.060000000000002274Avance: %0.07000000000000739Avance: %0.0799999999999983Avance: %0.09000000000000341Avance: %0.09999999999999432Avance: %0.10999999999999943Avance: %0.12000000000000455Avance: %0.12999999999999545Avance: %0.14000000000000057Avance: %0.14999999999999147Avance: %0.1600000000000108Avance: %0.1700000000000017Avance: %0.18000000000000682Avance: %0.18999999999999773Avance: %0.20000000000000284Avance: %0.20999999999999375Avance: %0.21999999999999886Avance: %0.22999999999998977Avance: %0.23999999999999488Avance: %0.25Avance: %0.2600000000000051Avance: %0.27000000000001023Avance: %0.28000000000000114Avance: %0.29000000000000625Avance: %0.29999999999999716Avance: %0.3100000000000023Avance: %0.3199999999999932Avance: %0.3299999999999983Avance: %0.3399999999999892Avance: %0.3499999999999943Avance: %0.35999999999999943Avance: %0.37000000000000455Avance: %0.37999999999999545Avance: %0.39000000000000057Avance: %0.4000000000000057Avance: %0.4099999999999966Avance: %0.4200000000000017Avance: %0.4299999999999926Avance: %0.4399999999999977Avance: %0.44999999999998863Avance: %0.46000000000000796Avance: %0.46999999999999886Avance: %0.480000000000004Avance: %0.4899999999999949Avance: %0.5Avance: %0.5100000000000051Avance: %0.519999999999996Avance: %0.5300000000000011Avance: %0.539999999999992Avance: %0.5499999999999972Avance: %0.5600000000000023Avance: %0.5700000000000074Avance: %0.5799999999999983Avance: %0.5900000000000034Avance: %0.5999999999999943Avance: %0.6099999999999994Avance: %0.6200000000000045Avance: %0.6299999999999955Avance: %0.6400000000000006Avance: %0.6499999999999915Avance: %0.6600000000000108Avance: %0.6700000000000017Avance: %0.6800000000000068Avance: %0.6899999999999977Avance: %0.7000000000000028Avance: %0.7099999999999937Avance: %0.7199999999999989Avance: %0.730000000000004Avance: %0.7399999999999949Avance: %0.75Avance: %0.7600000000000051Avance: %0.7700000000000102Avance: %0.7800000000000011Avance: %0.7900000000000063Avance: %0.7999999999999972Avance: %0.8100000000000023Avance: %0.8199999999999932Avance: %0.8299999999999983Avance: %0.8400000000000034Avance: %0.8499999999999943Avance: %0.8599999999999994Avance: %0.8700000000000045Avance: %0.8800000000000097Avance: %0.8900000000000006Avance: %0.9000000000000057Avance: %0.9099999999999966Avance: %0.9200000000000017Avance: %0.9299999999999926Avance: %0.9399999999999977Avance: %0.9499999999999886Avance: %0.960000000000008Avance: %0.9699999999999989Avance: %0.980000000000004Avance: %0.9900000000000091Avance: %1.0Avance: %1.0100000000000051Avance: %1.019999999999996Avance: %1.0300000000000011Avance: %1.039999999999992Avance: %1.0499999999999972Avance: %1.0600000000000023Avance: %1.0700000000000074Avance: %1.0799999999999983Avance: %1.0900000000000034Avance: %1.0999999999999943Avance: %1.1099999999999994Avance: %1.1200000000000045Avance: %1.1299999999999955Avance: %1.1400000000000006Avance: %1.1499999999999915Avance: %1.1600000000000108Avance: %1.1700000000000017Avance: %1.1800000000000068Avance: %1.1899999999999977Avance: %1.2000000000000028Avance: %1.2099999999999937Avance: %1.2199999999999989Avance: %1.230000000000004Avance: %1.2399999999999949Avance: %1.25Avance: %1.259999999999991Avance: %1.2700000000000102Avance: %1.2800000000000011Avance: %1.2900000000000063Avance: %1.2999999999999972Avance: %1.3100000000000023Avance: %1.3199999999999932Avance: %1.3299999999999983Avance: %1.3400000000000034Avance: %1.3499999999999943Avance: %1.3599999999999994Avance: %1.3700000000000045Avance: %1.3800000000000097Avance: %1.3900000000000006Avance: %1.4000000000000057Avance: %1.4099999999999966Avance: %1.4200000000000017Avance: %1.4299999999999926Avance: %1.4399999999999977Avance: %1.4499999999999886Avance: %1.4599999999999937Avance: %1.4699999999999989Avance: %1.480000000000004Avance: %1.490000000000009Avance: %1.5Avance: %1.5100000000000051Avance: %1.519999999999996Avance: %1.5300000000000011Avance: %1.539999999999992Avance: %1.5499999999999972Avance: %1.559999999999988Avance: %1.5700000000000074Avance: %1.5799999999999983Avance: %1.5900000000000034Avance: %1.5999999999999943Avance: %1.6099999999999994Avance: %1.6200000000000045Avance: %1.6299999999999955Avance: %1.6400000000000006Avance: %1.6499999999999915Avance: %1.6599999999999966Avance: %1.6700000000000017Avance: %1.6800000000000068Avance: %1.6899999999999977Avance: %1.7000000000000028Avance: %1.7099999999999937Avance: %1.7199999999999989Avance: %1.730000000000004Avance: %1.7399999999999949Avance: %1.75Avance: %1.759999999999991Avance: %1.7700000000000102Avance: %1.7800000000000011Avance: %1.7900000000000063Avance: %1.7999999999999972Avance: %1.8100000000000023Avance: %1.8199999999999932Avance: %1.8299999999999983Avance: %1.8400000000000034Avance: %1.8499999999999943Avance: %1.8599999999999994Avance: %1.8700000000000045Avance: %1.8800000000000097Avance: %1.8900000000000006Avance: %1.9000000000000057Avance: %1.9099999999999966Avance: %1.9200000000000017Avance: %1.9299999999999926Avance: %1.9399999999999977Avance: %1.9500000000000028Avance: %1.9599999999999937Avance: %1.9699999999999989Avance: %1.980000000000004Avance: %1.990000000000009Avance: %2.0Avance: %2.010000000000005Avance: %2.019999999999996Avance: %2.030000000000001Avance: %2.039999999999992Avance: %2.049999999999997Avance: %2.0600000000000023Avance: %2.0700000000000074Avance: %2.0799999999999983Avance: %2.0900000000000034Avance: %2.1000000000000085Avance: %2.1099999999999994Avance: %2.1200000000000045Avance: %2.1299999999999955Avance: %2.1400000000000006Avance: %2.1499999999999915Avance: %2.1599999999999966Avance: %2.1700000000000017Avance: %2.180000000000007Avance: %2.1899999999999977Avance: %2.200000000000003Avance: %2.210000000000008Avance: %2.219999999999999Avance: %2.230000000000004Avance: %2.239999999999995Avance: %2.25Avance: %2.259999999999991Avance: %2.2700000000000102Avance: %2.280000000000001Avance: %2.2900000000000063Avance: %2.299999999999997Avance: %2.3100000000000023Avance: %2.319999999999993Avance: %2.3299999999999983Avance: %2.3400000000000034Avance: %2.3499999999999943Avance: %2.3599999999999994Avance: %2.3700000000000045Avance: %2.3800000000000097Avance: %2.3900000000000006Avance: %2.4000000000000057Avance: %2.4099999999999966Avance: %2.4200000000000017Avance: %2.4299999999999926Avance: %2.4399999999999977Avance: %2.450000000000003Avance: %2.4599999999999937Avance: %2.469999999999999Avance: %2.480000000000004Avance: %2.490000000000009Avance: %2.5Avance: %2.510000000000005Avance: %2.519999999999996Avance: %2.530000000000001Avance: %2.539999999999992Avance: %2.549999999999997Avance: %2.5600000000000023Avance: %2.569999999999993Avance: %2.5799999999999983Avance: %2.5900000000000034Avance: %2.6000000000000085Avance: %2.6099999999999994Avance: %2.6200000000000045Avance: %2.6299999999999955Avance: %2.6400000000000006Avance: %2.6499999999999915Avance: %2.6599999999999966Avance: %2.6699999999999875Avance: %2.680000000000007Avance: %2.6899999999999977Avance: %2.700000000000003Avance: %2.710000000000008Avance: %2.719999999999999Avance: %2.730000000000004Avance: %2.739999999999995Avance: %2.75Avance: %2.759999999999991Avance: %2.769999999999996Avance: %2.780000000000001Avance: %2.7900000000000063Avance: %2.799999999999997Avance: %2.8100000000000023Avance: %2.819999999999993Avance: %2.8299999999999983Avance: %2.8400000000000034Avance: %2.8499999999999943Avance: %2.8599999999999994Avance: %2.8699999999999903Avance: %2.8800000000000097Avance: %2.8900000000000006Avance: %2.9000000000000057Avance: %2.9099999999999966Avance: %2.9200000000000017Avance: %2.9299999999999926Avance: %2.9399999999999977Avance: %2.950000000000003Avance: %2.9599999999999937Avance: %2.969999999999999Avance: %2.980000000000004Avance: %2.990000000000009Avance: %3.0Avance: %3.010000000000005Avance: %3.019999999999996Avance: %3.030000000000001Avance: %3.039999999999992Avance: %3.049999999999997Avance: %3.0600000000000023Avance: %3.069999999999993Avance: %3.0799999999999983Avance: %3.0900000000000034Avance: %3.1000000000000085Avance: %3.1099999999999994Avance: %3.1200000000000045Avance: %3.1299999999999955Avance: %3.1400000000000006Avance: %3.1499999999999915Avance: %3.1599999999999966Avance: %3.1700000000000017Avance: %3.180000000000007Avance: %3.1899999999999977Avance: %3.200000000000003Avance: %3.210000000000008Avance: %3.219999999999999Avance: %3.230000000000004Avance: %3.239999999999995Avance: %3.25Avance: %3.259999999999991Avance: %3.269999999999996Avance: %3.280000000000001Avance: %3.2900000000000063Avance: %3.299999999999997Avance: %3.3100000000000023Avance: %3.3200000000000074Avance: %3.3299999999999983Avance: %3.3400000000000034Avance: %3.3499999999999943Avance: %3.3599999999999994Avance: %3.3699999999999903Avance: %3.3800000000000097Avance: %3.3900000000000006Avance: %3.4000000000000057Avance: %3.4099999999999966Avance: %3.4200000000000017Avance: %3.430000000000007Avance: %3.4399999999999977Avance: %3.450000000000003Avance: %3.4599999999999937Avance: %3.469999999999999Avance: %3.480000000000004Avance: %3.490000000000009Avance: %3.5Avance: %3.510000000000005Avance: %3.519999999999996Avance: %3.530000000000001Avance: %3.539999999999992Avance: %3.549999999999997Avance: %3.5600000000000023Avance: %3.569999999999993Avance: %3.5800000000000125Avance: %3.5900000000000034Avance: %3.6000000000000085Avance: %3.6099999999999994Avance: %3.6200000000000045Avance: %3.6299999999999955Avance: %3.6400000000000006Avance: %3.6499999999999915Avance: %3.6599999999999966Avance: %3.6700000000000017Avance: %3.680000000000007Avance: %3.6899999999999977Avance: %3.700000000000003Avance: %3.710000000000008Avance: %3.719999999999999Avance: %3.730000000000004Avance: %3.739999999999995Avance: %3.75Avance: %3.759999999999991Avance: %3.769999999999996Avance: %3.780000000000001Avance: %3.7900000000000063Avance: %3.799999999999997Avance: %3.8100000000000023Avance: %3.8200000000000074Avance: %3.8299999999999983Avance: %3.8400000000000034Avance: %3.8499999999999943Avance: %3.8599999999999994Avance: %3.8699999999999903Avance: %3.8799999999999955Avance: %3.8900000000000006Avance: %3.9000000000000057Avance: %3.9099999999999966Avance: %3.9200000000000017Avance: %3.930000000000007Avance: %3.9399999999999977Avance: %3.950000000000003Avance: %3.9599999999999937Avance: %3.969999999999999Avance: %3.9799999999999898Avance: %3.990000000000009Avance: %4.0Avance: %4.010000000000005Avance: %4.019999999999996Avance: %4.030000000000001Avance: %4.039999999999992Avance: %4.049999999999997Avance: %4.060000000000002Avance: %4.069999999999993Avance: %4.079999999999998Avance: %4.090000000000003Avance: %4.1000000000000085Avance: %4.109999999999999Avance: %4.1200000000000045Avance: %4.1299999999999955Avance: %4.140000000000001Avance: %4.1499999999999915Avance: %4.159999999999997Avance: %4.170000000000002Avance: %4.179999999999993Avance: %4.189999999999998Avance: %4.200000000000003Avance: %4.210000000000008Avance: %4.219999999999999Avance: %4.230000000000004Avance: %4.239999999999995Avance: %4.25Avance: %4.259999999999991Avance: %4.269999999999996Avance: %4.280000000000001Avance: %4.290000000000006Avance: %4.299999999999997Avance: %4.310000000000002Avance: %4.320000000000007Avance: %4.329999999999998Avance: %4.340000000000003Avance: %4.349999999999994Avance: %4.359999999999999Avance: %4.36999999999999Avance: %4.3799999999999955Avance: %4.390000000000001Avance: %4.400000000000006Avance: %4.409999999999997Avance: %4.420000000000002Avance: %4.430000000000007Avance: %4.439999999999998Avance: %4.450000000000003Avance: %4.459999999999994Avance: %4.469999999999999Avance: %4.47999999999999Avance: %4.490000000000009Avance: %4.5Avance: %4.510000000000005Avance: %4.519999999999996Avance: %4.530000000000001Avance: %4.540000000000006Avance: %4.549999999999997Avance: %4.560000000000002Avance: %4.569999999999993Avance: %4.579999999999998Avance: %4.590000000000003Avance: %4.6000000000000085Avance: %4.609999999999999Avance: %4.6200000000000045Avance: %4.6299999999999955Avance: %4.640000000000001Avance: %4.650000000000006Avance: %4.659999999999997Avance: %4.670000000000002Avance: %4.679999999999993Avance: %4.690000000000012Avance: %4.700000000000003Avance: %4.710000000000008Avance: %4.719999999999999Avance: %4.730000000000004Avance: %4.739999999999995Avance: %4.75Avance: %4.759999999999991Avance: %4.769999999999996Avance: %4.780000000000001Avance: %4.790000000000006Avance: %4.800000000000011Avance: %4.810000000000002Avance: %4.820000000000007Avance: %4.829999999999998Avance: %4.840000000000003Avance: %4.849999999999994Avance: %4.859999999999999Avance: %4.86999999999999Avance: %4.8799999999999955Avance: %4.890000000000001Avance: %4.900000000000006Avance: %4.909999999999997Avance: %4.920000000000002Avance: %4.930000000000007Avance: %4.939999999999998Avance: %4.950000000000003Avance: %4.959999999999994Avance: %4.969999999999999Avance: %4.97999999999999Avance: %4.990000000000009Avance: %5.0Avance: %5.010000000000005Avance: %5.019999999999996Avance: %5.030000000000001Avance: %5.040000000000006Avance: %5.049999999999997Avance: %5.060000000000002Avance: %5.069999999999993Avance: %5.079999999999998Avance: %5.089999999999989Avance: %5.1000000000000085Avance: %5.109999999999999Avance: %5.1200000000000045Avance: %5.1299999999999955Avance: %5.140000000000001Avance: %5.150000000000006Avance: %5.159999999999997Avance: %5.170000000000002Avance: %5.179999999999993Avance: %5.189999999999998Avance: %5.200000000000003Avance: %5.210000000000008Avance: %5.219999999999999Avance: %5.230000000000004Avance: %5.239999999999995Avance: %5.25Avance: %5.259999999999991Avance: %5.269999999999996Avance: %5.280000000000001Avance: %5.289999999999992Avance: %5.300000000000011Avance: %5.310000000000002Avance: %5.320000000000007Avance: %5.329999999999998Avance: %5.340000000000003Avance: %5.349999999999994Avance: %5.359999999999999Avance: %5.36999999999999Avance: %5.3799999999999955Avance: %5.390000000000001Avance: %5.400000000000006Avance: %5.409999999999997Avance: %5.420000000000002Avance: %5.430000000000007Avance: %5.439999999999998Avance: %5.450000000000003Avance: %5.459999999999994Avance: %5.469999999999999Avance: %5.47999999999999Avance: %5.489999999999995Avance: %5.5Avance: %5.510000000000005Avance: %5.519999999999996Avance: %5.530000000000001Avance: %5.540000000000006Avance: %5.549999999999997Avance: %5.560000000000002Avance: %5.569999999999993Avance: %5.579999999999998Avance: %5.589999999999989Avance: %5.6000000000000085Avance: %5.609999999999999Avance: %5.6200000000000045Avance: %5.6299999999999955Avance: %5.640000000000001Avance: %5.650000000000006Avance: %5.659999999999997Avance: %5.670000000000002Avance: %5.679999999999993Avance: %5.689999999999998Avance: %5.700000000000003Avance: %5.710000000000008Avance: %5.719999999999999Avance: %5.730000000000004Avance: %5.739999999999995Avance: %5.75Avance: %5.760000000000005Avance: %5.769999999999996Avance: %5.780000000000001Avance: %5.789999999999992Avance: %5.800000000000011Avance: %5.810000000000002Avance: %5.820000000000007Avance: %5.829999999999998Avance: %5.840000000000003Avance: %5.849999999999994Avance: %5.859999999999999Avance: %5.8700000000000045Avance: %5.8799999999999955Avance: %5.890000000000001Avance: %5.900000000000006Avance: %5.910000000000011Avance: %5.920000000000002Avance: %5.930000000000007Avance: %5.939999999999998Avance: %5.950000000000003Avance: %5.959999999999994Avance: %5.969999999999999Avance: %5.97999999999999Avance: %5.989999999999995Avance: %6.0Avance: %6.010000000000005Avance: %6.02000000000001Avance: %6.030000000000001Avance: %6.040000000000006Avance: %6.049999999999997Avance: %6.060000000000002Avance: %6.069999999999993Avance: %6.079999999999998Avance: %6.089999999999989Avance: %6.1000000000000085Avance: %6.109999999999999Avance: %6.1200000000000045Avance: %6.1299999999999955Avance: %6.140000000000001Avance: %6.150000000000006Avance: %6.159999999999997Avance: %6.170000000000002Avance: %6.179999999999993Avance: %6.189999999999998Avance: %6.200000000000003Avance: %6.210000000000008Avance: %6.219999999999999Avance: %6.230000000000004Avance: %6.239999999999995Avance: %6.25Avance: %6.260000000000005Avance: %6.269999999999996Avance: %6.280000000000001Avance: %6.289999999999992Avance: %6.299999999999997Avance: %6.310000000000002Avance: %6.320000000000007Avance: %6.329999999999998Avance: %6.340000000000003Avance: %6.349999999999994Avance: %6.359999999999999Avance: %6.3700000000000045Avance: %6.3799999999999955Avance: %6.390000000000001Avance: %6.3999999999999915Avance: %6.410000000000011Avance: %6.420000000000002Avance: %6.430000000000007Avance: %6.439999999999998Avance: %6.450000000000003Avance: %6.459999999999994Avance: %6.469999999999999Avance: %6.47999999999999Avance: %6.489999999999995Avance: %6.5Avance: %6.510000000000005Avance: %6.52000000000001Avance: %6.530000000000001Avance: %6.540000000000006Avance: %6.549999999999997Avance: %6.560000000000002Avance: %6.569999999999993Avance: %6.579999999999998Avance: %6.589999999999989Avance: %6.599999999999994Avance: %6.609999999999999Avance: %6.6200000000000045Avance: %6.6299999999999955Avance: %6.640000000000001Avance: %6.650000000000006Avance: %6.659999999999997Avance: %6.670000000000002Avance: %6.679999999999993Avance: %6.689999999999998Avance: %6.699999999999989Avance: %6.710000000000008Avance: %6.719999999999999Avance: %6.730000000000004Done Extracting Features 0:06:08.670832
The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.
Try some users for user 202038737
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 473) (2000, 473) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6811
        True       1.00      0.99      1.00       189

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1946
        True       1.00      0.93      0.96        54

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.96      0.98      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 35778170
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 363) (2000, 363) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6914
        True       1.00      1.00      1.00        86

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1975
        True       1.00      1.00      1.00        25

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 233644860
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 594) (2000, 594) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6586
        True       1.00      1.00      1.00       414

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1882
        True       1.00      0.97      0.99       118

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.99      0.99      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 204567208
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 407) (2000, 407) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': None, 'gamma': 10, 'kernel': 'poly'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6847
        True       1.00      0.99      0.99       153

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1957
        True       1.00      0.98      0.99        43

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.99      0.99      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 289426556
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 748) (2000, 748) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6942
        True       1.00      1.00      1.00        58

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1983
        True       1.00      1.00      1.00        17

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 142126563
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 663) (2000, 663) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6955
        True       1.00      1.00      1.00        45

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1987
        True       1.00      1.00      1.00        13

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 6792352
Loading df
Done loading df. DF shape is :(12397359, 8) (Original: (12397359, 8)) 		Time delta is: None mins
Loading graph from ../data/graphs/latest_subgraph.gpickle
Fetched 553 level 2 neighbourhs for user 6792352.
Len of own retweets (positive examples) is (4, 2)
done getting tweets universe. Shape is  (13496, 2)
	Dataset was truncated to 10000 tweets
Extracting features Optimized. X shape is : (10000, 553)
Avance: %0.010000000000005116Avance: %0.01999999999999602Avance: %0.030000000000001137Avance: %0.03999999999999204Avance: %0.04999999999999716Avance: %0.060000000000002274Avance: %0.07000000000000739Avance: %0.0799999999999983Avance: %0.09000000000000341Avance: %0.09999999999999432Avance: %0.10999999999999943Avance: %0.12000000000000455Avance: %0.12999999999999545Avance: %0.14000000000000057Avance: %0.14999999999999147Avance: %0.1600000000000108Avance: %0.1700000000000017Avance: %0.18000000000000682Avance: %0.18999999999999773Avance: %0.20000000000000284Avance: %0.20999999999999375Avance: %0.21999999999999886Avance: %0.22999999999998977Avance: %0.23999999999999488Avance: %0.25Avance: %0.2600000000000051Avance: %0.27000000000001023Avance: %0.28000000000000114Avance: %0.29000000000000625Avance: %0.29999999999999716Avance: %0.3100000000000023Avance: %0.3199999999999932Avance: %0.3299999999999983Avance: %0.3399999999999892Avance: %0.3499999999999943Avance: %0.35999999999999943Avance: %0.37000000000000455Avance: %0.37999999999999545Avance: %0.39000000000000057Avance: %0.4000000000000057Avance: %0.4099999999999966Avance: %0.4200000000000017Avance: %0.4299999999999926Avance: %0.4399999999999977Avance: %0.44999999999998863Avance: %0.46000000000000796Avance: %0.46999999999999886Avance: %0.480000000000004Avance: %0.4899999999999949Avance: %0.5Avance: %0.5100000000000051Avance: %0.519999999999996Avance: %0.5300000000000011Avance: %0.539999999999992Avance: %0.5499999999999972Avance: %0.5600000000000023Avance: %0.5700000000000074Avance: %0.5799999999999983Avance: %0.5900000000000034Avance: %0.5999999999999943Avance: %0.6099999999999994Avance: %0.6200000000000045Avance: %0.6299999999999955Avance: %0.6400000000000006Avance: %0.6499999999999915Avance: %0.6600000000000108Avance: %0.6700000000000017Avance: %0.6800000000000068Avance: %0.6899999999999977Avance: %0.7000000000000028Avance: %0.7099999999999937Avance: %0.7199999999999989Avance: %0.730000000000004Avance: %0.7399999999999949Avance: %0.75Avance: %0.7600000000000051Avance: %0.7700000000000102Avance: %0.7800000000000011Avance: %0.7900000000000063Avance: %0.7999999999999972Avance: %0.8100000000000023Avance: %0.8199999999999932Avance: %0.8299999999999983Avance: %0.8400000000000034Avance: %0.8499999999999943Avance: %0.8599999999999994Avance: %0.8700000000000045Avance: %0.8800000000000097Avance: %0.8900000000000006Avance: %0.9000000000000057Avance: %0.9099999999999966Avance: %0.9200000000000017Avance: %0.9299999999999926Avance: %0.9399999999999977Avance: %0.9499999999999886Avance: %0.960000000000008Avance: %0.9699999999999989Avance: %0.980000000000004Avance: %0.9900000000000091Avance: %1.0Avance: %1.0100000000000051Avance: %1.019999999999996Avance: %1.0300000000000011Avance: %1.039999999999992Avance: %1.0499999999999972Avance: %1.0600000000000023Avance: %1.0700000000000074Avance: %1.0799999999999983Avance: %1.0900000000000034Avance: %1.0999999999999943Avance: %1.1099999999999994Avance: %1.1200000000000045Avance: %1.1299999999999955Avance: %1.1400000000000006Avance: %1.1499999999999915Avance: %1.1600000000000108Avance: %1.1700000000000017Avance: %1.1800000000000068Avance: %1.1899999999999977Avance: %1.2000000000000028Avance: %1.2099999999999937Avance: %1.2199999999999989Avance: %1.230000000000004Avance: %1.2399999999999949Avance: %1.25Avance: %1.259999999999991Avance: %1.2700000000000102Avance: %1.2800000000000011Avance: %1.2900000000000063Avance: %1.2999999999999972Avance: %1.3100000000000023Avance: %1.3199999999999932Avance: %1.3299999999999983Avance: %1.3400000000000034Avance: %1.3499999999999943Avance: %1.3599999999999994Avance: %1.3700000000000045Avance: %1.3800000000000097Avance: %1.3900000000000006Avance: %1.4000000000000057Avance: %1.4099999999999966Avance: %1.4200000000000017Avance: %1.4299999999999926Avance: %1.4399999999999977Avance: %1.4499999999999886Avance: %1.4599999999999937Avance: %1.4699999999999989Avance: %1.480000000000004Avance: %1.490000000000009Avance: %1.5Avance: %1.5100000000000051Avance: %1.519999999999996Avance: %1.5300000000000011Avance: %1.539999999999992Avance: %1.5499999999999972Avance: %1.559999999999988Avance: %1.5700000000000074Avance: %1.5799999999999983Avance: %1.5900000000000034Avance: %1.5999999999999943Avance: %1.6099999999999994Avance: %1.6200000000000045Avance: %1.6299999999999955Avance: %1.6400000000000006Avance: %1.6499999999999915Avance: %1.6599999999999966Avance: %1.6700000000000017Avance: %1.6800000000000068Avance: %1.6899999999999977Avance: %1.7000000000000028Avance: %1.7099999999999937Avance: %1.7199999999999989Avance: %1.730000000000004Avance: %1.7399999999999949Avance: %1.75Avance: %1.759999999999991Avance: %1.7700000000000102Avance: %1.7800000000000011Avance: %1.7900000000000063Avance: %1.7999999999999972Avance: %1.8100000000000023Avance: %1.8199999999999932Avance: %1.8299999999999983Avance: %1.8400000000000034Avance: %1.8499999999999943Avance: %1.8599999999999994Avance: %1.8700000000000045Avance: %1.8800000000000097Avance: %1.8900000000000006Avance: %1.9000000000000057Avance: %1.9099999999999966Avance: %1.9200000000000017Avance: %1.9299999999999926Avance: %1.9399999999999977Avance: %1.9500000000000028Avance: %1.9599999999999937Avance: %1.9699999999999989Avance: %1.980000000000004Avance: %1.990000000000009Avance: %2.0Avance: %2.010000000000005Avance: %2.019999999999996Avance: %2.030000000000001Avance: %2.039999999999992Avance: %2.049999999999997Avance: %2.0600000000000023Avance: %2.0700000000000074Avance: %2.0799999999999983Avance: %2.0900000000000034Avance: %2.1000000000000085Avance: %2.1099999999999994Avance: %2.1200000000000045Avance: %2.1299999999999955Avance: %2.1400000000000006Avance: %2.1499999999999915Avance: %2.1599999999999966Avance: %2.1700000000000017Avance: %2.180000000000007Avance: %2.1899999999999977Avance: %2.200000000000003Avance: %2.210000000000008Avance: %2.219999999999999Avance: %2.230000000000004Avance: %2.239999999999995Avance: %2.25Avance: %2.259999999999991Avance: %2.2700000000000102Avance: %2.280000000000001Avance: %2.2900000000000063Avance: %2.299999999999997Avance: %2.3100000000000023Avance: %2.319999999999993Avance: %2.3299999999999983Avance: %2.3400000000000034Avance: %2.3499999999999943Avance: %2.3599999999999994Avance: %2.3700000000000045Avance: %2.3800000000000097Avance: %2.3900000000000006Avance: %2.4000000000000057Avance: %2.4099999999999966Avance: %2.4200000000000017Avance: %2.4299999999999926Avance: %2.4399999999999977Avance: %2.450000000000003Avance: %2.4599999999999937Avance: %2.469999999999999Avance: %2.480000000000004Avance: %2.490000000000009Avance: %2.5Avance: %2.510000000000005Avance: %2.519999999999996Avance: %2.530000000000001Avance: %2.539999999999992Avance: %2.549999999999997Avance: %2.5600000000000023Avance: %2.569999999999993Avance: %2.5799999999999983Avance: %2.5900000000000034Avance: %2.6000000000000085Avance: %2.6099999999999994Avance: %2.6200000000000045Avance: %2.6299999999999955Avance: %2.6400000000000006Avance: %2.6499999999999915Avance: %2.6599999999999966Avance: %2.6699999999999875Avance: %2.680000000000007Avance: %2.6899999999999977Avance: %2.700000000000003Avance: %2.710000000000008Avance: %2.719999999999999Avance: %2.730000000000004Avance: %2.739999999999995Avance: %2.75Avance: %2.759999999999991Avance: %2.769999999999996Avance: %2.780000000000001Avance: %2.7900000000000063Avance: %2.799999999999997Avance: %2.8100000000000023Avance: %2.819999999999993Avance: %2.8299999999999983Avance: %2.8400000000000034Avance: %2.8499999999999943Avance: %2.8599999999999994Avance: %2.8699999999999903Avance: %2.8800000000000097Avance: %2.8900000000000006Avance: %2.9000000000000057Avance: %2.9099999999999966Avance: %2.9200000000000017Avance: %2.9299999999999926Avance: %2.9399999999999977Avance: %2.950000000000003Avance: %2.9599999999999937Avance: %2.969999999999999Avance: %2.980000000000004Avance: %2.990000000000009Avance: %3.0Avance: %3.010000000000005Avance: %3.019999999999996Avance: %3.030000000000001Avance: %3.039999999999992Avance: %3.049999999999997Avance: %3.0600000000000023Avance: %3.069999999999993Avance: %3.0799999999999983Avance: %3.0900000000000034Avance: %3.1000000000000085Avance: %3.1099999999999994Avance: %3.1200000000000045Avance: %3.1299999999999955Avance: %3.1400000000000006Avance: %3.1499999999999915Avance: %3.1599999999999966Avance: %3.1700000000000017Avance: %3.180000000000007Avance: %3.1899999999999977Avance: %3.200000000000003Avance: %3.210000000000008Avance: %3.219999999999999Avance: %3.230000000000004Avance: %3.239999999999995Avance: %3.25Avance: %3.259999999999991Avance: %3.269999999999996Avance: %3.280000000000001Avance: %3.2900000000000063Avance: %3.299999999999997Avance: %3.3100000000000023Avance: %3.3200000000000074Avance: %3.3299999999999983Avance: %3.3400000000000034Avance: %3.3499999999999943Avance: %3.3599999999999994Avance: %3.3699999999999903Avance: %3.3800000000000097Avance: %3.3900000000000006Avance: %3.4000000000000057Avance: %3.4099999999999966Avance: %3.4200000000000017Avance: %3.430000000000007Avance: %3.4399999999999977Avance: %3.450000000000003Avance: %3.4599999999999937Avance: %3.469999999999999Avance: %3.480000000000004Avance: %3.490000000000009Avance: %3.5Avance: %3.510000000000005Avance: %3.519999999999996Avance: %3.530000000000001Avance: %3.539999999999992Avance: %3.549999999999997Avance: %3.5600000000000023Avance: %3.569999999999993Avance: %3.5800000000000125Avance: %3.5900000000000034Avance: %3.6000000000000085Avance: %3.6099999999999994Avance: %3.6200000000000045Avance: %3.6299999999999955Avance: %3.6400000000000006Avance: %3.6499999999999915Avance: %3.6599999999999966Avance: %3.6700000000000017Avance: %3.680000000000007Avance: %3.6899999999999977Avance: %3.700000000000003Avance: %3.710000000000008Avance: %3.719999999999999Avance: %3.730000000000004Avance: %3.739999999999995Avance: %3.75Avance: %3.759999999999991Avance: %3.769999999999996Avance: %3.780000000000001Avance: %3.7900000000000063Avance: %3.799999999999997Avance: %3.8100000000000023Avance: %3.8200000000000074Avance: %3.8299999999999983Avance: %3.8400000000000034Avance: %3.8499999999999943Avance: %3.8599999999999994Avance: %3.8699999999999903Avance: %3.8799999999999955Avance: %3.8900000000000006Avance: %3.9000000000000057Avance: %3.9099999999999966Avance: %3.9200000000000017Avance: %3.930000000000007Avance: %3.9399999999999977Avance: %3.950000000000003Avance: %3.9599999999999937Avance: %3.969999999999999Avance: %3.9799999999999898Avance: %3.990000000000009Avance: %4.0Avance: %4.010000000000005Avance: %4.019999999999996Avance: %4.030000000000001Avance: %4.039999999999992Avance: %4.049999999999997Avance: %4.060000000000002Avance: %4.069999999999993Avance: %4.079999999999998Avance: %4.090000000000003Avance: %4.1000000000000085Avance: %4.109999999999999Avance: %4.1200000000000045Avance: %4.1299999999999955Avance: %4.140000000000001Avance: %4.1499999999999915Avance: %4.159999999999997Avance: %4.170000000000002Avance: %4.179999999999993Avance: %4.189999999999998Avance: %4.200000000000003Avance: %4.210000000000008Avance: %4.219999999999999Avance: %4.230000000000004Avance: %4.239999999999995Avance: %4.25Avance: %4.259999999999991Avance: %4.269999999999996Avance: %4.280000000000001Avance: %4.290000000000006Avance: %4.299999999999997Avance: %4.310000000000002Avance: %4.320000000000007Avance: %4.329999999999998Avance: %4.340000000000003Avance: %4.349999999999994Avance: %4.359999999999999Avance: %4.36999999999999Avance: %4.3799999999999955Avance: %4.390000000000001Avance: %4.400000000000006Avance: %4.409999999999997Avance: %4.420000000000002Avance: %4.430000000000007Avance: %4.439999999999998Avance: %4.450000000000003Avance: %4.459999999999994Avance: %4.469999999999999Avance: %4.47999999999999Avance: %4.490000000000009Avance: %4.5Avance: %4.510000000000005Avance: %4.519999999999996Avance: %4.530000000000001Avance: %4.540000000000006Avance: %4.549999999999997Avance: %4.560000000000002Avance: %4.569999999999993Avance: %4.579999999999998Avance: %4.590000000000003Avance: %4.6000000000000085Avance: %4.609999999999999Avance: %4.6200000000000045Avance: %4.6299999999999955Avance: %4.640000000000001Avance: %4.650000000000006Avance: %4.659999999999997Avance: %4.670000000000002Avance: %4.679999999999993Avance: %4.690000000000012Avance: %4.700000000000003Avance: %4.710000000000008Avance: %4.719999999999999Avance: %4.730000000000004Avance: %4.739999999999995Avance: %4.75Avance: %4.759999999999991Avance: %4.769999999999996Avance: %4.780000000000001Avance: %4.790000000000006Avance: %4.800000000000011Avance: %4.810000000000002Avance: %4.820000000000007Avance: %4.829999999999998Avance: %4.840000000000003Avance: %4.849999999999994Avance: %4.859999999999999Avance: %4.86999999999999Avance: %4.8799999999999955Avance: %4.890000000000001Avance: %4.900000000000006Avance: %4.909999999999997Avance: %4.920000000000002Avance: %4.930000000000007Avance: %4.939999999999998Avance: %4.950000000000003Avance: %4.959999999999994Avance: %4.969999999999999Avance: %4.97999999999999Avance: %4.990000000000009Avance: %5.0Avance: %5.010000000000005Avance: %5.019999999999996Avance: %5.030000000000001Avance: %5.040000000000006Avance: %5.049999999999997Avance: %5.060000000000002Avance: %5.069999999999993Avance: %5.079999999999998Avance: %5.089999999999989Avance: %5.1000000000000085Avance: %5.109999999999999Avance: %5.1200000000000045Avance: %5.1299999999999955Avance: %5.140000000000001Avance: %5.150000000000006Avance: %5.159999999999997Avance: %5.170000000000002Avance: %5.179999999999993Avance: %5.189999999999998Avance: %5.200000000000003Avance: %5.210000000000008Avance: %5.219999999999999Avance: %5.230000000000004Avance: %5.239999999999995Avance: %5.25Avance: %5.259999999999991Avance: %5.269999999999996Avance: %5.280000000000001Avance: %5.289999999999992Avance: %5.300000000000011Avance: %5.310000000000002Avance: %5.320000000000007Avance: %5.329999999999998Avance: %5.340000000000003Avance: %5.349999999999994Avance: %5.359999999999999Avance: %5.36999999999999Avance: %5.3799999999999955Avance: %5.390000000000001Avance: %5.400000000000006Avance: %5.409999999999997Avance: %5.420000000000002Avance: %5.430000000000007Avance: %5.439999999999998Avance: %5.450000000000003Avance: %5.459999999999994Avance: %5.469999999999999Avance: %5.47999999999999Avance: %5.489999999999995Avance: %5.5Avance: %5.510000000000005Avance: %5.519999999999996Avance: %5.530000000000001Done Extracting Features 0:05:10.892731
The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.
Try some users for user 92993433
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 655) (2000, 655) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6953
        True       1.00      1.00      1.00        47

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1987
        True       1.00      1.00      1.00        13

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 74250661
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 648) (2000, 648) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6937
        True       1.00      1.00      1.00        63

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1982
        True       1.00      1.00      1.00        18

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 89522960
Loading df
Done loading df. DF shape is :(12397359, 8) (Original: (12397359, 8)) 		Time delta is: None mins
Loading graph from ../data/graphs/latest_subgraph.gpickle
Fetched 356 level 2 neighbourhs for user 89522960.
Len of own retweets (positive examples) is (5, 2)
done getting tweets universe. Shape is  (10414, 2)
	Dataset was truncated to 10000 tweets
Extracting features Optimized. X shape is : (10000, 356)
Avance: %0.010000000000005116Avance: %0.01999999999999602Avance: %0.030000000000001137Avance: %0.03999999999999204Avance: %0.04999999999999716Avance: %0.060000000000002274Avance: %0.07000000000000739Avance: %0.0799999999999983Avance: %0.09000000000000341Avance: %0.09999999999999432Avance: %0.10999999999999943Avance: %0.12000000000000455Avance: %0.12999999999999545Avance: %0.14000000000000057Avance: %0.14999999999999147Avance: %0.1600000000000108Avance: %0.1700000000000017Avance: %0.18000000000000682Avance: %0.18999999999999773Avance: %0.20000000000000284Avance: %0.20999999999999375Avance: %0.21999999999999886Avance: %0.22999999999998977Avance: %0.23999999999999488Avance: %0.25Avance: %0.2600000000000051Avance: %0.27000000000001023Avance: %0.28000000000000114Avance: %0.29000000000000625Avance: %0.29999999999999716Avance: %0.3100000000000023Avance: %0.3199999999999932Avance: %0.3299999999999983Avance: %0.3399999999999892Avance: %0.3499999999999943Avance: %0.35999999999999943Avance: %0.37000000000000455Avance: %0.37999999999999545Avance: %0.39000000000000057Avance: %0.4000000000000057Avance: %0.4099999999999966Avance: %0.4200000000000017Avance: %0.4299999999999926Avance: %0.4399999999999977Avance: %0.44999999999998863Avance: %0.46000000000000796Avance: %0.46999999999999886Avance: %0.480000000000004Avance: %0.4899999999999949Avance: %0.5Avance: %0.5100000000000051Avance: %0.519999999999996Avance: %0.5300000000000011Avance: %0.539999999999992Avance: %0.5499999999999972Avance: %0.5600000000000023Avance: %0.5700000000000074Avance: %0.5799999999999983Avance: %0.5900000000000034Avance: %0.5999999999999943Avance: %0.6099999999999994Avance: %0.6200000000000045Avance: %0.6299999999999955Avance: %0.6400000000000006Avance: %0.6499999999999915Avance: %0.6600000000000108Avance: %0.6700000000000017Avance: %0.6800000000000068Avance: %0.6899999999999977Avance: %0.7000000000000028Avance: %0.7099999999999937Avance: %0.7199999999999989Avance: %0.730000000000004Avance: %0.7399999999999949Avance: %0.75Avance: %0.7600000000000051Avance: %0.7700000000000102Avance: %0.7800000000000011Avance: %0.7900000000000063Avance: %0.7999999999999972Avance: %0.8100000000000023Avance: %0.8199999999999932Avance: %0.8299999999999983Avance: %0.8400000000000034Avance: %0.8499999999999943Avance: %0.8599999999999994Avance: %0.8700000000000045Avance: %0.8800000000000097Avance: %0.8900000000000006Avance: %0.9000000000000057Avance: %0.9099999999999966Avance: %0.9200000000000017Avance: %0.9299999999999926Avance: %0.9399999999999977Avance: %0.9499999999999886Avance: %0.960000000000008Avance: %0.9699999999999989Avance: %0.980000000000004Avance: %0.9900000000000091Avance: %1.0Avance: %1.0100000000000051Avance: %1.019999999999996Avance: %1.0300000000000011Avance: %1.039999999999992Avance: %1.0499999999999972Avance: %1.0600000000000023Avance: %1.0700000000000074Avance: %1.0799999999999983Avance: %1.0900000000000034Avance: %1.0999999999999943Avance: %1.1099999999999994Avance: %1.1200000000000045Avance: %1.1299999999999955Avance: %1.1400000000000006Avance: %1.1499999999999915Avance: %1.1600000000000108Avance: %1.1700000000000017Avance: %1.1800000000000068Avance: %1.1899999999999977Avance: %1.2000000000000028Avance: %1.2099999999999937Avance: %1.2199999999999989Avance: %1.230000000000004Avance: %1.2399999999999949Avance: %1.25Avance: %1.259999999999991Avance: %1.2700000000000102Avance: %1.2800000000000011Avance: %1.2900000000000063Avance: %1.2999999999999972Avance: %1.3100000000000023Avance: %1.3199999999999932Avance: %1.3299999999999983Avance: %1.3400000000000034Avance: %1.3499999999999943Avance: %1.3599999999999994Avance: %1.3700000000000045Avance: %1.3800000000000097Avance: %1.3900000000000006Avance: %1.4000000000000057Avance: %1.4099999999999966Avance: %1.4200000000000017Avance: %1.4299999999999926Avance: %1.4399999999999977Avance: %1.4499999999999886Avance: %1.4599999999999937Avance: %1.4699999999999989Avance: %1.480000000000004Avance: %1.490000000000009Avance: %1.5Avance: %1.5100000000000051Avance: %1.519999999999996Avance: %1.5300000000000011Avance: %1.539999999999992Avance: %1.5499999999999972Avance: %1.559999999999988Avance: %1.5700000000000074Avance: %1.5799999999999983Avance: %1.5900000000000034Avance: %1.5999999999999943Avance: %1.6099999999999994Avance: %1.6200000000000045Avance: %1.6299999999999955Avance: %1.6400000000000006Avance: %1.6499999999999915Avance: %1.6599999999999966Avance: %1.6700000000000017Avance: %1.6800000000000068Avance: %1.6899999999999977Avance: %1.7000000000000028Avance: %1.7099999999999937Avance: %1.7199999999999989Avance: %1.730000000000004Avance: %1.7399999999999949Avance: %1.75Avance: %1.759999999999991Avance: %1.7700000000000102Avance: %1.7800000000000011Avance: %1.7900000000000063Avance: %1.7999999999999972Avance: %1.8100000000000023Avance: %1.8199999999999932Avance: %1.8299999999999983Avance: %1.8400000000000034Avance: %1.8499999999999943Avance: %1.8599999999999994Avance: %1.8700000000000045Avance: %1.8800000000000097Avance: %1.8900000000000006Avance: %1.9000000000000057Avance: %1.9099999999999966Avance: %1.9200000000000017Avance: %1.9299999999999926Avance: %1.9399999999999977Avance: %1.9500000000000028Avance: %1.9599999999999937Avance: %1.9699999999999989Avance: %1.980000000000004Avance: %1.990000000000009Avance: %2.0Avance: %2.010000000000005Avance: %2.019999999999996Avance: %2.030000000000001Avance: %2.039999999999992Avance: %2.049999999999997Avance: %2.0600000000000023Avance: %2.0700000000000074Avance: %2.0799999999999983Avance: %2.0900000000000034Avance: %2.1000000000000085Avance: %2.1099999999999994Avance: %2.1200000000000045Avance: %2.1299999999999955Avance: %2.1400000000000006Avance: %2.1499999999999915Avance: %2.1599999999999966Avance: %2.1700000000000017Avance: %2.180000000000007Avance: %2.1899999999999977Avance: %2.200000000000003Avance: %2.210000000000008Avance: %2.219999999999999Avance: %2.230000000000004Avance: %2.239999999999995Avance: %2.25Avance: %2.259999999999991Avance: %2.2700000000000102Avance: %2.280000000000001Avance: %2.2900000000000063Avance: %2.299999999999997Avance: %2.3100000000000023Avance: %2.319999999999993Avance: %2.3299999999999983Avance: %2.3400000000000034Avance: %2.3499999999999943Avance: %2.3599999999999994Avance: %2.3700000000000045Avance: %2.3800000000000097Avance: %2.3900000000000006Avance: %2.4000000000000057Avance: %2.4099999999999966Avance: %2.4200000000000017Avance: %2.4299999999999926Avance: %2.4399999999999977Avance: %2.450000000000003Avance: %2.4599999999999937Avance: %2.469999999999999Avance: %2.480000000000004Avance: %2.490000000000009Avance: %2.5Avance: %2.510000000000005Avance: %2.519999999999996Avance: %2.530000000000001Avance: %2.539999999999992Avance: %2.549999999999997Avance: %2.5600000000000023Avance: %2.569999999999993Avance: %2.5799999999999983Avance: %2.5900000000000034Avance: %2.6000000000000085Avance: %2.6099999999999994Avance: %2.6200000000000045Avance: %2.6299999999999955Avance: %2.6400000000000006Avance: %2.6499999999999915Avance: %2.6599999999999966Avance: %2.6699999999999875Avance: %2.680000000000007Avance: %2.6899999999999977Avance: %2.700000000000003Avance: %2.710000000000008Avance: %2.719999999999999Avance: %2.730000000000004Avance: %2.739999999999995Avance: %2.75Avance: %2.759999999999991Avance: %2.769999999999996Avance: %2.780000000000001Avance: %2.7900000000000063Avance: %2.799999999999997Avance: %2.8100000000000023Avance: %2.819999999999993Avance: %2.8299999999999983Avance: %2.8400000000000034Avance: %2.8499999999999943Avance: %2.8599999999999994Avance: %2.8699999999999903Avance: %2.8800000000000097Avance: %2.8900000000000006Avance: %2.9000000000000057Avance: %2.9099999999999966Avance: %2.9200000000000017Avance: %2.9299999999999926Avance: %2.9399999999999977Avance: %2.950000000000003Avance: %2.9599999999999937Avance: %2.969999999999999Avance: %2.980000000000004Avance: %2.990000000000009Avance: %3.0Avance: %3.010000000000005Avance: %3.019999999999996Avance: %3.030000000000001Avance: %3.039999999999992Avance: %3.049999999999997Avance: %3.0600000000000023Avance: %3.069999999999993Avance: %3.0799999999999983Avance: %3.0900000000000034Avance: %3.1000000000000085Avance: %3.1099999999999994Avance: %3.1200000000000045Avance: %3.1299999999999955Avance: %3.1400000000000006Avance: %3.1499999999999915Avance: %3.1599999999999966Avance: %3.1700000000000017Avance: %3.180000000000007Avance: %3.1899999999999977Avance: %3.200000000000003Avance: %3.210000000000008Avance: %3.219999999999999Avance: %3.230000000000004Avance: %3.239999999999995Avance: %3.25Avance: %3.259999999999991Avance: %3.269999999999996Avance: %3.280000000000001Avance: %3.2900000000000063Avance: %3.299999999999997Avance: %3.3100000000000023Avance: %3.3200000000000074Avance: %3.3299999999999983Avance: %3.3400000000000034Avance: %3.3499999999999943Avance: %3.3599999999999994Avance: %3.3699999999999903Avance: %3.3800000000000097Avance: %3.3900000000000006Avance: %3.4000000000000057Avance: %3.4099999999999966Avance: %3.4200000000000017Avance: %3.430000000000007Avance: %3.4399999999999977Avance: %3.450000000000003Avance: %3.4599999999999937Avance: %3.469999999999999Avance: %3.480000000000004Avance: %3.490000000000009Avance: %3.5Avance: %3.510000000000005Avance: %3.519999999999996Avance: %3.530000000000001Avance: %3.539999999999992Avance: %3.549999999999997Avance: %3.5600000000000023Done Extracting Features 0:03:13.843629
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 356) (2000, 356) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6996
        True       1.00      1.00      1.00         4

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1999
        True       1.00      1.00      1.00         1

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 111645984
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 519) (2000, 519) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6908
        True       1.00      1.00      1.00        92

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1973
        True       1.00      1.00      1.00        27

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 72207487
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 746) (2000, 746) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6980
        True       1.00      1.00      1.00        20

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1994
        True       1.00      1.00      1.00         6

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 142192618
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 514) (2000, 514) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6539
        True       1.00      1.00      1.00       461

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1869
        True       1.00      0.99      1.00       131

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 44043305
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 585) (2000, 585) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6968
        True       1.00      1.00      1.00        32

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1991
        True       1.00      1.00      1.00         9

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 28785486
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 243) (2000, 243) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6995
        True       1.00      1.00      1.00         5

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1999
        True       1.00      1.00      1.00         1

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 16002828
Loading df
Done loading df. DF shape is :(12397359, 8) (Original: (12397359, 8)) 		Time delta is: None mins
Loading graph from ../data/graphs/latest_subgraph.gpickle
Fetched 857 level 2 neighbourhs for user 16002828.
Len of own retweets (positive examples) is (4, 2)
done getting tweets universe. Shape is  (11254, 2)
	Dataset was truncated to 10000 tweets
Extracting features Optimized. X shape is : (10000, 857)
Avance: %0.010000000000005116Avance: %0.01999999999999602Avance: %0.030000000000001137Avance: %0.03999999999999204Avance: %0.04999999999999716Avance: %0.060000000000002274Avance: %0.07000000000000739Avance: %0.0799999999999983Avance: %0.09000000000000341Avance: %0.09999999999999432Avance: %0.10999999999999943Avance: %0.12000000000000455Avance: %0.12999999999999545Avance: %0.14000000000000057Avance: %0.14999999999999147Avance: %0.1600000000000108Avance: %0.1700000000000017Avance: %0.18000000000000682Avance: %0.18999999999999773Avance: %0.20000000000000284Avance: %0.20999999999999375Avance: %0.21999999999999886Avance: %0.22999999999998977Avance: %0.23999999999999488Avance: %0.25Avance: %0.2600000000000051Avance: %0.27000000000001023Avance: %0.28000000000000114Avance: %0.29000000000000625Avance: %0.29999999999999716Avance: %0.3100000000000023Avance: %0.3199999999999932Avance: %0.3299999999999983Avance: %0.3399999999999892Avance: %0.3499999999999943Avance: %0.35999999999999943Avance: %0.37000000000000455Avance: %0.37999999999999545Avance: %0.39000000000000057Avance: %0.4000000000000057Avance: %0.4099999999999966Avance: %0.4200000000000017Avance: %0.4299999999999926Avance: %0.4399999999999977Avance: %0.44999999999998863Avance: %0.46000000000000796Avance: %0.46999999999999886Avance: %0.480000000000004Avance: %0.4899999999999949Avance: %0.5Avance: %0.5100000000000051Avance: %0.519999999999996Avance: %0.5300000000000011Avance: %0.539999999999992Avance: %0.5499999999999972Avance: %0.5600000000000023Avance: %0.5700000000000074Avance: %0.5799999999999983Avance: %0.5900000000000034Avance: %0.5999999999999943Avance: %0.6099999999999994Avance: %0.6200000000000045Avance: %0.6299999999999955Avance: %0.6400000000000006Avance: %0.6499999999999915Avance: %0.6600000000000108Avance: %0.6700000000000017Avance: %0.6800000000000068Avance: %0.6899999999999977Avance: %0.7000000000000028Avance: %0.7099999999999937Avance: %0.7199999999999989Avance: %0.730000000000004Avance: %0.7399999999999949Avance: %0.75Avance: %0.7600000000000051Avance: %0.7700000000000102Avance: %0.7800000000000011Avance: %0.7900000000000063Avance: %0.7999999999999972Avance: %0.8100000000000023Avance: %0.8199999999999932Avance: %0.8299999999999983Avance: %0.8400000000000034Avance: %0.8499999999999943Avance: %0.8599999999999994Avance: %0.8700000000000045Avance: %0.8800000000000097Avance: %0.8900000000000006Avance: %0.9000000000000057Avance: %0.9099999999999966Avance: %0.9200000000000017Avance: %0.9299999999999926Avance: %0.9399999999999977Avance: %0.9499999999999886Avance: %0.960000000000008Avance: %0.9699999999999989Avance: %0.980000000000004Avance: %0.9900000000000091Avance: %1.0Avance: %1.0100000000000051Avance: %1.019999999999996Avance: %1.0300000000000011Avance: %1.039999999999992Avance: %1.0499999999999972Avance: %1.0600000000000023Avance: %1.0700000000000074Avance: %1.0799999999999983Avance: %1.0900000000000034Avance: %1.0999999999999943Avance: %1.1099999999999994Avance: %1.1200000000000045Avance: %1.1299999999999955Avance: %1.1400000000000006Avance: %1.1499999999999915Avance: %1.1600000000000108Avance: %1.1700000000000017Avance: %1.1800000000000068Avance: %1.1899999999999977Avance: %1.2000000000000028Avance: %1.2099999999999937Avance: %1.2199999999999989Avance: %1.230000000000004Avance: %1.2399999999999949Avance: %1.25Avance: %1.259999999999991Avance: %1.2700000000000102Avance: %1.2800000000000011Avance: %1.2900000000000063Avance: %1.2999999999999972Avance: %1.3100000000000023Avance: %1.3199999999999932Avance: %1.3299999999999983Avance: %1.3400000000000034Avance: %1.3499999999999943Avance: %1.3599999999999994Avance: %1.3700000000000045Avance: %1.3800000000000097Avance: %1.3900000000000006Avance: %1.4000000000000057Avance: %1.4099999999999966Avance: %1.4200000000000017Avance: %1.4299999999999926Avance: %1.4399999999999977Avance: %1.4499999999999886Avance: %1.4599999999999937Avance: %1.4699999999999989Avance: %1.480000000000004Avance: %1.490000000000009Avance: %1.5Avance: %1.5100000000000051Avance: %1.519999999999996Avance: %1.5300000000000011Avance: %1.539999999999992Avance: %1.5499999999999972Avance: %1.559999999999988Avance: %1.5700000000000074Avance: %1.5799999999999983Avance: %1.5900000000000034Avance: %1.5999999999999943Avance: %1.6099999999999994Avance: %1.6200000000000045Avance: %1.6299999999999955Avance: %1.6400000000000006Avance: %1.6499999999999915Avance: %1.6599999999999966Avance: %1.6700000000000017Avance: %1.6800000000000068Avance: %1.6899999999999977Avance: %1.7000000000000028Avance: %1.7099999999999937Avance: %1.7199999999999989Avance: %1.730000000000004Avance: %1.7399999999999949Avance: %1.75Avance: %1.759999999999991Avance: %1.7700000000000102Avance: %1.7800000000000011Avance: %1.7900000000000063Avance: %1.7999999999999972Avance: %1.8100000000000023Avance: %1.8199999999999932Avance: %1.8299999999999983Avance: %1.8400000000000034Avance: %1.8499999999999943Avance: %1.8599999999999994Avance: %1.8700000000000045Avance: %1.8800000000000097Avance: %1.8900000000000006Avance: %1.9000000000000057Avance: %1.9099999999999966Avance: %1.9200000000000017Avance: %1.9299999999999926Avance: %1.9399999999999977Avance: %1.9500000000000028Avance: %1.9599999999999937Avance: %1.9699999999999989Avance: %1.980000000000004Avance: %1.990000000000009Avance: %2.0Avance: %2.010000000000005Avance: %2.019999999999996Avance: %2.030000000000001Avance: %2.039999999999992Avance: %2.049999999999997Avance: %2.0600000000000023Avance: %2.0700000000000074Avance: %2.0799999999999983Avance: %2.0900000000000034Avance: %2.1000000000000085Avance: %2.1099999999999994Avance: %2.1200000000000045Avance: %2.1299999999999955Avance: %2.1400000000000006Avance: %2.1499999999999915Avance: %2.1599999999999966Avance: %2.1700000000000017Avance: %2.180000000000007Avance: %2.1899999999999977Avance: %2.200000000000003Avance: %2.210000000000008Avance: %2.219999999999999Avance: %2.230000000000004Avance: %2.239999999999995Avance: %2.25Avance: %2.259999999999991Avance: %2.2700000000000102Avance: %2.280000000000001Avance: %2.2900000000000063Avance: %2.299999999999997Avance: %2.3100000000000023Avance: %2.319999999999993Avance: %2.3299999999999983Avance: %2.3400000000000034Avance: %2.3499999999999943Avance: %2.3599999999999994Avance: %2.3700000000000045Avance: %2.3800000000000097Avance: %2.3900000000000006Avance: %2.4000000000000057Avance: %2.4099999999999966Avance: %2.4200000000000017Avance: %2.4299999999999926Avance: %2.4399999999999977Avance: %2.450000000000003Avance: %2.4599999999999937Avance: %2.469999999999999Avance: %2.480000000000004Avance: %2.490000000000009Avance: %2.5Avance: %2.510000000000005Avance: %2.519999999999996Avance: %2.530000000000001Avance: %2.539999999999992Avance: %2.549999999999997Avance: %2.5600000000000023Avance: %2.569999999999993Avance: %2.5799999999999983Avance: %2.5900000000000034Avance: %2.6000000000000085Avance: %2.6099999999999994Avance: %2.6200000000000045Avance: %2.6299999999999955Avance: %2.6400000000000006Avance: %2.6499999999999915Avance: %2.6599999999999966Avance: %2.6699999999999875Avance: %2.680000000000007Avance: %2.6899999999999977Avance: %2.700000000000003Avance: %2.710000000000008Avance: %2.719999999999999Avance: %2.730000000000004Avance: %2.739999999999995Avance: %2.75Avance: %2.759999999999991Avance: %2.769999999999996Avance: %2.780000000000001Avance: %2.7900000000000063Avance: %2.799999999999997Avance: %2.8100000000000023Avance: %2.819999999999993Avance: %2.8299999999999983Avance: %2.8400000000000034Avance: %2.8499999999999943Avance: %2.8599999999999994Avance: %2.8699999999999903Avance: %2.8800000000000097Avance: %2.8900000000000006Avance: %2.9000000000000057Avance: %2.9099999999999966Avance: %2.9200000000000017Avance: %2.9299999999999926Avance: %2.9399999999999977Avance: %2.950000000000003Avance: %2.9599999999999937Avance: %2.969999999999999Avance: %2.980000000000004Avance: %2.990000000000009Avance: %3.0Avance: %3.010000000000005Avance: %3.019999999999996Avance: %3.030000000000001Avance: %3.039999999999992Avance: %3.049999999999997Avance: %3.0600000000000023Avance: %3.069999999999993Avance: %3.0799999999999983Avance: %3.0900000000000034Avance: %3.1000000000000085Avance: %3.1099999999999994Avance: %3.1200000000000045Avance: %3.1299999999999955Avance: %3.1400000000000006Avance: %3.1499999999999915Avance: %3.1599999999999966Avance: %3.1700000000000017Avance: %3.180000000000007Avance: %3.1899999999999977Avance: %3.200000000000003Avance: %3.210000000000008Avance: %3.219999999999999Avance: %3.230000000000004Avance: %3.239999999999995Avance: %3.25Avance: %3.259999999999991Avance: %3.269999999999996Avance: %3.280000000000001Avance: %3.2900000000000063Avance: %3.299999999999997Avance: %3.3100000000000023Avance: %3.3200000000000074Avance: %3.3299999999999983Avance: %3.3400000000000034Avance: %3.3499999999999943Avance: %3.3599999999999994Avance: %3.3699999999999903Avance: %3.3800000000000097Avance: %3.3900000000000006Avance: %3.4000000000000057Avance: %3.4099999999999966Avance: %3.4200000000000017Avance: %3.430000000000007Avance: %3.4399999999999977Avance: %3.450000000000003Avance: %3.4599999999999937Avance: %3.469999999999999Avance: %3.480000000000004Avance: %3.490000000000009Avance: %3.5Avance: %3.510000000000005Avance: %3.519999999999996Avance: %3.530000000000001Avance: %3.539999999999992Avance: %3.549999999999997Avance: %3.5600000000000023Avance: %3.569999999999993Avance: %3.5800000000000125Avance: %3.5900000000000034Avance: %3.6000000000000085Avance: %3.6099999999999994Avance: %3.6200000000000045Avance: %3.6299999999999955Avance: %3.6400000000000006Avance: %3.6499999999999915Avance: %3.6599999999999966Avance: %3.6700000000000017Avance: %3.680000000000007Avance: %3.6899999999999977Avance: %3.700000000000003Avance: %3.710000000000008Avance: %3.719999999999999Avance: %3.730000000000004Avance: %3.739999999999995Avance: %3.75Avance: %3.759999999999991Avance: %3.769999999999996Avance: %3.780000000000001Avance: %3.7900000000000063Avance: %3.799999999999997Avance: %3.8100000000000023Avance: %3.8200000000000074Avance: %3.8299999999999983Avance: %3.8400000000000034Avance: %3.8499999999999943Avance: %3.8599999999999994Avance: %3.8699999999999903Avance: %3.8799999999999955Avance: %3.8900000000000006Avance: %3.9000000000000057Avance: %3.9099999999999966Avance: %3.9200000000000017Avance: %3.930000000000007Avance: %3.9399999999999977Avance: %3.950000000000003Avance: %3.9599999999999937Avance: %3.969999999999999Avance: %3.9799999999999898Avance: %3.990000000000009Avance: %4.0Avance: %4.010000000000005Avance: %4.019999999999996Avance: %4.030000000000001Avance: %4.039999999999992Avance: %4.049999999999997Avance: %4.060000000000002Avance: %4.069999999999993Avance: %4.079999999999998Avance: %4.090000000000003Avance: %4.1000000000000085Avance: %4.109999999999999Avance: %4.1200000000000045Avance: %4.1299999999999955Avance: %4.140000000000001Avance: %4.1499999999999915Avance: %4.159999999999997Avance: %4.170000000000002Avance: %4.179999999999993Avance: %4.189999999999998Avance: %4.200000000000003Avance: %4.210000000000008Avance: %4.219999999999999Avance: %4.230000000000004Avance: %4.239999999999995Avance: %4.25Avance: %4.259999999999991Avance: %4.269999999999996Avance: %4.280000000000001Avance: %4.290000000000006Avance: %4.299999999999997Avance: %4.310000000000002Avance: %4.320000000000007Avance: %4.329999999999998Avance: %4.340000000000003Avance: %4.349999999999994Avance: %4.359999999999999Avance: %4.36999999999999Avance: %4.3799999999999955Avance: %4.390000000000001Avance: %4.400000000000006Avance: %4.409999999999997Avance: %4.420000000000002Avance: %4.430000000000007Avance: %4.439999999999998Avance: %4.450000000000003Avance: %4.459999999999994Avance: %4.469999999999999Avance: %4.47999999999999Avance: %4.490000000000009Avance: %4.5Avance: %4.510000000000005Avance: %4.519999999999996Avance: %4.530000000000001Avance: %4.540000000000006Avance: %4.549999999999997Avance: %4.560000000000002Avance: %4.569999999999993Avance: %4.579999999999998Avance: %4.590000000000003Avance: %4.6000000000000085Avance: %4.609999999999999Avance: %4.6200000000000045Avance: %4.6299999999999955Avance: %4.640000000000001Avance: %4.650000000000006Avance: %4.659999999999997Avance: %4.670000000000002Avance: %4.679999999999993Avance: %4.690000000000012Avance: %4.700000000000003Avance: %4.710000000000008Avance: %4.719999999999999Avance: %4.730000000000004Avance: %4.739999999999995Avance: %4.75Avance: %4.759999999999991Avance: %4.769999999999996Avance: %4.780000000000001Avance: %4.790000000000006Avance: %4.800000000000011Avance: %4.810000000000002Avance: %4.820000000000007Avance: %4.829999999999998Avance: %4.840000000000003Avance: %4.849999999999994Avance: %4.859999999999999Avance: %4.86999999999999Avance: %4.8799999999999955Avance: %4.890000000000001Avance: %4.900000000000006Avance: %4.909999999999997Avance: %4.920000000000002Avance: %4.930000000000007Avance: %4.939999999999998Avance: %4.950000000000003Avance: %4.959999999999994Avance: %4.969999999999999Avance: %4.97999999999999Avance: %4.990000000000009Avance: %5.0Avance: %5.010000000000005Avance: %5.019999999999996Avance: %5.030000000000001Avance: %5.040000000000006Avance: %5.049999999999997Avance: %5.060000000000002Avance: %5.069999999999993Avance: %5.079999999999998Avance: %5.089999999999989Avance: %5.1000000000000085Avance: %5.109999999999999Avance: %5.1200000000000045Avance: %5.1299999999999955Avance: %5.140000000000001Avance: %5.150000000000006Avance: %5.159999999999997Avance: %5.170000000000002Avance: %5.179999999999993Avance: %5.189999999999998Avance: %5.200000000000003Avance: %5.210000000000008Avance: %5.219999999999999Avance: %5.230000000000004Avance: %5.239999999999995Avance: %5.25Avance: %5.259999999999991Avance: %5.269999999999996Avance: %5.280000000000001Avance: %5.289999999999992Avance: %5.300000000000011Avance: %5.310000000000002Avance: %5.320000000000007Avance: %5.329999999999998Avance: %5.340000000000003Avance: %5.349999999999994Avance: %5.359999999999999Avance: %5.36999999999999Avance: %5.3799999999999955Avance: %5.390000000000001Avance: %5.400000000000006Avance: %5.409999999999997Avance: %5.420000000000002Avance: %5.430000000000007Avance: %5.439999999999998Avance: %5.450000000000003Avance: %5.459999999999994Avance: %5.469999999999999Avance: %5.47999999999999Avance: %5.489999999999995Avance: %5.5Avance: %5.510000000000005Avance: %5.519999999999996Avance: %5.530000000000001Avance: %5.540000000000006Avance: %5.549999999999997Avance: %5.560000000000002Avance: %5.569999999999993Avance: %5.579999999999998Avance: %5.589999999999989Avance: %5.6000000000000085Avance: %5.609999999999999Avance: %5.6200000000000045Avance: %5.6299999999999955Avance: %5.640000000000001Avance: %5.650000000000006Avance: %5.659999999999997Avance: %5.670000000000002Avance: %5.679999999999993Avance: %5.689999999999998Avance: %5.700000000000003Avance: %5.710000000000008Avance: %5.719999999999999Avance: %5.730000000000004Avance: %5.739999999999995Avance: %5.75Avance: %5.760000000000005Avance: %5.769999999999996Avance: %5.780000000000001Avance: %5.789999999999992Avance: %5.800000000000011Avance: %5.810000000000002Avance: %5.820000000000007Avance: %5.829999999999998Avance: %5.840000000000003Avance: %5.849999999999994Avance: %5.859999999999999Avance: %5.8700000000000045Avance: %5.8799999999999955Avance: %5.890000000000001Avance: %5.900000000000006Avance: %5.910000000000011Avance: %5.920000000000002Avance: %5.930000000000007Avance: %5.939999999999998Avance: %5.950000000000003Avance: %5.959999999999994Avance: %5.969999999999999Avance: %5.97999999999999Avance: %5.989999999999995Avance: %6.0Avance: %6.010000000000005Avance: %6.02000000000001Avance: %6.030000000000001Avance: %6.040000000000006Avance: %6.049999999999997Avance: %6.060000000000002Avance: %6.069999999999993Avance: %6.079999999999998Avance: %6.089999999999989Avance: %6.1000000000000085Avance: %6.109999999999999Avance: %6.1200000000000045Avance: %6.1299999999999955Avance: %6.140000000000001Avance: %6.150000000000006Avance: %6.159999999999997Avance: %6.170000000000002Avance: %6.179999999999993Avance: %6.189999999999998Avance: %6.200000000000003Avance: %6.210000000000008Avance: %6.219999999999999Avance: %6.230000000000004Avance: %6.239999999999995Avance: %6.25Avance: %6.260000000000005Avance: %6.269999999999996Avance: %6.280000000000001Avance: %6.289999999999992Avance: %6.299999999999997Avance: %6.310000000000002Avance: %6.320000000000007Avance: %6.329999999999998Avance: %6.340000000000003Avance: %6.349999999999994Avance: %6.359999999999999Avance: %6.3700000000000045Avance: %6.3799999999999955Avance: %6.390000000000001Avance: %6.3999999999999915Avance: %6.410000000000011Avance: %6.420000000000002Avance: %6.430000000000007Avance: %6.439999999999998Avance: %6.450000000000003Avance: %6.459999999999994Avance: %6.469999999999999Avance: %6.47999999999999Avance: %6.489999999999995Avance: %6.5Avance: %6.510000000000005Avance: %6.52000000000001Avance: %6.530000000000001Avance: %6.540000000000006Avance: %6.549999999999997Avance: %6.560000000000002Avance: %6.569999999999993Avance: %6.579999999999998Avance: %6.589999999999989Avance: %6.599999999999994Avance: %6.609999999999999Avance: %6.6200000000000045Avance: %6.6299999999999955Avance: %6.640000000000001Avance: %6.650000000000006Avance: %6.659999999999997Avance: %6.670000000000002Avance: %6.679999999999993Avance: %6.689999999999998Avance: %6.699999999999989Avance: %6.710000000000008Avance: %6.719999999999999Avance: %6.730000000000004Avance: %6.739999999999995Avance: %6.75Avance: %6.760000000000005Avance: %6.769999999999996Avance: %6.780000000000001Avance: %6.789999999999992Avance: %6.799999999999997Avance: %6.810000000000002Avance: %6.820000000000007Avance: %6.829999999999998Avance: %6.840000000000003Avance: %6.849999999999994Avance: %6.859999999999999Avance: %6.8700000000000045Avance: %6.8799999999999955Avance: %6.890000000000001Avance: %6.8999999999999915Avance: %6.910000000000011Avance: %6.920000000000002Avance: %6.930000000000007Avance: %6.939999999999998Avance: %6.950000000000003Avance: %6.959999999999994Avance: %6.969999999999999Avance: %6.980000000000004Avance: %6.989999999999995Avance: %7.0Avance: %7.010000000000005Avance: %7.02000000000001Avance: %7.030000000000001Avance: %7.040000000000006Avance: %7.049999999999997Avance: %7.060000000000002Avance: %7.069999999999993Avance: %7.079999999999998Avance: %7.090000000000003Avance: %7.099999999999994Avance: %7.109999999999999Avance: %7.1200000000000045Avance: %7.13000000000001Avance: %7.140000000000001Avance: %7.150000000000006Avance: %7.159999999999997Avance: %7.170000000000002Avance: %7.179999999999993Avance: %7.189999999999998Avance: %7.199999999999989Avance: %7.210000000000008Avance: %7.219999999999999Avance: %7.230000000000004Avance: %7.240000000000009Avance: %7.25Avance: %7.260000000000005Avance: %7.269999999999996Avance: %7.280000000000001Avance: %7.289999999999992Avance: %7.299999999999997Avance: %7.310000000000002Avance: %7.320000000000007Avance: %7.329999999999998Avance: %7.340000000000003Avance: %7.349999999999994Avance: %7.359999999999999Avance: %7.3700000000000045Avance: %7.3799999999999955Avance: %7.390000000000001Avance: %7.3999999999999915Avance: %7.410000000000011Avance: %7.420000000000002Avance: %7.430000000000007Avance: %7.439999999999998Avance: %7.450000000000003Avance: %7.459999999999994Avance: %7.469999999999999Avance: %7.480000000000004Avance: %7.489999999999995Avance: %7.5Avance: %7.509999999999991Avance: %7.52000000000001Avance: %7.530000000000001Avance: %7.540000000000006Avance: %7.549999999999997Avance: %7.560000000000002Avance: %7.569999999999993Avance: %7.579999999999998Avance: %7.590000000000003Avance: %7.599999999999994Avance: %7.609999999999999Avance: %7.6200000000000045Avance: %7.63000000000001Avance: %7.640000000000001Avance: %7.650000000000006Avance: %7.659999999999997Avance: %7.670000000000002Avance: %7.679999999999993Avance: %7.689999999999998Avance: %7.699999999999989Avance: %7.709999999999994Avance: %7.719999999999999Avance: %7.730000000000004Avance: %7.740000000000009Avance: %7.75Avance: %7.760000000000005Avance: %7.769999999999996Avance: %7.780000000000001Avance: %7.789999999999992Avance: %7.799999999999997Avance: %7.809999999999988Avance: %7.820000000000007Avance: %7.829999999999998Avance: %7.840000000000003Avance: %7.849999999999994Avance: %7.859999999999999Avance: %7.8700000000000045Avance: %7.8799999999999955Avance: %7.890000000000001Avance: %7.8999999999999915Avance: %7.909999999999997Avance: %7.920000000000002Avance: %7.930000000000007Avance: %7.939999999999998Avance: %7.950000000000003Avance: %7.959999999999994Avance: %7.969999999999999Avance: %7.980000000000004Avance: %7.989999999999995Avance: %8.0Avance: %8.009999999999991Avance: %8.02000000000001Avance: %8.030000000000001Avance: %8.040000000000006Avance: %8.049999999999997Avance: %8.060000000000002Avance: %8.069999999999993Avance: %8.079999999999998Avance: %8.090000000000003Avance: %8.099999999999994Avance: %8.11Avance: %8.120000000000005Avance: %8.13000000000001Avance: %8.14Avance: %8.150000000000006Avance: %8.159999999999997Avance: %8.170000000000002Avance: %8.179999999999993Avance: %8.189999999999998Avance: %8.200000000000003Avance: %8.209999999999994Avance: %8.219999999999999Avance: %8.230000000000004Avance: %8.240000000000009Avance: %8.25Avance: %8.260000000000005Avance: %8.269999999999996Avance: %8.280000000000001Avance: %8.289999999999992Avance: %8.299999999999997Avance: %8.310000000000002Avance: %8.320000000000007Avance: %8.329999999999998Avance: %8.340000000000003Avance: %8.350000000000009Avance: %8.36Avance: %8.370000000000005Avance: %8.379999999999995Avance: %8.39Avance: %8.399999999999991Avance: %8.409999999999997Avance: %8.420000000000002Avance: %8.430000000000007Avance: %8.439999999999998Avance: %8.450000000000003Avance: %8.460000000000008Avance: %8.469999999999999Avance: %8.480000000000004Avance: %8.489999999999995Avance: %8.5Avance: %8.509999999999991Avance: %8.52000000000001Avance: %8.530000000000001Avance: %8.540000000000006Avance: %8.549999999999997Avance: %8.560000000000002Avance: %8.569999999999993Done Extracting Features 0:07:27.551584
The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.
Try some users for user 274532892
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 614) (2000, 614) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6954
        True       1.00      1.00      1.00        46

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1987
        True       1.00      1.00      1.00        13

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 5695632
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 342) (2000, 342) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6994
        True       1.00      1.00      1.00         6

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1999
        True       1.00      1.00      1.00         1

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 205827262
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 736) (2000, 736) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6927
        True       1.00      1.00      1.00        73

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1979
        True       1.00      1.00      1.00        21

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 136706767
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 224) (2000, 224) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6513
        True       1.00      0.98      0.99       487

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1861
        True       1.00      0.99      1.00       139

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 142532652
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 526) (2000, 526) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': None, 'gamma': 10, 'kernel': 'poly'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      5371
        True       1.00      1.00      1.00      1629

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       0.99      1.00      0.99      1535
        True       0.99      0.97      0.98       465

   micro avg       0.99      0.99      0.99      2000
   macro avg       0.99      0.98      0.99      2000
weighted avg       0.99      0.99      0.99      2000


			END SVC
Try some users for user 14342112
Loading df
Done loading df. DF shape is :(12397359, 8) (Original: (12397359, 8)) 		Time delta is: None mins
Loading graph from ../data/graphs/latest_subgraph.gpickle
Fetched 500 level 2 neighbourhs for user 14342112.
Len of own retweets (positive examples) is (4, 2)
done getting tweets universe. Shape is  (10241, 2)
	Dataset was truncated to 10000 tweets
Extracting features Optimized. X shape is : (10000, 500)
Avance: %0.010000000000005116Avance: %0.01999999999999602Avance: %0.030000000000001137Avance: %0.03999999999999204Avance: %0.04999999999999716Avance: %0.060000000000002274Avance: %0.07000000000000739Avance: %0.0799999999999983Avance: %0.09000000000000341Avance: %0.09999999999999432Avance: %0.10999999999999943Avance: %0.12000000000000455Avance: %0.12999999999999545Avance: %0.14000000000000057Avance: %0.14999999999999147Avance: %0.1600000000000108Avance: %0.1700000000000017Avance: %0.18000000000000682Avance: %0.18999999999999773Avance: %0.20000000000000284Avance: %0.20999999999999375Avance: %0.21999999999999886Avance: %0.22999999999998977Avance: %0.23999999999999488Avance: %0.25Avance: %0.2600000000000051Avance: %0.27000000000001023Avance: %0.28000000000000114Avance: %0.29000000000000625Avance: %0.29999999999999716Avance: %0.3100000000000023Avance: %0.3199999999999932Avance: %0.3299999999999983Avance: %0.3399999999999892Avance: %0.3499999999999943Avance: %0.35999999999999943Avance: %0.37000000000000455Avance: %0.37999999999999545Avance: %0.39000000000000057Avance: %0.4000000000000057Avance: %0.4099999999999966Avance: %0.4200000000000017Avance: %0.4299999999999926Avance: %0.4399999999999977Avance: %0.44999999999998863Avance: %0.46000000000000796Avance: %0.46999999999999886Avance: %0.480000000000004Avance: %0.4899999999999949Avance: %0.5Avance: %0.5100000000000051Avance: %0.519999999999996Avance: %0.5300000000000011Avance: %0.539999999999992Avance: %0.5499999999999972Avance: %0.5600000000000023Avance: %0.5700000000000074Avance: %0.5799999999999983Avance: %0.5900000000000034Avance: %0.5999999999999943Avance: %0.6099999999999994Avance: %0.6200000000000045Avance: %0.6299999999999955Avance: %0.6400000000000006Avance: %0.6499999999999915Avance: %0.6600000000000108Avance: %0.6700000000000017Avance: %0.6800000000000068Avance: %0.6899999999999977Avance: %0.7000000000000028Avance: %0.7099999999999937Avance: %0.7199999999999989Avance: %0.730000000000004Avance: %0.7399999999999949Avance: %0.75Avance: %0.7600000000000051Avance: %0.7700000000000102Avance: %0.7800000000000011Avance: %0.7900000000000063Avance: %0.7999999999999972Avance: %0.8100000000000023Avance: %0.8199999999999932Avance: %0.8299999999999983Avance: %0.8400000000000034Avance: %0.8499999999999943Avance: %0.8599999999999994Avance: %0.8700000000000045Avance: %0.8800000000000097Avance: %0.8900000000000006Avance: %0.9000000000000057Avance: %0.9099999999999966Avance: %0.9200000000000017Avance: %0.9299999999999926Avance: %0.9399999999999977Avance: %0.9499999999999886Avance: %0.960000000000008Avance: %0.9699999999999989Avance: %0.980000000000004Avance: %0.9900000000000091Avance: %1.0Avance: %1.0100000000000051Avance: %1.019999999999996Avance: %1.0300000000000011Avance: %1.039999999999992Avance: %1.0499999999999972Avance: %1.0600000000000023Avance: %1.0700000000000074Avance: %1.0799999999999983Avance: %1.0900000000000034Avance: %1.0999999999999943Avance: %1.1099999999999994Avance: %1.1200000000000045Avance: %1.1299999999999955Avance: %1.1400000000000006Avance: %1.1499999999999915Avance: %1.1600000000000108Avance: %1.1700000000000017Avance: %1.1800000000000068Avance: %1.1899999999999977Avance: %1.2000000000000028Avance: %1.2099999999999937Avance: %1.2199999999999989Avance: %1.230000000000004Avance: %1.2399999999999949Avance: %1.25Avance: %1.259999999999991Avance: %1.2700000000000102Avance: %1.2800000000000011Avance: %1.2900000000000063Avance: %1.2999999999999972Avance: %1.3100000000000023Avance: %1.3199999999999932Avance: %1.3299999999999983Avance: %1.3400000000000034Avance: %1.3499999999999943Avance: %1.3599999999999994Avance: %1.3700000000000045Avance: %1.3800000000000097Avance: %1.3900000000000006Avance: %1.4000000000000057Avance: %1.4099999999999966Avance: %1.4200000000000017Avance: %1.4299999999999926Avance: %1.4399999999999977Avance: %1.4499999999999886Avance: %1.4599999999999937Avance: %1.4699999999999989Avance: %1.480000000000004Avance: %1.490000000000009Avance: %1.5Avance: %1.5100000000000051Avance: %1.519999999999996Avance: %1.5300000000000011Avance: %1.539999999999992Avance: %1.5499999999999972Avance: %1.559999999999988Avance: %1.5700000000000074Avance: %1.5799999999999983Avance: %1.5900000000000034Avance: %1.5999999999999943Avance: %1.6099999999999994Avance: %1.6200000000000045Avance: %1.6299999999999955Avance: %1.6400000000000006Avance: %1.6499999999999915Avance: %1.6599999999999966Avance: %1.6700000000000017Avance: %1.6800000000000068Avance: %1.6899999999999977Avance: %1.7000000000000028Avance: %1.7099999999999937Avance: %1.7199999999999989Avance: %1.730000000000004Avance: %1.7399999999999949Avance: %1.75Avance: %1.759999999999991Avance: %1.7700000000000102Avance: %1.7800000000000011Avance: %1.7900000000000063Avance: %1.7999999999999972Avance: %1.8100000000000023Avance: %1.8199999999999932Avance: %1.8299999999999983Avance: %1.8400000000000034Avance: %1.8499999999999943Avance: %1.8599999999999994Avance: %1.8700000000000045Avance: %1.8800000000000097Avance: %1.8900000000000006Avance: %1.9000000000000057Avance: %1.9099999999999966Avance: %1.9200000000000017Avance: %1.9299999999999926Avance: %1.9399999999999977Avance: %1.9500000000000028Avance: %1.9599999999999937Avance: %1.9699999999999989Avance: %1.980000000000004Avance: %1.990000000000009Avance: %2.0Avance: %2.010000000000005Avance: %2.019999999999996Avance: %2.030000000000001Avance: %2.039999999999992Avance: %2.049999999999997Avance: %2.0600000000000023Avance: %2.0700000000000074Avance: %2.0799999999999983Avance: %2.0900000000000034Avance: %2.1000000000000085Avance: %2.1099999999999994Avance: %2.1200000000000045Avance: %2.1299999999999955Avance: %2.1400000000000006Avance: %2.1499999999999915Avance: %2.1599999999999966Avance: %2.1700000000000017Avance: %2.180000000000007Avance: %2.1899999999999977Avance: %2.200000000000003Avance: %2.210000000000008Avance: %2.219999999999999Avance: %2.230000000000004Avance: %2.239999999999995Avance: %2.25Avance: %2.259999999999991Avance: %2.2700000000000102Avance: %2.280000000000001Avance: %2.2900000000000063Avance: %2.299999999999997Avance: %2.3100000000000023Avance: %2.319999999999993Avance: %2.3299999999999983Avance: %2.3400000000000034Avance: %2.3499999999999943Avance: %2.3599999999999994Avance: %2.3700000000000045Avance: %2.3800000000000097Avance: %2.3900000000000006Avance: %2.4000000000000057Avance: %2.4099999999999966Avance: %2.4200000000000017Avance: %2.4299999999999926Avance: %2.4399999999999977Avance: %2.450000000000003Avance: %2.4599999999999937Avance: %2.469999999999999Avance: %2.480000000000004Avance: %2.490000000000009Avance: %2.5Avance: %2.510000000000005Avance: %2.519999999999996Avance: %2.530000000000001Avance: %2.539999999999992Avance: %2.549999999999997Avance: %2.5600000000000023Avance: %2.569999999999993Avance: %2.5799999999999983Avance: %2.5900000000000034Avance: %2.6000000000000085Avance: %2.6099999999999994Avance: %2.6200000000000045Avance: %2.6299999999999955Avance: %2.6400000000000006Avance: %2.6499999999999915Avance: %2.6599999999999966Avance: %2.6699999999999875Avance: %2.680000000000007Avance: %2.6899999999999977Avance: %2.700000000000003Avance: %2.710000000000008Avance: %2.719999999999999Avance: %2.730000000000004Avance: %2.739999999999995Avance: %2.75Avance: %2.759999999999991Avance: %2.769999999999996Avance: %2.780000000000001Avance: %2.7900000000000063Avance: %2.799999999999997Avance: %2.8100000000000023Avance: %2.819999999999993Avance: %2.8299999999999983Avance: %2.8400000000000034Avance: %2.8499999999999943Avance: %2.8599999999999994Avance: %2.8699999999999903Avance: %2.8800000000000097Avance: %2.8900000000000006Avance: %2.9000000000000057Avance: %2.9099999999999966Avance: %2.9200000000000017Avance: %2.9299999999999926Avance: %2.9399999999999977Avance: %2.950000000000003Avance: %2.9599999999999937Avance: %2.969999999999999Avance: %2.980000000000004Avance: %2.990000000000009Avance: %3.0Avance: %3.010000000000005Avance: %3.019999999999996Avance: %3.030000000000001Avance: %3.039999999999992Avance: %3.049999999999997Avance: %3.0600000000000023Avance: %3.069999999999993Avance: %3.0799999999999983Avance: %3.0900000000000034Avance: %3.1000000000000085Avance: %3.1099999999999994Avance: %3.1200000000000045Avance: %3.1299999999999955Avance: %3.1400000000000006Avance: %3.1499999999999915Avance: %3.1599999999999966Avance: %3.1700000000000017Avance: %3.180000000000007Avance: %3.1899999999999977Avance: %3.200000000000003Avance: %3.210000000000008Avance: %3.219999999999999Avance: %3.230000000000004Avance: %3.239999999999995Avance: %3.25Avance: %3.259999999999991Avance: %3.269999999999996Avance: %3.280000000000001Avance: %3.2900000000000063Avance: %3.299999999999997Avance: %3.3100000000000023Avance: %3.3200000000000074Avance: %3.3299999999999983Avance: %3.3400000000000034Avance: %3.3499999999999943Avance: %3.3599999999999994Avance: %3.3699999999999903Avance: %3.3800000000000097Avance: %3.3900000000000006Avance: %3.4000000000000057Avance: %3.4099999999999966Avance: %3.4200000000000017Avance: %3.430000000000007Avance: %3.4399999999999977Avance: %3.450000000000003Avance: %3.4599999999999937Avance: %3.469999999999999Avance: %3.480000000000004Avance: %3.490000000000009Avance: %3.5Avance: %3.510000000000005Avance: %3.519999999999996Avance: %3.530000000000001Avance: %3.539999999999992Avance: %3.549999999999997Avance: %3.5600000000000023Avance: %3.569999999999993Avance: %3.5800000000000125Avance: %3.5900000000000034Avance: %3.6000000000000085Avance: %3.6099999999999994Avance: %3.6200000000000045Avance: %3.6299999999999955Avance: %3.6400000000000006Avance: %3.6499999999999915Avance: %3.6599999999999966Avance: %3.6700000000000017Avance: %3.680000000000007Avance: %3.6899999999999977Avance: %3.700000000000003Avance: %3.710000000000008Avance: %3.719999999999999Avance: %3.730000000000004Avance: %3.739999999999995Avance: %3.75Avance: %3.759999999999991Avance: %3.769999999999996Avance: %3.780000000000001Avance: %3.7900000000000063Avance: %3.799999999999997Avance: %3.8100000000000023Avance: %3.8200000000000074Avance: %3.8299999999999983Avance: %3.8400000000000034Avance: %3.8499999999999943Avance: %3.8599999999999994Avance: %3.8699999999999903Avance: %3.8799999999999955Avance: %3.8900000000000006Avance: %3.9000000000000057Avance: %3.9099999999999966Avance: %3.9200000000000017Avance: %3.930000000000007Avance: %3.9399999999999977Avance: %3.950000000000003Avance: %3.9599999999999937Avance: %3.969999999999999Avance: %3.9799999999999898Avance: %3.990000000000009Avance: %4.0Avance: %4.010000000000005Avance: %4.019999999999996Avance: %4.030000000000001Avance: %4.039999999999992Avance: %4.049999999999997Avance: %4.060000000000002Avance: %4.069999999999993Avance: %4.079999999999998Avance: %4.090000000000003Avance: %4.1000000000000085Avance: %4.109999999999999Avance: %4.1200000000000045Avance: %4.1299999999999955Avance: %4.140000000000001Avance: %4.1499999999999915Avance: %4.159999999999997Avance: %4.170000000000002Avance: %4.179999999999993Avance: %4.189999999999998Avance: %4.200000000000003Avance: %4.210000000000008Avance: %4.219999999999999Avance: %4.230000000000004Avance: %4.239999999999995Avance: %4.25Avance: %4.259999999999991Avance: %4.269999999999996Avance: %4.280000000000001Avance: %4.290000000000006Avance: %4.299999999999997Avance: %4.310000000000002Avance: %4.320000000000007Avance: %4.329999999999998Avance: %4.340000000000003Avance: %4.349999999999994Avance: %4.359999999999999Avance: %4.36999999999999Avance: %4.3799999999999955Avance: %4.390000000000001Avance: %4.400000000000006Avance: %4.409999999999997Avance: %4.420000000000002Avance: %4.430000000000007Avance: %4.439999999999998Avance: %4.450000000000003Avance: %4.459999999999994Avance: %4.469999999999999Avance: %4.47999999999999Avance: %4.490000000000009Avance: %4.5Avance: %4.510000000000005Avance: %4.519999999999996Avance: %4.530000000000001Avance: %4.540000000000006Avance: %4.549999999999997Avance: %4.560000000000002Avance: %4.569999999999993Avance: %4.579999999999998Avance: %4.590000000000003Avance: %4.6000000000000085Avance: %4.609999999999999Avance: %4.6200000000000045Avance: %4.6299999999999955Avance: %4.640000000000001Avance: %4.650000000000006Avance: %4.659999999999997Avance: %4.670000000000002Avance: %4.679999999999993Avance: %4.690000000000012Avance: %4.700000000000003Avance: %4.710000000000008Avance: %4.719999999999999Avance: %4.730000000000004Avance: %4.739999999999995Avance: %4.75Avance: %4.759999999999991Avance: %4.769999999999996Avance: %4.780000000000001Avance: %4.790000000000006Avance: %4.800000000000011Avance: %4.810000000000002Avance: %4.820000000000007Avance: %4.829999999999998Avance: %4.840000000000003Avance: %4.849999999999994Avance: %4.859999999999999Avance: %4.86999999999999Avance: %4.8799999999999955Avance: %4.890000000000001Avance: %4.900000000000006Avance: %4.909999999999997Avance: %4.920000000000002Avance: %4.930000000000007Avance: %4.939999999999998Avance: %4.950000000000003Avance: %4.959999999999994Avance: %4.969999999999999Avance: %4.97999999999999Avance: %4.990000000000009Avance: %5.0Done Extracting Features 0:04:50.241645
The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.
Try some users for user 67334466
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 585) (2000, 585) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6989
        True       1.00      1.00      1.00        11

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1997
        True       1.00      1.00      1.00         3

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 2467791
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 347) (2000, 347) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 10, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6994
        True       1.00      0.83      0.91         6

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.92      0.95      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1998
        True       1.00      1.00      1.00         2

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 61577575
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 551) (2000, 551) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6975
        True       1.00      1.00      1.00        25

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1993
        True       1.00      1.00      1.00         7

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 48023
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 747) (2000, 747) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6976
        True       1.00      1.00      1.00        24

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1993
        True       1.00      1.00      1.00         7

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 7764352
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 733) (2000, 733) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6980
        True       1.00      1.00      1.00        20

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1995
        True       1.00      1.00      1.00         5

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 182831320
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 372) (2000, 372) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6569
        True       1.00      0.97      0.99       431

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1877
        True       1.00      0.99      1.00       123

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 8129862
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 498) (2000, 498) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6966
        True       1.00      1.00      1.00        34

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1990
        True       1.00      1.00      1.00        10

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 263865213
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 562) (2000, 562) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6590
        True       1.00      0.98      0.99       410

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1883
        True       1.00      0.98      0.99       117

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.99      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 276169885
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 475) (2000, 475) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6983
        True       1.00      0.88      0.94        17

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.94      0.97      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1995
        True       1.00      0.80      0.89         5

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.90      0.94      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 471363110
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 379) (2000, 379) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': None, 'gamma': 10, 'kernel': 'poly'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6995
        True       1.00      0.80      0.89         5

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.90      0.94      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1999
        True       1.00      1.00      1.00         1

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 90197038
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 628) (2000, 628) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6931
        True       1.00      1.00      1.00        69

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1981
        True       1.00      1.00      1.00        19

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 292458412
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 539) (2000, 539) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6912
        True       1.00      0.98      0.99        88

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1975
        True       1.00      1.00      1.00        25

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 29858524
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 819) (2000, 819) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6868
        True       1.00      1.00      1.00       132

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1962
        True       1.00      1.00      1.00        38

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 169965323
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 524) (2000, 524) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6933
        True       1.00      0.99      0.99        67

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1981
        True       1.00      1.00      1.00        19

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 35744749
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 529) (2000, 529) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6429
        True       1.00      0.97      0.98       571

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.98      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       0.99      1.00      1.00      1837
        True       1.00      0.93      0.97       163

   micro avg       0.99      0.99      0.99      2000
   macro avg       1.00      0.97      0.98      2000
weighted avg       0.99      0.99      0.99      2000


			END SVC
Try some users for user 69321798
Loading df
Done loading df. DF shape is :(12397359, 8) (Original: (12397359, 8)) 		Time delta is: None mins
Loading graph from ../data/graphs/latest_subgraph.gpickle
Fetched 694 level 2 neighbourhs for user 69321798.
Len of own retweets (positive examples) is (5, 2)
done getting tweets universe. Shape is  (12351, 2)
	Dataset was truncated to 10000 tweets
Extracting features Optimized. X shape is : (10000, 694)
Avance: %0.010000000000005116Avance: %0.01999999999999602Avance: %0.030000000000001137Avance: %0.03999999999999204Avance: %0.04999999999999716Avance: %0.060000000000002274Avance: %0.07000000000000739Avance: %0.0799999999999983Avance: %0.09000000000000341Avance: %0.09999999999999432Avance: %0.10999999999999943Avance: %0.12000000000000455Avance: %0.12999999999999545Avance: %0.14000000000000057Avance: %0.14999999999999147Avance: %0.1600000000000108Avance: %0.1700000000000017Avance: %0.18000000000000682Avance: %0.18999999999999773Avance: %0.20000000000000284Avance: %0.20999999999999375Avance: %0.21999999999999886Avance: %0.22999999999998977Avance: %0.23999999999999488Avance: %0.25Avance: %0.2600000000000051Avance: %0.27000000000001023Avance: %0.28000000000000114Avance: %0.29000000000000625Avance: %0.29999999999999716Avance: %0.3100000000000023Avance: %0.3199999999999932Avance: %0.3299999999999983Avance: %0.3399999999999892Avance: %0.3499999999999943Avance: %0.35999999999999943Avance: %0.37000000000000455Avance: %0.37999999999999545Avance: %0.39000000000000057Avance: %0.4000000000000057Avance: %0.4099999999999966Avance: %0.4200000000000017Avance: %0.4299999999999926Avance: %0.4399999999999977Avance: %0.44999999999998863Avance: %0.46000000000000796Avance: %0.46999999999999886Avance: %0.480000000000004Avance: %0.4899999999999949Avance: %0.5Avance: %0.5100000000000051Avance: %0.519999999999996Avance: %0.5300000000000011Avance: %0.539999999999992Avance: %0.5499999999999972Avance: %0.5600000000000023Avance: %0.5700000000000074Avance: %0.5799999999999983Avance: %0.5900000000000034Avance: %0.5999999999999943Avance: %0.6099999999999994Avance: %0.6200000000000045Avance: %0.6299999999999955Avance: %0.6400000000000006Avance: %0.6499999999999915Avance: %0.6600000000000108Avance: %0.6700000000000017Avance: %0.6800000000000068Avance: %0.6899999999999977Avance: %0.7000000000000028Avance: %0.7099999999999937Avance: %0.7199999999999989Avance: %0.730000000000004Avance: %0.7399999999999949Avance: %0.75Avance: %0.7600000000000051Avance: %0.7700000000000102Avance: %0.7800000000000011Avance: %0.7900000000000063Avance: %0.7999999999999972Avance: %0.8100000000000023Avance: %0.8199999999999932Avance: %0.8299999999999983Avance: %0.8400000000000034Avance: %0.8499999999999943Avance: %0.8599999999999994Avance: %0.8700000000000045Avance: %0.8800000000000097Avance: %0.8900000000000006Avance: %0.9000000000000057Avance: %0.9099999999999966Avance: %0.9200000000000017Avance: %0.9299999999999926Avance: %0.9399999999999977Avance: %0.9499999999999886Avance: %0.960000000000008Avance: %0.9699999999999989Avance: %0.980000000000004Avance: %0.9900000000000091Avance: %1.0Avance: %1.0100000000000051Avance: %1.019999999999996Avance: %1.0300000000000011Avance: %1.039999999999992Avance: %1.0499999999999972Avance: %1.0600000000000023Avance: %1.0700000000000074Avance: %1.0799999999999983Avance: %1.0900000000000034Avance: %1.0999999999999943Avance: %1.1099999999999994Avance: %1.1200000000000045Avance: %1.1299999999999955Avance: %1.1400000000000006Avance: %1.1499999999999915Avance: %1.1600000000000108Avance: %1.1700000000000017Avance: %1.1800000000000068Avance: %1.1899999999999977Avance: %1.2000000000000028Avance: %1.2099999999999937Avance: %1.2199999999999989Avance: %1.230000000000004Avance: %1.2399999999999949Avance: %1.25Avance: %1.259999999999991Avance: %1.2700000000000102Avance: %1.2800000000000011Avance: %1.2900000000000063Avance: %1.2999999999999972Avance: %1.3100000000000023Avance: %1.3199999999999932Avance: %1.3299999999999983Avance: %1.3400000000000034Avance: %1.3499999999999943Avance: %1.3599999999999994Avance: %1.3700000000000045Avance: %1.3800000000000097Avance: %1.3900000000000006Avance: %1.4000000000000057Avance: %1.4099999999999966Avance: %1.4200000000000017Avance: %1.4299999999999926Avance: %1.4399999999999977Avance: %1.4499999999999886Avance: %1.4599999999999937Avance: %1.4699999999999989Avance: %1.480000000000004Avance: %1.490000000000009Avance: %1.5Avance: %1.5100000000000051Avance: %1.519999999999996Avance: %1.5300000000000011Avance: %1.539999999999992Avance: %1.5499999999999972Avance: %1.559999999999988Avance: %1.5700000000000074Avance: %1.5799999999999983Avance: %1.5900000000000034Avance: %1.5999999999999943Avance: %1.6099999999999994Avance: %1.6200000000000045Avance: %1.6299999999999955Avance: %1.6400000000000006Avance: %1.6499999999999915Avance: %1.6599999999999966Avance: %1.6700000000000017Avance: %1.6800000000000068Avance: %1.6899999999999977Avance: %1.7000000000000028Avance: %1.7099999999999937Avance: %1.7199999999999989Avance: %1.730000000000004Avance: %1.7399999999999949Avance: %1.75Avance: %1.759999999999991Avance: %1.7700000000000102Avance: %1.7800000000000011Avance: %1.7900000000000063Avance: %1.7999999999999972Avance: %1.8100000000000023Avance: %1.8199999999999932Avance: %1.8299999999999983Avance: %1.8400000000000034Avance: %1.8499999999999943Avance: %1.8599999999999994Avance: %1.8700000000000045Avance: %1.8800000000000097Avance: %1.8900000000000006Avance: %1.9000000000000057Avance: %1.9099999999999966Avance: %1.9200000000000017Avance: %1.9299999999999926Avance: %1.9399999999999977Avance: %1.9500000000000028Avance: %1.9599999999999937Avance: %1.9699999999999989Avance: %1.980000000000004Avance: %1.990000000000009Avance: %2.0Avance: %2.010000000000005Avance: %2.019999999999996Avance: %2.030000000000001Avance: %2.039999999999992Avance: %2.049999999999997Avance: %2.0600000000000023Avance: %2.0700000000000074Avance: %2.0799999999999983Avance: %2.0900000000000034Avance: %2.1000000000000085Avance: %2.1099999999999994Avance: %2.1200000000000045Avance: %2.1299999999999955Avance: %2.1400000000000006Avance: %2.1499999999999915Avance: %2.1599999999999966Avance: %2.1700000000000017Avance: %2.180000000000007Avance: %2.1899999999999977Avance: %2.200000000000003Avance: %2.210000000000008Avance: %2.219999999999999Avance: %2.230000000000004Avance: %2.239999999999995Avance: %2.25Avance: %2.259999999999991Avance: %2.2700000000000102Avance: %2.280000000000001Avance: %2.2900000000000063Avance: %2.299999999999997Avance: %2.3100000000000023Avance: %2.319999999999993Avance: %2.3299999999999983Avance: %2.3400000000000034Avance: %2.3499999999999943Avance: %2.3599999999999994Avance: %2.3700000000000045Avance: %2.3800000000000097Avance: %2.3900000000000006Avance: %2.4000000000000057Avance: %2.4099999999999966Avance: %2.4200000000000017Avance: %2.4299999999999926Avance: %2.4399999999999977Avance: %2.450000000000003Avance: %2.4599999999999937Avance: %2.469999999999999Avance: %2.480000000000004Avance: %2.490000000000009Avance: %2.5Avance: %2.510000000000005Avance: %2.519999999999996Avance: %2.530000000000001Avance: %2.539999999999992Avance: %2.549999999999997Avance: %2.5600000000000023Avance: %2.569999999999993Avance: %2.5799999999999983Avance: %2.5900000000000034Avance: %2.6000000000000085Avance: %2.6099999999999994Avance: %2.6200000000000045Avance: %2.6299999999999955Avance: %2.6400000000000006Avance: %2.6499999999999915Avance: %2.6599999999999966Avance: %2.6699999999999875Avance: %2.680000000000007Avance: %2.6899999999999977Avance: %2.700000000000003Avance: %2.710000000000008Avance: %2.719999999999999Avance: %2.730000000000004Avance: %2.739999999999995Avance: %2.75Avance: %2.759999999999991Avance: %2.769999999999996Avance: %2.780000000000001Avance: %2.7900000000000063Avance: %2.799999999999997Avance: %2.8100000000000023Avance: %2.819999999999993Avance: %2.8299999999999983Avance: %2.8400000000000034Avance: %2.8499999999999943Avance: %2.8599999999999994Avance: %2.8699999999999903Avance: %2.8800000000000097Avance: %2.8900000000000006Avance: %2.9000000000000057Avance: %2.9099999999999966Avance: %2.9200000000000017Avance: %2.9299999999999926Avance: %2.9399999999999977Avance: %2.950000000000003Avance: %2.9599999999999937Avance: %2.969999999999999Avance: %2.980000000000004Avance: %2.990000000000009Avance: %3.0Avance: %3.010000000000005Avance: %3.019999999999996Avance: %3.030000000000001Avance: %3.039999999999992Avance: %3.049999999999997Avance: %3.0600000000000023Avance: %3.069999999999993Avance: %3.0799999999999983Avance: %3.0900000000000034Avance: %3.1000000000000085Avance: %3.1099999999999994Avance: %3.1200000000000045Avance: %3.1299999999999955Avance: %3.1400000000000006Avance: %3.1499999999999915Avance: %3.1599999999999966Avance: %3.1700000000000017Avance: %3.180000000000007Avance: %3.1899999999999977Avance: %3.200000000000003Avance: %3.210000000000008Avance: %3.219999999999999Avance: %3.230000000000004Avance: %3.239999999999995Avance: %3.25Avance: %3.259999999999991Avance: %3.269999999999996Avance: %3.280000000000001Avance: %3.2900000000000063Avance: %3.299999999999997Avance: %3.3100000000000023Avance: %3.3200000000000074Avance: %3.3299999999999983Avance: %3.3400000000000034Avance: %3.3499999999999943Avance: %3.3599999999999994Avance: %3.3699999999999903Avance: %3.3800000000000097Avance: %3.3900000000000006Avance: %3.4000000000000057Avance: %3.4099999999999966Avance: %3.4200000000000017Avance: %3.430000000000007Avance: %3.4399999999999977Avance: %3.450000000000003Avance: %3.4599999999999937Avance: %3.469999999999999Avance: %3.480000000000004Avance: %3.490000000000009Avance: %3.5Avance: %3.510000000000005Avance: %3.519999999999996Avance: %3.530000000000001Avance: %3.539999999999992Avance: %3.549999999999997Avance: %3.5600000000000023Avance: %3.569999999999993Avance: %3.5800000000000125Avance: %3.5900000000000034Avance: %3.6000000000000085Avance: %3.6099999999999994Avance: %3.6200000000000045Avance: %3.6299999999999955Avance: %3.6400000000000006Avance: %3.6499999999999915Avance: %3.6599999999999966Avance: %3.6700000000000017Avance: %3.680000000000007Avance: %3.6899999999999977Avance: %3.700000000000003Avance: %3.710000000000008Avance: %3.719999999999999Avance: %3.730000000000004Avance: %3.739999999999995Avance: %3.75Avance: %3.759999999999991Avance: %3.769999999999996Avance: %3.780000000000001Avance: %3.7900000000000063Avance: %3.799999999999997Avance: %3.8100000000000023Avance: %3.8200000000000074Avance: %3.8299999999999983Avance: %3.8400000000000034Avance: %3.8499999999999943Avance: %3.8599999999999994Avance: %3.8699999999999903Avance: %3.8799999999999955Avance: %3.8900000000000006Avance: %3.9000000000000057Avance: %3.9099999999999966Avance: %3.9200000000000017Avance: %3.930000000000007Avance: %3.9399999999999977Avance: %3.950000000000003Avance: %3.9599999999999937Avance: %3.969999999999999Avance: %3.9799999999999898Avance: %3.990000000000009Avance: %4.0Avance: %4.010000000000005Avance: %4.019999999999996Avance: %4.030000000000001Avance: %4.039999999999992Avance: %4.049999999999997Avance: %4.060000000000002Avance: %4.069999999999993Avance: %4.079999999999998Avance: %4.090000000000003Avance: %4.1000000000000085Avance: %4.109999999999999Avance: %4.1200000000000045Avance: %4.1299999999999955Avance: %4.140000000000001Avance: %4.1499999999999915Avance: %4.159999999999997Avance: %4.170000000000002Avance: %4.179999999999993Avance: %4.189999999999998Avance: %4.200000000000003Avance: %4.210000000000008Avance: %4.219999999999999Avance: %4.230000000000004Avance: %4.239999999999995Avance: %4.25Avance: %4.259999999999991Avance: %4.269999999999996Avance: %4.280000000000001Avance: %4.290000000000006Avance: %4.299999999999997Avance: %4.310000000000002Avance: %4.320000000000007Avance: %4.329999999999998Avance: %4.340000000000003Avance: %4.349999999999994Avance: %4.359999999999999Avance: %4.36999999999999Avance: %4.3799999999999955Avance: %4.390000000000001Avance: %4.400000000000006Avance: %4.409999999999997Avance: %4.420000000000002Avance: %4.430000000000007Avance: %4.439999999999998Avance: %4.450000000000003Avance: %4.459999999999994Avance: %4.469999999999999Avance: %4.47999999999999Avance: %4.490000000000009Avance: %4.5Avance: %4.510000000000005Avance: %4.519999999999996Avance: %4.530000000000001Avance: %4.540000000000006Avance: %4.549999999999997Avance: %4.560000000000002Avance: %4.569999999999993Avance: %4.579999999999998Avance: %4.590000000000003Avance: %4.6000000000000085Avance: %4.609999999999999Avance: %4.6200000000000045Avance: %4.6299999999999955Avance: %4.640000000000001Avance: %4.650000000000006Avance: %4.659999999999997Avance: %4.670000000000002Avance: %4.679999999999993Avance: %4.690000000000012Avance: %4.700000000000003Avance: %4.710000000000008Avance: %4.719999999999999Avance: %4.730000000000004Avance: %4.739999999999995Avance: %4.75Avance: %4.759999999999991Avance: %4.769999999999996Avance: %4.780000000000001Avance: %4.790000000000006Avance: %4.800000000000011Avance: %4.810000000000002Avance: %4.820000000000007Avance: %4.829999999999998Avance: %4.840000000000003Avance: %4.849999999999994Avance: %4.859999999999999Avance: %4.86999999999999Avance: %4.8799999999999955Avance: %4.890000000000001Avance: %4.900000000000006Avance: %4.909999999999997Avance: %4.920000000000002Avance: %4.930000000000007Avance: %4.939999999999998Avance: %4.950000000000003Avance: %4.959999999999994Avance: %4.969999999999999Avance: %4.97999999999999Avance: %4.990000000000009Avance: %5.0Avance: %5.010000000000005Avance: %5.019999999999996Avance: %5.030000000000001Avance: %5.040000000000006Avance: %5.049999999999997Avance: %5.060000000000002Avance: %5.069999999999993Avance: %5.079999999999998Avance: %5.089999999999989Avance: %5.1000000000000085Avance: %5.109999999999999Avance: %5.1200000000000045Avance: %5.1299999999999955Avance: %5.140000000000001Avance: %5.150000000000006Avance: %5.159999999999997Avance: %5.170000000000002Avance: %5.179999999999993Avance: %5.189999999999998Avance: %5.200000000000003Avance: %5.210000000000008Avance: %5.219999999999999Avance: %5.230000000000004Avance: %5.239999999999995Avance: %5.25Avance: %5.259999999999991Avance: %5.269999999999996Avance: %5.280000000000001Avance: %5.289999999999992Avance: %5.300000000000011Avance: %5.310000000000002Avance: %5.320000000000007Avance: %5.329999999999998Avance: %5.340000000000003Avance: %5.349999999999994Avance: %5.359999999999999Avance: %5.36999999999999Avance: %5.3799999999999955Avance: %5.390000000000001Avance: %5.400000000000006Avance: %5.409999999999997Avance: %5.420000000000002Avance: %5.430000000000007Avance: %5.439999999999998Avance: %5.450000000000003Avance: %5.459999999999994Avance: %5.469999999999999Avance: %5.47999999999999Avance: %5.489999999999995Avance: %5.5Avance: %5.510000000000005Avance: %5.519999999999996Avance: %5.530000000000001Avance: %5.540000000000006Avance: %5.549999999999997Avance: %5.560000000000002Avance: %5.569999999999993Avance: %5.579999999999998Avance: %5.589999999999989Avance: %5.6000000000000085Avance: %5.609999999999999Avance: %5.6200000000000045Avance: %5.6299999999999955Avance: %5.640000000000001Avance: %5.650000000000006Avance: %5.659999999999997Avance: %5.670000000000002Avance: %5.679999999999993Avance: %5.689999999999998Avance: %5.700000000000003Avance: %5.710000000000008Avance: %5.719999999999999Avance: %5.730000000000004Avance: %5.739999999999995Avance: %5.75Avance: %5.760000000000005Avance: %5.769999999999996Avance: %5.780000000000001Avance: %5.789999999999992Avance: %5.800000000000011Avance: %5.810000000000002Avance: %5.820000000000007Avance: %5.829999999999998Avance: %5.840000000000003Avance: %5.849999999999994Avance: %5.859999999999999Avance: %5.8700000000000045Avance: %5.8799999999999955Avance: %5.890000000000001Avance: %5.900000000000006Avance: %5.910000000000011Avance: %5.920000000000002Avance: %5.930000000000007Avance: %5.939999999999998Avance: %5.950000000000003Avance: %5.959999999999994Avance: %5.969999999999999Avance: %5.97999999999999Avance: %5.989999999999995Avance: %6.0Avance: %6.010000000000005Avance: %6.02000000000001Avance: %6.030000000000001Avance: %6.040000000000006Avance: %6.049999999999997Avance: %6.060000000000002Avance: %6.069999999999993Avance: %6.079999999999998Avance: %6.089999999999989Avance: %6.1000000000000085Avance: %6.109999999999999Avance: %6.1200000000000045Avance: %6.1299999999999955Avance: %6.140000000000001Avance: %6.150000000000006Avance: %6.159999999999997Avance: %6.170000000000002Avance: %6.179999999999993Avance: %6.189999999999998Avance: %6.200000000000003Avance: %6.210000000000008Avance: %6.219999999999999Avance: %6.230000000000004Avance: %6.239999999999995Avance: %6.25Avance: %6.260000000000005Avance: %6.269999999999996Avance: %6.280000000000001Avance: %6.289999999999992Avance: %6.299999999999997Avance: %6.310000000000002Avance: %6.320000000000007Avance: %6.329999999999998Avance: %6.340000000000003Avance: %6.349999999999994Avance: %6.359999999999999Avance: %6.3700000000000045Avance: %6.3799999999999955Avance: %6.390000000000001Avance: %6.3999999999999915Avance: %6.410000000000011Avance: %6.420000000000002Avance: %6.430000000000007Avance: %6.439999999999998Avance: %6.450000000000003Avance: %6.459999999999994Avance: %6.469999999999999Avance: %6.47999999999999Avance: %6.489999999999995Avance: %6.5Avance: %6.510000000000005Avance: %6.52000000000001Avance: %6.530000000000001Avance: %6.540000000000006Avance: %6.549999999999997Avance: %6.560000000000002Avance: %6.569999999999993Avance: %6.579999999999998Avance: %6.589999999999989Avance: %6.599999999999994Avance: %6.609999999999999Avance: %6.6200000000000045Avance: %6.6299999999999955Avance: %6.640000000000001Avance: %6.650000000000006Avance: %6.659999999999997Avance: %6.670000000000002Avance: %6.679999999999993Avance: %6.689999999999998Avance: %6.699999999999989Avance: %6.710000000000008Avance: %6.719999999999999Avance: %6.730000000000004Avance: %6.739999999999995Avance: %6.75Avance: %6.760000000000005Avance: %6.769999999999996Avance: %6.780000000000001Avance: %6.789999999999992Avance: %6.799999999999997Avance: %6.810000000000002Avance: %6.820000000000007Avance: %6.829999999999998Avance: %6.840000000000003Avance: %6.849999999999994Avance: %6.859999999999999Avance: %6.8700000000000045Avance: %6.8799999999999955Avance: %6.890000000000001Avance: %6.8999999999999915Avance: %6.910000000000011Avance: %6.920000000000002Avance: %6.930000000000007Avance: %6.939999999999998Done Extracting Features 0:06:15.099038
The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.
Try some users for user 517682344
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 330) (2000, 330) False False
The number of classes has to be greater than one; got 1 class
Try some users for user 555684585
Loading df
Done loading df. DF shape is :(12397359, 8) (Original: (12397359, 8)) 		Time delta is: None mins
Loading graph from ../data/graphs/latest_subgraph.gpickle
Fetched 413 level 2 neighbourhs for user 555684585.
Len of own retweets (positive examples) is (2, 2)
done getting tweets universe. Shape is  (12649, 2)
	Dataset was truncated to 10000 tweets
Extracting features Optimized. X shape is : (10000, 413)
Avance: %0.010000000000005116Avance: %0.01999999999999602Avance: %0.030000000000001137Avance: %0.03999999999999204Avance: %0.04999999999999716Avance: %0.060000000000002274Avance: %0.07000000000000739Avance: %0.0799999999999983Avance: %0.09000000000000341Avance: %0.09999999999999432Avance: %0.10999999999999943Avance: %0.12000000000000455Avance: %0.12999999999999545Avance: %0.14000000000000057Avance: %0.14999999999999147Avance: %0.1600000000000108Avance: %0.1700000000000017Avance: %0.18000000000000682Avance: %0.18999999999999773Avance: %0.20000000000000284Avance: %0.20999999999999375Avance: %0.21999999999999886Avance: %0.22999999999998977Avance: %0.23999999999999488Avance: %0.25Avance: %0.2600000000000051Avance: %0.27000000000001023Avance: %0.28000000000000114Avance: %0.29000000000000625Avance: %0.29999999999999716Avance: %0.3100000000000023Avance: %0.3199999999999932Avance: %0.3299999999999983Avance: %0.3399999999999892Avance: %0.3499999999999943Avance: %0.35999999999999943Avance: %0.37000000000000455Avance: %0.37999999999999545Avance: %0.39000000000000057Avance: %0.4000000000000057Avance: %0.4099999999999966Avance: %0.4200000000000017Avance: %0.4299999999999926Avance: %0.4399999999999977Avance: %0.44999999999998863Avance: %0.46000000000000796Avance: %0.46999999999999886Avance: %0.480000000000004Avance: %0.4899999999999949Avance: %0.5Avance: %0.5100000000000051Avance: %0.519999999999996Avance: %0.5300000000000011Avance: %0.539999999999992Avance: %0.5499999999999972Avance: %0.5600000000000023Avance: %0.5700000000000074Avance: %0.5799999999999983Avance: %0.5900000000000034Avance: %0.5999999999999943Avance: %0.6099999999999994Avance: %0.6200000000000045Avance: %0.6299999999999955Avance: %0.6400000000000006Avance: %0.6499999999999915Avance: %0.6600000000000108Avance: %0.6700000000000017Avance: %0.6800000000000068Avance: %0.6899999999999977Avance: %0.7000000000000028Avance: %0.7099999999999937Avance: %0.7199999999999989Avance: %0.730000000000004Avance: %0.7399999999999949Avance: %0.75Avance: %0.7600000000000051Avance: %0.7700000000000102Avance: %0.7800000000000011Avance: %0.7900000000000063Avance: %0.7999999999999972Avance: %0.8100000000000023Avance: %0.8199999999999932Avance: %0.8299999999999983Avance: %0.8400000000000034Avance: %0.8499999999999943Avance: %0.8599999999999994Avance: %0.8700000000000045Avance: %0.8800000000000097Avance: %0.8900000000000006Avance: %0.9000000000000057Avance: %0.9099999999999966Avance: %0.9200000000000017Avance: %0.9299999999999926Avance: %0.9399999999999977Avance: %0.9499999999999886Avance: %0.960000000000008Avance: %0.9699999999999989Avance: %0.980000000000004Avance: %0.9900000000000091Avance: %1.0Avance: %1.0100000000000051Avance: %1.019999999999996Avance: %1.0300000000000011Avance: %1.039999999999992Avance: %1.0499999999999972Avance: %1.0600000000000023Avance: %1.0700000000000074Avance: %1.0799999999999983Avance: %1.0900000000000034Avance: %1.0999999999999943Avance: %1.1099999999999994Avance: %1.1200000000000045Avance: %1.1299999999999955Avance: %1.1400000000000006Avance: %1.1499999999999915Avance: %1.1600000000000108Avance: %1.1700000000000017Avance: %1.1800000000000068Avance: %1.1899999999999977Avance: %1.2000000000000028Avance: %1.2099999999999937Avance: %1.2199999999999989Avance: %1.230000000000004Avance: %1.2399999999999949Avance: %1.25Avance: %1.259999999999991Avance: %1.2700000000000102Avance: %1.2800000000000011Avance: %1.2900000000000063Avance: %1.2999999999999972Avance: %1.3100000000000023Avance: %1.3199999999999932Avance: %1.3299999999999983Avance: %1.3400000000000034Avance: %1.3499999999999943Avance: %1.3599999999999994Avance: %1.3700000000000045Avance: %1.3800000000000097Avance: %1.3900000000000006Avance: %1.4000000000000057Avance: %1.4099999999999966Avance: %1.4200000000000017Avance: %1.4299999999999926Avance: %1.4399999999999977Avance: %1.4499999999999886Avance: %1.4599999999999937Avance: %1.4699999999999989Avance: %1.480000000000004Avance: %1.490000000000009Avance: %1.5Avance: %1.5100000000000051Avance: %1.519999999999996Avance: %1.5300000000000011Avance: %1.539999999999992Avance: %1.5499999999999972Avance: %1.559999999999988Avance: %1.5700000000000074Avance: %1.5799999999999983Avance: %1.5900000000000034Avance: %1.5999999999999943Avance: %1.6099999999999994Avance: %1.6200000000000045Avance: %1.6299999999999955Avance: %1.6400000000000006Avance: %1.6499999999999915Avance: %1.6599999999999966Avance: %1.6700000000000017Avance: %1.6800000000000068Avance: %1.6899999999999977Avance: %1.7000000000000028Avance: %1.7099999999999937Avance: %1.7199999999999989Avance: %1.730000000000004Avance: %1.7399999999999949Avance: %1.75Avance: %1.759999999999991Avance: %1.7700000000000102Avance: %1.7800000000000011Avance: %1.7900000000000063Avance: %1.7999999999999972Avance: %1.8100000000000023Avance: %1.8199999999999932Avance: %1.8299999999999983Avance: %1.8400000000000034Avance: %1.8499999999999943Avance: %1.8599999999999994Avance: %1.8700000000000045Avance: %1.8800000000000097Avance: %1.8900000000000006Avance: %1.9000000000000057Avance: %1.9099999999999966Avance: %1.9200000000000017Avance: %1.9299999999999926Avance: %1.9399999999999977Avance: %1.9500000000000028Avance: %1.9599999999999937Avance: %1.9699999999999989Avance: %1.980000000000004Avance: %1.990000000000009Avance: %2.0Avance: %2.010000000000005Avance: %2.019999999999996Avance: %2.030000000000001Avance: %2.039999999999992Avance: %2.049999999999997Avance: %2.0600000000000023Avance: %2.0700000000000074Avance: %2.0799999999999983Avance: %2.0900000000000034Avance: %2.1000000000000085Avance: %2.1099999999999994Avance: %2.1200000000000045Avance: %2.1299999999999955Avance: %2.1400000000000006Avance: %2.1499999999999915Avance: %2.1599999999999966Avance: %2.1700000000000017Avance: %2.180000000000007Avance: %2.1899999999999977Avance: %2.200000000000003Avance: %2.210000000000008Avance: %2.219999999999999Avance: %2.230000000000004Avance: %2.239999999999995Avance: %2.25Avance: %2.259999999999991Avance: %2.2700000000000102Avance: %2.280000000000001Avance: %2.2900000000000063Avance: %2.299999999999997Avance: %2.3100000000000023Avance: %2.319999999999993Avance: %2.3299999999999983Avance: %2.3400000000000034Avance: %2.3499999999999943Avance: %2.3599999999999994Avance: %2.3700000000000045Avance: %2.3800000000000097Avance: %2.3900000000000006Avance: %2.4000000000000057Avance: %2.4099999999999966Avance: %2.4200000000000017Avance: %2.4299999999999926Avance: %2.4399999999999977Avance: %2.450000000000003Avance: %2.4599999999999937Avance: %2.469999999999999Avance: %2.480000000000004Avance: %2.490000000000009Avance: %2.5Avance: %2.510000000000005Avance: %2.519999999999996Avance: %2.530000000000001Avance: %2.539999999999992Avance: %2.549999999999997Avance: %2.5600000000000023Avance: %2.569999999999993Avance: %2.5799999999999983Avance: %2.5900000000000034Avance: %2.6000000000000085Avance: %2.6099999999999994Avance: %2.6200000000000045Avance: %2.6299999999999955Avance: %2.6400000000000006Avance: %2.6499999999999915Avance: %2.6599999999999966Avance: %2.6699999999999875Avance: %2.680000000000007Avance: %2.6899999999999977Avance: %2.700000000000003Avance: %2.710000000000008Avance: %2.719999999999999Avance: %2.730000000000004Avance: %2.739999999999995Avance: %2.75Avance: %2.759999999999991Avance: %2.769999999999996Avance: %2.780000000000001Avance: %2.7900000000000063Avance: %2.799999999999997Avance: %2.8100000000000023Avance: %2.819999999999993Avance: %2.8299999999999983Avance: %2.8400000000000034Avance: %2.8499999999999943Avance: %2.8599999999999994Avance: %2.8699999999999903Avance: %2.8800000000000097Avance: %2.8900000000000006Avance: %2.9000000000000057Avance: %2.9099999999999966Avance: %2.9200000000000017Avance: %2.9299999999999926Avance: %2.9399999999999977Avance: %2.950000000000003Avance: %2.9599999999999937Avance: %2.969999999999999Avance: %2.980000000000004Avance: %2.990000000000009Avance: %3.0Avance: %3.010000000000005Avance: %3.019999999999996Avance: %3.030000000000001Avance: %3.039999999999992Avance: %3.049999999999997Avance: %3.0600000000000023Avance: %3.069999999999993Avance: %3.0799999999999983Avance: %3.0900000000000034Avance: %3.1000000000000085Avance: %3.1099999999999994Avance: %3.1200000000000045Avance: %3.1299999999999955Avance: %3.1400000000000006Avance: %3.1499999999999915Avance: %3.1599999999999966Avance: %3.1700000000000017Avance: %3.180000000000007Avance: %3.1899999999999977Avance: %3.200000000000003Avance: %3.210000000000008Avance: %3.219999999999999Avance: %3.230000000000004Avance: %3.239999999999995Avance: %3.25Avance: %3.259999999999991Avance: %3.269999999999996Avance: %3.280000000000001Avance: %3.2900000000000063Avance: %3.299999999999997Avance: %3.3100000000000023Avance: %3.3200000000000074Avance: %3.3299999999999983Avance: %3.3400000000000034Avance: %3.3499999999999943Avance: %3.3599999999999994Avance: %3.3699999999999903Avance: %3.3800000000000097Avance: %3.3900000000000006Avance: %3.4000000000000057Avance: %3.4099999999999966Avance: %3.4200000000000017Avance: %3.430000000000007Avance: %3.4399999999999977Avance: %3.450000000000003Avance: %3.4599999999999937Avance: %3.469999999999999Avance: %3.480000000000004Avance: %3.490000000000009Avance: %3.5Avance: %3.510000000000005Avance: %3.519999999999996Avance: %3.530000000000001Avance: %3.539999999999992Avance: %3.549999999999997Avance: %3.5600000000000023Avance: %3.569999999999993Avance: %3.5800000000000125Avance: %3.5900000000000034Avance: %3.6000000000000085Avance: %3.6099999999999994Avance: %3.6200000000000045Avance: %3.6299999999999955Avance: %3.6400000000000006Avance: %3.6499999999999915Avance: %3.6599999999999966Avance: %3.6700000000000017Avance: %3.680000000000007Avance: %3.6899999999999977Avance: %3.700000000000003Avance: %3.710000000000008Avance: %3.719999999999999Avance: %3.730000000000004Avance: %3.739999999999995Avance: %3.75Avance: %3.759999999999991Avance: %3.769999999999996Avance: %3.780000000000001Avance: %3.7900000000000063Avance: %3.799999999999997Avance: %3.8100000000000023Avance: %3.8200000000000074Avance: %3.8299999999999983Avance: %3.8400000000000034Avance: %3.8499999999999943Avance: %3.8599999999999994Avance: %3.8699999999999903Avance: %3.8799999999999955Avance: %3.8900000000000006Avance: %3.9000000000000057Avance: %3.9099999999999966Avance: %3.9200000000000017Avance: %3.930000000000007Avance: %3.9399999999999977Avance: %3.950000000000003Avance: %3.9599999999999937Avance: %3.969999999999999Avance: %3.9799999999999898Avance: %3.990000000000009Avance: %4.0Avance: %4.010000000000005Avance: %4.019999999999996Avance: %4.030000000000001Avance: %4.039999999999992Avance: %4.049999999999997Avance: %4.060000000000002Avance: %4.069999999999993Avance: %4.079999999999998Avance: %4.090000000000003Avance: %4.1000000000000085Avance: %4.109999999999999Avance: %4.1200000000000045Avance: %4.1299999999999955Done Extracting Features 0:03:47.847700
The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.
Try some users for user 130866630
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 338) (2000, 338) True True
Best parameters set found on training set:

{'C': 0.1, 'class_weight': None, 'gamma': 1, 'kernel': 'poly'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6860
        True       1.00      0.94      0.97       140

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.97      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1960
        True       1.00      0.85      0.92        40

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.93      0.96      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 157523545
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 169) (2000, 169) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': None, 'gamma': 1, 'kernel': 'poly'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       0.99      1.00      1.00      5433
        True       1.00      0.97      0.98      1567

   micro avg       0.99      0.99      0.99      7000
   macro avg       1.00      0.98      0.99      7000
weighted avg       0.99      0.99      0.99      7000


Scores on test set.

              precision    recall  f1-score   support

       False       0.98      1.00      0.99      1553
        True       0.99      0.93      0.96       447

   micro avg       0.98      0.98      0.98      2000
   macro avg       0.99      0.96      0.97      2000
weighted avg       0.98      0.98      0.98      2000


			END SVC
Try some users for user 150903507
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 582) (2000, 582) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6958
        True       1.00      1.00      1.00        42

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1988
        True       1.00      1.00      1.00        12

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 8936082
Loading df
Done loading df. DF shape is :(12397359, 8) (Original: (12397359, 8)) 		Time delta is: None mins
Loading graph from ../data/graphs/latest_subgraph.gpickle
Fetched 344 level 2 neighbourhs for user 8936082.
Len of own retweets (positive examples) is (1, 2)
done getting tweets universe. Shape is  (11954, 2)
	Dataset was truncated to 10000 tweets
Extracting features Optimized. X shape is : (10000, 344)
Avance: %0.010000000000005116Avance: %0.01999999999999602Avance: %0.030000000000001137Avance: %0.03999999999999204Avance: %0.04999999999999716Avance: %0.060000000000002274Avance: %0.07000000000000739Avance: %0.0799999999999983Avance: %0.09000000000000341Avance: %0.09999999999999432Avance: %0.10999999999999943Avance: %0.12000000000000455Avance: %0.12999999999999545Avance: %0.14000000000000057Avance: %0.14999999999999147Avance: %0.1600000000000108Avance: %0.1700000000000017Avance: %0.18000000000000682Avance: %0.18999999999999773Avance: %0.20000000000000284Avance: %0.20999999999999375Avance: %0.21999999999999886Avance: %0.22999999999998977Avance: %0.23999999999999488Avance: %0.25Avance: %0.2600000000000051Avance: %0.27000000000001023Avance: %0.28000000000000114Avance: %0.29000000000000625Avance: %0.29999999999999716Avance: %0.3100000000000023Avance: %0.3199999999999932Avance: %0.3299999999999983Avance: %0.3399999999999892Avance: %0.3499999999999943Avance: %0.35999999999999943Avance: %0.37000000000000455Avance: %0.37999999999999545Avance: %0.39000000000000057Avance: %0.4000000000000057Avance: %0.4099999999999966Avance: %0.4200000000000017Avance: %0.4299999999999926Avance: %0.4399999999999977Avance: %0.44999999999998863Avance: %0.46000000000000796Avance: %0.46999999999999886Avance: %0.480000000000004Avance: %0.4899999999999949Avance: %0.5Avance: %0.5100000000000051Avance: %0.519999999999996Avance: %0.5300000000000011Avance: %0.539999999999992Avance: %0.5499999999999972Avance: %0.5600000000000023Avance: %0.5700000000000074Avance: %0.5799999999999983Avance: %0.5900000000000034Avance: %0.5999999999999943Avance: %0.6099999999999994Avance: %0.6200000000000045Avance: %0.6299999999999955Avance: %0.6400000000000006Avance: %0.6499999999999915Avance: %0.6600000000000108Avance: %0.6700000000000017Avance: %0.6800000000000068Avance: %0.6899999999999977Avance: %0.7000000000000028Avance: %0.7099999999999937Avance: %0.7199999999999989Avance: %0.730000000000004Avance: %0.7399999999999949Avance: %0.75Avance: %0.7600000000000051Avance: %0.7700000000000102Avance: %0.7800000000000011Avance: %0.7900000000000063Avance: %0.7999999999999972Avance: %0.8100000000000023Avance: %0.8199999999999932Avance: %0.8299999999999983Avance: %0.8400000000000034Avance: %0.8499999999999943Avance: %0.8599999999999994Avance: %0.8700000000000045Avance: %0.8800000000000097Avance: %0.8900000000000006Avance: %0.9000000000000057Avance: %0.9099999999999966Avance: %0.9200000000000017Avance: %0.9299999999999926Avance: %0.9399999999999977Avance: %0.9499999999999886Avance: %0.960000000000008Avance: %0.9699999999999989Avance: %0.980000000000004Avance: %0.9900000000000091Avance: %1.0Avance: %1.0100000000000051Avance: %1.019999999999996Avance: %1.0300000000000011Avance: %1.039999999999992Avance: %1.0499999999999972Avance: %1.0600000000000023Avance: %1.0700000000000074Avance: %1.0799999999999983Avance: %1.0900000000000034Avance: %1.0999999999999943Avance: %1.1099999999999994Avance: %1.1200000000000045Avance: %1.1299999999999955Avance: %1.1400000000000006Avance: %1.1499999999999915Avance: %1.1600000000000108Avance: %1.1700000000000017Avance: %1.1800000000000068Avance: %1.1899999999999977Avance: %1.2000000000000028Avance: %1.2099999999999937Avance: %1.2199999999999989Avance: %1.230000000000004Avance: %1.2399999999999949Avance: %1.25Avance: %1.259999999999991Avance: %1.2700000000000102Avance: %1.2800000000000011Avance: %1.2900000000000063Avance: %1.2999999999999972Avance: %1.3100000000000023Avance: %1.3199999999999932Avance: %1.3299999999999983Avance: %1.3400000000000034Avance: %1.3499999999999943Avance: %1.3599999999999994Avance: %1.3700000000000045Avance: %1.3800000000000097Avance: %1.3900000000000006Avance: %1.4000000000000057Avance: %1.4099999999999966Avance: %1.4200000000000017Avance: %1.4299999999999926Avance: %1.4399999999999977Avance: %1.4499999999999886Avance: %1.4599999999999937Avance: %1.4699999999999989Avance: %1.480000000000004Avance: %1.490000000000009Avance: %1.5Avance: %1.5100000000000051Avance: %1.519999999999996Avance: %1.5300000000000011Avance: %1.539999999999992Avance: %1.5499999999999972Avance: %1.559999999999988Avance: %1.5700000000000074Avance: %1.5799999999999983Avance: %1.5900000000000034Avance: %1.5999999999999943Avance: %1.6099999999999994Avance: %1.6200000000000045Avance: %1.6299999999999955Avance: %1.6400000000000006Avance: %1.6499999999999915Avance: %1.6599999999999966Avance: %1.6700000000000017Avance: %1.6800000000000068Avance: %1.6899999999999977Avance: %1.7000000000000028Avance: %1.7099999999999937Avance: %1.7199999999999989Avance: %1.730000000000004Avance: %1.7399999999999949Avance: %1.75Avance: %1.759999999999991Avance: %1.7700000000000102Avance: %1.7800000000000011Avance: %1.7900000000000063Avance: %1.7999999999999972Avance: %1.8100000000000023Avance: %1.8199999999999932Avance: %1.8299999999999983Avance: %1.8400000000000034Avance: %1.8499999999999943Avance: %1.8599999999999994Avance: %1.8700000000000045Avance: %1.8800000000000097Avance: %1.8900000000000006Avance: %1.9000000000000057Avance: %1.9099999999999966Avance: %1.9200000000000017Avance: %1.9299999999999926Avance: %1.9399999999999977Avance: %1.9500000000000028Avance: %1.9599999999999937Avance: %1.9699999999999989Avance: %1.980000000000004Avance: %1.990000000000009Avance: %2.0Avance: %2.010000000000005Avance: %2.019999999999996Avance: %2.030000000000001Avance: %2.039999999999992Avance: %2.049999999999997Avance: %2.0600000000000023Avance: %2.0700000000000074Avance: %2.0799999999999983Avance: %2.0900000000000034Avance: %2.1000000000000085Avance: %2.1099999999999994Avance: %2.1200000000000045Avance: %2.1299999999999955Avance: %2.1400000000000006Avance: %2.1499999999999915Avance: %2.1599999999999966Avance: %2.1700000000000017Avance: %2.180000000000007Avance: %2.1899999999999977Avance: %2.200000000000003Avance: %2.210000000000008Avance: %2.219999999999999Avance: %2.230000000000004Avance: %2.239999999999995Avance: %2.25Avance: %2.259999999999991Avance: %2.2700000000000102Avance: %2.280000000000001Avance: %2.2900000000000063Avance: %2.299999999999997Avance: %2.3100000000000023Avance: %2.319999999999993Avance: %2.3299999999999983Avance: %2.3400000000000034Avance: %2.3499999999999943Avance: %2.3599999999999994Avance: %2.3700000000000045Avance: %2.3800000000000097Avance: %2.3900000000000006Avance: %2.4000000000000057Avance: %2.4099999999999966Avance: %2.4200000000000017Avance: %2.4299999999999926Avance: %2.4399999999999977Avance: %2.450000000000003Avance: %2.4599999999999937Avance: %2.469999999999999Avance: %2.480000000000004Avance: %2.490000000000009Avance: %2.5Avance: %2.510000000000005Avance: %2.519999999999996Avance: %2.530000000000001Avance: %2.539999999999992Avance: %2.549999999999997Avance: %2.5600000000000023Avance: %2.569999999999993Avance: %2.5799999999999983Avance: %2.5900000000000034Avance: %2.6000000000000085Avance: %2.6099999999999994Avance: %2.6200000000000045Avance: %2.6299999999999955Avance: %2.6400000000000006Avance: %2.6499999999999915Avance: %2.6599999999999966Avance: %2.6699999999999875Avance: %2.680000000000007Avance: %2.6899999999999977Avance: %2.700000000000003Avance: %2.710000000000008Avance: %2.719999999999999Avance: %2.730000000000004Avance: %2.739999999999995Avance: %2.75Avance: %2.759999999999991Avance: %2.769999999999996Avance: %2.780000000000001Avance: %2.7900000000000063Avance: %2.799999999999997Avance: %2.8100000000000023Avance: %2.819999999999993Avance: %2.8299999999999983Avance: %2.8400000000000034Avance: %2.8499999999999943Avance: %2.8599999999999994Avance: %2.8699999999999903Avance: %2.8800000000000097Avance: %2.8900000000000006Avance: %2.9000000000000057Avance: %2.9099999999999966Avance: %2.9200000000000017Avance: %2.9299999999999926Avance: %2.9399999999999977Avance: %2.950000000000003Avance: %2.9599999999999937Avance: %2.969999999999999Avance: %2.980000000000004Avance: %2.990000000000009Avance: %3.0Avance: %3.010000000000005Avance: %3.019999999999996Avance: %3.030000000000001Avance: %3.039999999999992Avance: %3.049999999999997Avance: %3.0600000000000023Avance: %3.069999999999993Avance: %3.0799999999999983Avance: %3.0900000000000034Avance: %3.1000000000000085Avance: %3.1099999999999994Avance: %3.1200000000000045Avance: %3.1299999999999955Avance: %3.1400000000000006Avance: %3.1499999999999915Avance: %3.1599999999999966Avance: %3.1700000000000017Avance: %3.180000000000007Avance: %3.1899999999999977Avance: %3.200000000000003Avance: %3.210000000000008Avance: %3.219999999999999Avance: %3.230000000000004Avance: %3.239999999999995Avance: %3.25Avance: %3.259999999999991Avance: %3.269999999999996Avance: %3.280000000000001Avance: %3.2900000000000063Avance: %3.299999999999997Avance: %3.3100000000000023Avance: %3.3200000000000074Avance: %3.3299999999999983Avance: %3.3400000000000034Avance: %3.3499999999999943Avance: %3.3599999999999994Avance: %3.3699999999999903Avance: %3.3800000000000097Avance: %3.3900000000000006Avance: %3.4000000000000057Avance: %3.4099999999999966Avance: %3.4200000000000017Avance: %3.430000000000007Avance: %3.4399999999999977Done Extracting Features 0:02:50.631414
The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.
Try some users for user 7761842
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 695) (2000, 695) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6992
        True       1.00      0.88      0.93         8

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.94      0.97      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1997
        True       1.00      0.67      0.80         3

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.83      0.90      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 150115389
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 598) (2000, 598) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6758
        True       1.00      1.00      1.00       242

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1931
        True       1.00      1.00      1.00        69

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 7712092
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 696) (2000, 696) False False
The number of classes has to be greater than one; got 1 class
Try some users for user 149970952
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 390) (2000, 390) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6513
        True       1.00      0.97      0.98       487

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.98      0.99      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1861
        True       1.00      0.97      0.99       139

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      0.99      0.99      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 117416319
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 379) (2000, 379) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6994
        True       1.00      1.00      1.00         6

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1999
        True       1.00      1.00      1.00         1

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 218894949
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 498) (2000, 498) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6944
        True       1.00      0.98      0.99        56

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1984
        True       1.00      1.00      1.00        16

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 14884391
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 567) (2000, 567) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 0.1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6996
        True       1.00      1.00      1.00         4

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1999
        True       1.00      1.00      1.00         1

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 199793152
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 235) (2000, 235) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6987
        True       1.00      1.00      1.00        13

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1997
        True       1.00      1.00      1.00         3

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 139160485
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 638) (2000, 638) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6978
        True       1.00      1.00      1.00        22

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1993
        True       1.00      1.00      1.00         7

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 64524269
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 537) (2000, 537) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6810
        True       1.00      1.00      1.00       190

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1945
        True       1.00      1.00      1.00        55

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 70724306
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 642) (2000, 642) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6985
        True       1.00      1.00      1.00        15

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1995
        True       1.00      1.00      1.00         5

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 239200202
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 509) (2000, 509) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6892
        True       1.00      1.00      1.00       108

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      1.00      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1969
        True       1.00      1.00      1.00        31

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
Try some users for user 60083391
			START SVC
# Tuning hyper-parameters for f1
X_train.shape, X_test.shape, True in y_train, True in y_test :
(7000, 382) (2000, 382) True True
Best parameters set found on training set:

{'C': 0.01, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'rbf'}
Detailed classification report:

Scores on training set.
              precision    recall  f1-score   support

       False       1.00      1.00      1.00      6942
        True       1.00      0.98      0.99        58

   micro avg       1.00      1.00      1.00      7000
   macro avg       1.00      0.99      1.00      7000
weighted avg       1.00      1.00      1.00      7000


Scores on test set.

              precision    recall  f1-score   support

       False       1.00      1.00      1.00      1983
        True       1.00      1.00      1.00        17

   micro avg       1.00      1.00      1.00      2000
   macro avg       1.00      1.00      1.00      2000
weighted avg       1.00      1.00      1.00      2000


			END SVC
